{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 Avaliação de Modelos LLM: Como Saber se Nosso \"Filho\" está Indo Bem na Escola da IA\n\n## Módulo 9 - Curso Introdução à LLMs\n\n**Instrutor:** Pedro Nunes Guth  \n**Tema:** Métricas, benchmarks e técnicas para avaliar nossos LLMs\n\n---\n\nE aí, pessoal! 🚀\n\nImagina que você é pai/mãe de um filho que está na escola. Como você sabe se ele está aprendendo direito? Você olha as notas, conversa com os professores, vê se ele consegue aplicar o que aprendeu no dia a dia, né?\n\nCom LLMs é a MESMA COISA! Depois de treinar nossos modelos (lembram do módulo 7?), precisamos saber: **\"Será que esse bichinho está funcionando direito?\"**\n\nBora descobrir as melhores formas de avaliar nossos modelos! 🎯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Vamos preparar nosso ambiente!\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurações para gráficos bonitos\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(\"🎯 Ambiente configurado! Bora avaliar alguns modelos!\")\n",
        "print(\"📊 Bibliotecas carregadas com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤔 Tá, mas o que é Avaliação de Modelos?\n\nGalera, avaliar um modelo é como avaliar um chef de cozinha. Você não vai só perguntar \"você sabe cozinhar?\". Você vai:\n\n1. **Testar receitas específicas** (tarefas específicas)\n2. **Ver a consistência** (ele faz o mesmo prato sempre igual?)\n3. **Medir o tempo** (eficiência)\n4. **Pedir opinião de diferentes pessoas** (múltiplos avaliadores)\n5. **Comparar com outros chefs** (benchmarks)\n\n### Por que é TÃO importante?\n\nLembram do módulo 8 sobre prompting? Lá vimos como **fazer** o modelo funcionar melhor. Agora precisamos **medir** se realmente funcionou!\n\n**Sem avaliação adequada:**\n- 🚫 Você não sabe se seu modelo está melhorando\n- 🚫 Pode estar gastando dinheiro à toa\n- 🚫 Usuários podem ter experiências ruins\n- 🚫 Seu chefe vai te perguntar \"tá, mas funciona mesmo?\"\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdução-à-llms-modulo-09_img_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Tipos de Avaliação: O Cardápio Completo\n\nAssim como num restaurante tem entrada, prato principal e sobremesa, na avaliação de LLMs temos diferentes \"pratos\":\n\n### 1. Avaliação Intrínseca vs Extrínseca\n\n**Intrínseca** = Como o modelo se sai em tarefas gerais (tipo prova do ENEM)  \n**Extrínseca** = Como ele se sai na SUA aplicação específica (tipo prova de medicina)\n\n### 2. Avaliação Automática vs Humana\n\n**Automática** = Computador corrige (rápido, barato, mas às vezes \"burro\")  \n**Humana** = Gente real avalia (lento, caro, mas entende nuances)\n\n### 3. Avaliação Online vs Offline\n\n**Offline** = Testa antes de colocar em produção  \n**Online** = Monitora enquanto usuários reais usam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um exemplo simples de dados de avaliação\n",
        "# Simulando respostas de um modelo de classificação de sentimentos\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulando dados de teste\n",
        "n_samples = 1000\n",
        "true_labels = np.random.choice(['positivo', 'negativo', 'neutro'], n_samples, p=[0.4, 0.3, 0.3])\n",
        "\n",
        "# Simulando predições de 3 modelos diferentes\n",
        "# Modelo A: Bem balanceado\n",
        "modelo_a_accuracy = 0.85\n",
        "modelo_a_preds = []\n",
        "\n",
        "for true_label in true_labels:\n",
        "    if np.random.random() < modelo_a_accuracy:\n",
        "        modelo_a_preds.append(true_label)\n",
        "    else:\n",
        "        # Erro aleatório\n",
        "        options = ['positivo', 'negativo', 'neutro']\n",
        "        options.remove(true_label)\n",
        "        modelo_a_preds.append(np.random.choice(options))\n",
        "\n",
        "# Criando DataFrame para facilitar análise\n",
        "results_df = pd.DataFrame({\n",
        "    'true_label': true_labels,\n",
        "    'modelo_a_pred': modelo_a_preds\n",
        "})\n",
        "\n",
        "print(\"📊 Dataset de avaliação criado!\")\n",
        "print(f\"📝 {len(results_df)} amostras de teste\")\n",
        "print(\"\\n🔍 Primeiras 10 linhas:\")\n",
        "print(results_df.head(10))\n",
        "\n",
        "print(\"\\n📈 Distribuição das classes:\")\n",
        "print(results_df['true_label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Métricas Clássicas: Os \"Clássicos da MPB\" da Avaliação\n\nAssim como na música brasileira temos clássicos que todo mundo conhece, na avaliação de modelos temos as métricas fundamentais:\n\n### 1. Acurácia (Accuracy)\n**\"De 100 tentativas, quantas eu acertei?\"**  \nÉ tipo aprovação no vestibular: simples e direto!\n\n### 2. Precisão (Precision)\n**\"Das vezes que eu disse 'é positivo', quantas realmente eram?\"**  \nÉ tipo um detector de mentiras: quando ele fala que é verdade, geralmente é mesmo?\n\n### 3. Recall (Revocação)\n**\"De todos os casos positivos, quantos eu consegui encontrar?\"**  \nÉ tipo um detetive: consegue achar TODOS os criminosos?\n\n### 4. F1-Score\n**\"A média harmônica entre precisão e recall\"**  \nÉ tipo o \"meio termo\" entre ser certeiro e ser abrangente.\n\n**💡 Dica do Pedro:** Use F1 quando você quer balancear precisão e recall. É tipo escolher entre ser um sniper (alta precisão) ou uma metralhadora (alto recall)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculando métricas clássicas para nosso modelo\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Calculando métricas básicas\n",
        "accuracy = accuracy_score(results_df['true_label'], results_df['modelo_a_pred'])\n",
        "\n",
        "print(\"🎯 MÉTRICAS DO MODELO A\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"📊 Acurácia: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
        "print(\"\\n📋 Relatório Completo:\")\n",
        "print(classification_report(results_df['true_label'], results_df['modelo_a_pred']))\n",
        "\n",
        "# Matriz de Confusão\n",
        "print(\"\\n🤔 Matriz de Confusão:\")\n",
        "cm = confusion_matrix(results_df['true_label'], results_df['modelo_a_pred'])\n",
        "print(cm)\n",
        "\n",
        "# Vamos interpretar\n",
        "print(\"\\n🧠 INTERPRETAÇÃO:\")\n",
        "print(f\"✅ O modelo acerta {accuracy*100:.1f}% das vezes\")\n",
        "print(\"📊 A matriz mostra onde ele mais erra\")\n",
        "print(\"🎯 F1-score nos dá uma visão balanceada por classe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a matriz de confusão de forma bonita\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Matriz de confusão normalizada\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_normalized, \n",
        "            annot=True, \n",
        "            fmt='.2f', \n",
        "            cmap='Blues',\n",
        "            xticklabels=['negativo', 'neutro', 'positivo'],\n",
        "            yticklabels=['negativo', 'neutro', 'positivo'])\n",
        "\n",
        "plt.title('🎯 Matriz de Confusão - Modelo A\\n(Valores Normalizados)', fontsize=14)\n",
        "plt.xlabel('Predição do Modelo', fontsize=12)\n",
        "plt.ylabel('Verdade Real', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"📊 Matriz de Confusão Visualizada!\")\n",
        "print(\"💡 Quanto mais azul escuro, melhor o modelo naquela classe\")\n",
        "print(\"🎯 Diagonal principal = acertos, resto = erros\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🏆 Benchmarks Famosos: O \"Top 10 da Billboard\" dos LLMs\n\nAssim como na música temos charts para comparar artistas, no mundo dos LLMs temos benchmarks padrão:\n\n### 📚 GLUE & SuperGLUE\n**\"O ENEM dos modelos de linguagem\"**  \n- Conjunto de tarefas variadas\n- Compreensão de texto, inferência, similaridade\n- Todo mundo usa para comparar modelos\n\n### 🧠 HellaSwag\n**\"Teste de senso comum\"**  \n- \"Maria abriu a geladeira e...\" (o que acontece depois?)\n- Testa se o modelo entende o mundo real\n\n### 📖 SQuAD\n**\"Interpretação de texto\"**  \n- Lê um texto e responde perguntas\n- Tipo prova de português do ensino médio\n\n### 🔢 GSM8K\n**\"Matemática de escola\"**  \n- Problemas matemáticos em linguagem natural\n- \"João tem 5 maçãs e come 2, quantas sobraram?\"\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdução-à-llms-modulo-09_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulando resultados de diferentes modelos em benchmarks famosos\n",
        "benchmarks_data = {\n",
        "    'Modelo': ['GPT-3.5', 'GPT-4', 'Claude-2', 'Llama-2-70B', 'Gemini Pro'],\n",
        "    'GLUE': [87.2, 91.4, 88.7, 84.3, 89.1],\n",
        "    'HellaSwag': [76.4, 85.2, 78.9, 73.1, 81.2],\n",
        "    'SQuAD': [88.1, 92.3, 89.7, 85.4, 90.8],\n",
        "    'GSM8K': [23.5, 67.1, 41.2, 28.7, 52.3],\n",
        "    'MMLU': [70.0, 86.4, 75.2, 68.9, 79.1]\n",
        "}\n",
        "\n",
        "benchmarks_df = pd.DataFrame(benchmarks_data)\n",
        "print(\"🏆 LEADERBOARD DOS MODELOS\")\n",
        "print(\"=\" * 50)\n",
        "print(benchmarks_df.to_string(index=False))\n",
        "\n",
        "# Calculando score médio\n",
        "numeric_cols = ['GLUE', 'HellaSwag', 'SQuAD', 'GSM8K', 'MMLU']\n",
        "benchmarks_df['Score_Médio'] = benchmarks_df[numeric_cols].mean(axis=1)\n",
        "\n",
        "print(\"\\n🎯 RANKING POR SCORE MÉDIO:\")\n",
        "ranking = benchmarks_df.sort_values('Score_Médio', ascending=False)[['Modelo', 'Score_Médio']]\n",
        "print(ranking.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando o desempenho nos benchmarks\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Gráfico 1: Comparação por benchmark\n",
        "benchmarks_melted = benchmarks_df.melt(id_vars=['Modelo'], \n",
        "                                      value_vars=numeric_cols,\n",
        "                                      var_name='Benchmark', \n",
        "                                      value_name='Score')\n",
        "\n",
        "sns.barplot(data=benchmarks_melted, x='Benchmark', y='Score', hue='Modelo', ax=ax1)\n",
        "ax1.set_title('🏆 Desempenho por Benchmark', fontsize=14)\n",
        "ax1.set_ylabel('Score (%)')\n",
        "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Gráfico 2: Score médio geral\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']\n",
        "bars = ax2.bar(benchmarks_df['Modelo'], benchmarks_df['Score_Médio'], color=colors)\n",
        "ax2.set_title('🎯 Score Médio Geral', fontsize=14)\n",
        "ax2.set_ylabel('Score Médio (%)')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "             f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"📊 Gráficos de benchmark gerados!\")\n",
        "print(\"💡 Note como diferentes modelos se saem melhor em diferentes tarefas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 Fluxo de Avaliação: A \"Receita de Bolo\" da Avaliação\n\nToda avaliação séria segue um processo, tipo receita de bolo da vovó:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\ngraph TD\n    A[📊 Definir Objetivos] --> B[📝 Escolher Métricas]\n    B --> C[🎯 Preparar Datasets]\n    C --> D[🔄 Executar Testes]\n    D --> E[📈 Calcular Métricas]\n    E --> F[🤔 Analisar Resultados]\n    F --> G{✅ Satisfatório?}\n    G -->|Não| H[🔧 Melhorar Modelo]\n    H --> D\n    G -->|Sim| I[🚀 Deploy/Conclusão]\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 Passo 1: Definir Objetivos\n**\"O que meu modelo precisa fazer MUITO bem?\"**\n\n- Chatbot de atendimento → Compreensão + Cortesia\n- Tradutor → Precisão + Fluência  \n- Resumidor → Concisão + Relevância\n\n### 📊 Passo 2: Escolher Métricas\n**\"Como vou medir se está bom?\"**\n\n- **Objetivas:** BLEU, ROUGE, Perplexidade\n- **Subjetivas:** Avaliação humana, A/B testing\n- **Técnicas:** Latência, throughput, uso de memória\n\n**💡 Dica do Pedro:** Nunca use só UMA métrica! É tipo avaliar um namorado só pela aparência... você precisa de múltiplos critérios! 😄"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando métricas específicas para LLMs\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def calculate_bleu_simple(reference, candidate):\n",
        "    \"\"\"Implementação simplificada do BLEU score\"\"\"\n",
        "    ref_words = reference.lower().split()\n",
        "    cand_words = candidate.lower().split()\n",
        "    \n",
        "    if len(cand_words) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Contagem de palavras em comum (1-gram precision)\n",
        "    ref_counter = Counter(ref_words)\n",
        "    cand_counter = Counter(cand_words)\n",
        "    \n",
        "    overlap = sum((ref_counter & cand_counter).values())\n",
        "    precision = overlap / len(cand_words)\n",
        "    \n",
        "    # Brevity penalty (penaliza textos muito curtos)\n",
        "    bp = 1.0 if len(cand_words) >= len(ref_words) else math.exp(1 - len(ref_words)/len(cand_words))\n",
        "    \n",
        "    return bp * precision\n",
        "\n",
        "def calculate_perplexity_estimate(text_length, num_errors):\n",
        "    \"\"\"Estimativa simplificada de perplexidade\"\"\"\n",
        "    # Quanto menor, melhor\n",
        "    error_rate = num_errors / text_length if text_length > 0 else 1.0\n",
        "    return math.exp(error_rate)\n",
        "\n",
        "# Exemplo de avaliação para tarefa de geração de texto\n",
        "evaluation_examples = [\n",
        "    {\n",
        "        'referencia': 'O gato subiu no telhado da casa',\n",
        "        'modelo_a': 'O gato escalou o telhado da residência',\n",
        "        'modelo_b': 'O felino subiu no teto do lar'\n",
        "    },\n",
        "    {\n",
        "        'referencia': 'Python é uma linguagem de programação',\n",
        "        'modelo_a': 'Python é uma linguagem para programar',\n",
        "        'modelo_b': 'Python programa linguagem computador'\n",
        "    },\n",
        "    {\n",
        "        'referencia': 'Machine learning está revolucionando a tecnologia',\n",
        "        'modelo_a': 'Aprendizado de máquina está transformando a tecnologia',\n",
        "        'modelo_b': 'ML mudando tech muito'\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"🎯 AVALIAÇÃO DE GERAÇÃO DE TEXTO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, example in enumerate(evaluation_examples, 1):\n",
        "    print(f\"\\n📝 Exemplo {i}:\")\n",
        "    print(f\"Referência: {example['referencia']}\")\n",
        "    print(f\"Modelo A: {example['modelo_a']}\")\n",
        "    print(f\"Modelo B: {example['modelo_b']}\")\n",
        "    \n",
        "    # BLEU scores\n",
        "    bleu_a = calculate_bleu_simple(example['referencia'], example['modelo_a'])\n",
        "    bleu_b = calculate_bleu_simple(example['referencia'], example['modelo_b'])\n",
        "    \n",
        "    print(f\"\\n📊 BLEU Score:\")\n",
        "    print(f\"  Modelo A: {bleu_a:.3f}\")\n",
        "    print(f\"  Modelo B: {bleu_b:.3f}\")\n",
        "    \n",
        "    winner = \"A\" if bleu_a > bleu_b else \"B\"\n",
        "    print(f\"🏆 Vencedor: Modelo {winner}\")\n",
        "\n",
        "print(\"\\n💡 BLEU mede similaridade lexical com a referência\")\n",
        "print(\"📈 Valores mais altos = mais similar ao texto esperado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Métricas Específicas para LLMs: Os \"Hits do Momento\"\n\nAgora vamos falar das métricas que são específicas para modelos de linguagem grandes:\n\n### 🔤 BLEU Score\n**\"Quantas palavras em comum com a resposta esperada?\"**  \nOriginalmente para tradução, agora usado em geração de texto.\n\n### 📄 ROUGE Score\n**\"Bom para avaliar resumos\"**  \nMede overlap de n-gramas entre texto gerado e referência.\n\n### 🌊 Perplexidade (Perplexity)\n**\"Quão 'surpreso' o modelo fica com o texto?\"**  \nQuanto menor, melhor. É tipo medir se o texto \"flui naturalmente\".\n\n### 🎯 BERTScore\n**\"Usa embeddings para comparar significado\"**  \nMais sofisticado que BLEU, entende sinônimos.\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdução-à-llms-modulo-09_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparando diferentes métricas para LLMs\n",
        "import random\n",
        "\n",
        "# Simulando dados de múltiplos modelos\n",
        "models_data = {\n",
        "    'GPT-3.5': {'bleu': 0.72, 'rouge': 0.68, 'perplexity': 12.4, 'bert_score': 0.81},\n",
        "    'GPT-4': {'bleu': 0.79, 'rouge': 0.75, 'perplexity': 8.9, 'bert_score': 0.87},\n",
        "    'Claude-2': {'bleu': 0.74, 'rouge': 0.71, 'perplexity': 10.2, 'bert_score': 0.83},\n",
        "    'Llama-2': {'bleu': 0.69, 'rouge': 0.66, 'perplexity': 14.1, 'bert_score': 0.78},\n",
        "    'Gemini': {'bleu': 0.76, 'rouge': 0.73, 'perplexity': 9.7, 'bert_score': 0.85}\n",
        "}\n",
        "\n",
        "# Convertendo para DataFrame\n",
        "metrics_df = pd.DataFrame(models_data).T\n",
        "print(\"🎯 COMPARAÇÃO DE MÉTRICAS ESPECÍFICAS PARA LLMs\")\n",
        "print(\"=\" * 60)\n",
        "print(metrics_df.round(3))\n",
        "\n",
        "# Normalizando perplexity (invertendo porque menor é melhor)\n",
        "max_perplexity = metrics_df['perplexity'].max()\n",
        "metrics_df['perplexity_norm'] = (max_perplexity - metrics_df['perplexity']) / max_perplexity\n",
        "\n",
        "# Calculando score composto\n",
        "score_cols = ['bleu', 'rouge', 'perplexity_norm', 'bert_score']\n",
        "metrics_df['score_composto'] = metrics_df[score_cols].mean(axis=1)\n",
        "\n",
        "print(\"\\n🏆 RANKING FINAL (Score Composto):\")\n",
        "ranking = metrics_df.sort_values('score_composto', ascending=False)[['score_composto']]\n",
        "for i, (model, score) in enumerate(ranking.iterrows(), 1):\n",
        "    print(f\"{i}º lugar: {model} - {score['score_composto']:.3f}\")\n",
        "\n",
        "print(\"\\n💡 Score composto = média de todas as métricas normalizadas\")\n",
        "print(\"📊 Perplexity foi invertida (menor é melhor)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualização radar das métricas\n",
        "import math\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
        "\n",
        "# Métricas para o radar\n",
        "metrics = ['BLEU', 'ROUGE', 'Perplexity\\n(norm)', 'BERTScore']\n",
        "angles = [n / float(len(metrics)) * 2 * math.pi for n in range(len(metrics))]\n",
        "angles += angles[:1]  # Completar o círculo\n",
        "\n",
        "# Cores para cada modelo\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']\n",
        "\n",
        "# Plotar cada modelo\n",
        "for i, (model, color) in enumerate(zip(metrics_df.index, colors)):\n",
        "    values = [\n",
        "        metrics_df.loc[model, 'bleu'],\n",
        "        metrics_df.loc[model, 'rouge'], \n",
        "        metrics_df.loc[model, 'perplexity_norm'],\n",
        "        metrics_df.loc[model, 'bert_score']\n",
        "    ]\n",
        "    values += values[:1]  # Completar o círculo\n",
        "    \n",
        "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=color)\n",
        "    ax.fill(angles, values, alpha=0.1, color=color)\n",
        "\n",
        "# Configurações do gráfico\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_title('🎯 Comparação Radar das Métricas LLM\\n', size=16, pad=20)\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"📊 Gráfico radar criado!\")\n",
        "print(\"🎯 Quanto mais longe do centro, melhor o desempenho\")\n",
        "print(\"💡 Facilita visualizar pontos fortes e fracos de cada modelo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 👥 Avaliação Humana: Quando a Máquina Precisa da Opinião Humana\n\nTá, galera, nem tudo na vida pode ser medido por números! Imaginem avaliar um poema só contando palavras... 🤔\n\n### Quando Precisamos de Humanos?\n\n1. **Criatividade** - \"Esse texto é interessante?\"\n2. **Adequação Cultural** - \"Isso faz sentido no Brasil?\"\n3. **Ética e Vieses** - \"Isso pode ofender alguém?\"\n4. **Nuances Sutis** - \"O tom está adequado?\"\n\n### 🎯 Métodos de Avaliação Humana\n\n**📊 Escalas Likert**  \n\"De 1 a 5, quão boa é essa resposta?\"  \n\n**⚖️ Comparação Pareada**  \n\"Entre A e B, qual resposta é melhor?\"  \n\n**📝 Avaliação Qualitativa**  \n\"Descreva em suas palavras o que achou\"\n\n**💡 Dica do Pedro:** Avaliação humana é cara e demorada, mas às vezes é a ÚNICA forma de saber se seu modelo realmente está bom!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulando dados de avaliação humana\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulando avaliações humanas para diferentes aspectos\n",
        "human_eval_data = {\n",
        "    'Modelo': ['GPT-3.5', 'GPT-4', 'Claude-2', 'Llama-2', 'Gemini'],\n",
        "    'Fluência': np.random.normal(loc=[7.2, 8.5, 7.8, 6.9, 8.1], scale=0.5, size=5).clip(1, 10),\n",
        "    'Relevância': np.random.normal(loc=[7.0, 8.3, 7.6, 6.7, 7.9], scale=0.6, size=5).clip(1, 10),\n",
        "    'Criatividade': np.random.normal(loc=[6.8, 8.1, 7.9, 6.5, 7.7], scale=0.7, size=5).clip(1, 10),\n",
        "    'Segurança': np.random.normal(loc=[8.1, 9.0, 8.6, 7.8, 8.4], scale=0.4, size=5).clip(1, 10),\n",
        "    'Satisfação_Geral': np.random.normal(loc=[7.1, 8.4, 7.8, 6.8, 8.0], scale=0.5, size=5).clip(1, 10)\n",
        "}\n",
        "\n",
        "human_df = pd.DataFrame(human_eval_data)\n",
        "human_df.iloc[:, 1:] = human_df.iloc[:, 1:].round(1)\n",
        "\n",
        "print(\"👥 AVALIAÇÃO HUMANA DOS MODELOS\")\n",
        "print(\"=\" * 50)\n",
        "print(\"📊 Escala: 1-10 (onde 10 = excelente)\")\n",
        "print(\"\\n\", human_df.to_string(index=False))\n",
        "\n",
        "# Calculando score médio da avaliação humana\n",
        "human_metrics = ['Fluência', 'Relevância', 'Criatividade', 'Segurança']\n",
        "human_df['Score_Humano'] = human_df[human_metrics].mean(axis=1)\n",
        "\n",
        "print(\"\\n🏆 RANKING POR AVALIAÇÃO HUMANA:\")\n",
        "human_ranking = human_df.sort_values('Score_Humano', ascending=False)[['Modelo', 'Score_Humano', 'Satisfação_Geral']]\n",
        "for i, row in human_ranking.iterrows():\n",
        "    print(f\"{list(human_ranking.index).index(i)+1}º {row['Modelo']}: {row['Score_Humano']:.1f} (Satisfação: {row['Satisfação_Geral']:.1f})\")\n",
        "\n",
        "print(\"\\n💡 Note que a ordem pode ser diferente das métricas automáticas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparando avaliação automática vs humana\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gráfico 1: Avaliação Humana por aspecto\n",
        "human_melted = human_df.melt(id_vars=['Modelo'], \n",
        "                            value_vars=human_metrics,\n",
        "                            var_name='Aspecto', \n",
        "                            value_name='Score')\n",
        "\n",
        "sns.boxplot(data=human_melted, x='Aspecto', y='Score', ax=ax1)\n",
        "ax1.set_title('👥 Distribuição das Avaliações Humanas', fontsize=14)\n",
        "ax1.set_ylabel('Score (1-10)')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Gráfico 2: Correlação entre avaliação automática e humana\n",
        "# Combinando dados para comparação\n",
        "combined_scores = pd.DataFrame({\n",
        "    'Modelo': metrics_df.index,\n",
        "    'Score_Automático': metrics_df['score_composto'].values,\n",
        "    'Score_Humano': human_df.set_index('Modelo')['Score_Humano'].values\n",
        "})\n",
        "\n",
        "ax2.scatter(combined_scores['Score_Automático'], combined_scores['Score_Humano'], \n",
        "           s=100, alpha=0.7, c=colors)\n",
        "\n",
        "# Adicionando labels dos modelos\n",
        "for i, row in combined_scores.iterrows():\n",
        "    ax2.annotate(row['Modelo'], \n",
        "                (row['Score_Automático'], row['Score_Humano']),\n",
        "                xytext=(5, 5), textcoords='offset points')\n",
        "\n",
        "ax2.set_xlabel('Score Automático')\n",
        "ax2.set_ylabel('Score Humano')\n",
        "ax2.set_title('🤖 vs 👥 Correlação Auto x Humano', fontsize=14)\n",
        "\n",
        "# Linha de tendência\n",
        "z = np.polyfit(combined_scores['Score_Automático'], combined_scores['Score_Humano'], 1)\n",
        "p = np.poly1d(z)\n",
        "ax2.plot(combined_scores['Score_Automático'], p(combined_scores['Score_Automático']), \"r--\", alpha=0.8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculando correlação\n",
        "correlation = combined_scores['Score_Automático'].corr(combined_scores['Score_Humano'])\n",
        "print(f\"📊 Correlação entre avaliação automática e humana: {correlation:.3f}\")\n",
        "print(f\"💡 Correlação de {correlation:.1f} indica {'forte' if abs(correlation) > 0.7 else 'moderada' if abs(correlation) > 0.5 else 'fraca'} concordância\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ⚡ Avaliação de Performance: Velocidade e Eficiência\n\nNão adianta ter um modelo super inteligente se ele demora 1 hora para responder \"oi\"! 😅\n\n### Métricas Técnicas Importantes:\n\n**⏱️ Latência**  \nTempo entre fazer a pergunta e receber a resposta\n\n**🚀 Throughput**  \nQuantas requisições por segundo consegue processar\n\n**💾 Uso de Memória**  \nQuanto de RAM/VRAM consome\n\n**⚡ Tokens por Segundo**  \nVelocidade de geração de texto\n\n**💰 Custo por Token**  \nQuanto custa cada palavra gerada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulando métricas de performance\n",
        "import time\n",
        "\n",
        "performance_data = {\n",
        "    'Modelo': ['GPT-3.5-Turbo', 'GPT-4', 'Claude-2', 'Llama-2-70B', 'Gemini-Pro'],\n",
        "    'Latência_ms': [850, 1200, 950, 2100, 750],\n",
        "    'Throughput_req_s': [45, 25, 38, 12, 52],\n",
        "    'Tokens_por_segundo': [120, 80, 95, 35, 140],\n",
        "    'Memória_GB': [12, 28, 16, 140, 20],\n",
        "    'Custo_por_1k_tokens': [0.002, 0.06, 0.008, 0.001, 0.003]\n",
        "}\n",
        "\n",
        "perf_df = pd.DataFrame(performance_data)\n",
        "\n",
        "print(\"⚡ MÉTRICAS DE PERFORMANCE\")\n",
        "print(\"=\" * 50)\n",
        "print(perf_df.to_string(index=False))\n",
        "\n",
        "# Calculando score de performance normalizado\n",
        "# Para métricas onde MENOR é melhor (latência, custo, memória)\n",
        "perf_df['latencia_score'] = (perf_df['Latência_ms'].max() - perf_df['Latência_ms']) / perf_df['Latência_ms'].max()\n",
        "perf_df['memoria_score'] = (perf_df['Memória_GB'].max() - perf_df['Memória_GB']) / perf_df['Memória_GB'].max()\n",
        "perf_df['custo_score'] = (perf_df['Custo_por_1k_tokens'].max() - perf_df['Custo_por_1k_tokens']) / perf_df['Custo_por_1k_tokens'].max()\n",
        "\n",
        "# Para métricas onde MAIOR é melhor (throughput, tokens/s)\n",
        "perf_df['throughput_score'] = perf_df['Throughput_req_s'] / perf_df['Throughput_req_s'].max()\n",
        "perf_df['velocidade_score'] = perf_df['Tokens_por_segundo'] / perf_df['Tokens_por_segundo'].max()\n",
        "\n",
        "# Score composto de performance\n",
        "perf_metrics = ['latencia_score', 'throughput_score', 'velocidade_score', 'memoria_score', 'custo_score']\n",
        "perf_df['performance_score'] = perf_df[perf_metrics].mean(axis=1)\n",
        "\n",
        "print(\"\\n🏆 RANKING DE PERFORMANCE:\")\n",
        "perf_ranking = perf_df.sort_values('performance_score', ascending=False)[['Modelo', 'performance_score']]\n",
        "for i, (_, row) in enumerate(perf_ranking.iterrows(), 1):\n",
        "    print(f\"{i}º {row['Modelo']}: {row['performance_score']:.3f}\")\n",
        "\n",
        "print(\"\\n💡 Score considera velocidade, eficiência e custo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎓 EXERCÍCIO PRÁTICO 1: Monte Sua Própria Avaliação\n\nAgora é sua vez! Vamos criar uma avaliação personalizada para um caso específico.\n\n**📝 CENÁRIO:**  \nVocê precisa escolher um LLM para um chatbot de atendimento ao cliente de uma loja online brasileira.\n\n**🎯 CRITÉRIOS IMPORTANTES:**\n1. Entendimento de português brasileiro\n2. Velocidade de resposta (cliente não gosta de esperar)\n3. Cortesia e tom adequado\n4. Custo operacional\n5. Capacidade de lidar com reclamações\n\n**📊 SUA TAREFA:**\nComplete o código abaixo definindo pesos para cada critério e implementando a função de avaliação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÍCIO 1: Complete o código para criar sua avaliação personalizada\n",
        "\n",
        "def avaliar_modelo_chatbot(modelo_name, criterios_scores, pesos):\n",
        "    \"\"\"\n",
        "    Avalia um modelo para chatbot baseado em critérios específicos\n",
        "    \n",
        "    Args:\n",
        "        modelo_name: Nome do modelo\n",
        "        criterios_scores: Dict com scores de cada critério (0-10)\n",
        "        pesos: Dict com pesos de cada critério (devem somar 1.0)\n",
        "    \n",
        "    Returns:\n",
        "        Score final ponderado\n",
        "    \"\"\"\n",
        "    # TODO: Implemente a lógica de cálculo do score ponderado\n",
        "    score_final = 0\n",
        "    \n",
        "    # DICA: Para cada critério, multiplique o score pelo peso\n",
        "    # SEU CÓDIGO AQUI:\n",
        "    \n",
        "    \n",
        "    return score_final\n",
        "\n",
        "# Dados dos modelos (já preenchidos)\n",
        "modelos_chatbot = {\n",
        "    'GPT-3.5-Turbo': {\n",
        "        'portugues_br': 8.5,\n",
        "        'velocidade': 9.0,\n",
        "        'cortesia': 8.0,\n",
        "        'custo': 8.5,\n",
        "        'reclamacoes': 7.5\n",
        "    },\n",
        "    'GPT-4': {\n",
        "        'portugues_br': 9.5,\n",
        "        'velocidade': 7.0,\n",
        "        'cortesia': 9.0,\n",
        "        'custo': 5.0,\n",
        "        'reclamacoes': 9.5\n",
        "    },\n",
        "    'Claude-2': {\n",
        "        'portugues_br': 8.0,\n",
        "        'velocidade': 8.0,\n",
        "        'cortesia': 9.5,\n",
        "        'custo': 7.0,\n",
        "        'reclamacoes': 8.5\n",
        "    }\n",
        "}\n",
        "\n",
        "# TODO: Defina os pesos (devem somar 1.0)\n",
        "# Pense: o que é mais importante para um chatbot?\n",
        "pesos = {\n",
        "    'portugues_br': 0.0,    # SEU PESO AQUI (ex: 0.25 = 25%)\n",
        "    'velocidade': 0.0,      # SEU PESO AQUI\n",
        "    'cortesia': 0.0,       # SEU PESO AQUI\n",
        "    'custo': 0.0,          # SEU PESO AQUI\n",
        "    'reclamacoes': 0.0     # SEU PESO AQUI\n",
        "    # TOTAL DEVE SER 1.0!\n",
        "}\n",
        "\n",
        "print(\"🤖 AVALIAÇÃO DE MODELOS PARA CHATBOT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Testando sua implementação\n",
        "if sum(pesos.values()) == 1.0:\n",
        "    for modelo, scores in modelos_chatbot.items():\n",
        "        score = avaliar_modelo_chatbot(modelo, scores, pesos)\n",
        "        print(f\"{modelo}: {score:.2f}\")\n",
        "else:\n",
        "    print(\"❌ ERRO: Os pesos devem somar 1.0!\")\n",
        "    print(f\"Soma atual: {sum(pesos.values()):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚨 Problemas Comuns na Avaliação: Os \"Pegadinhas\" que Todo Mundo Cai\n\nGalera, avaliar LLMs tem suas armadilhas! É tipo dirigir no Rio... tem que conhecer os \"furos\" 😄\n\n### 🎭 1. Data Leakage (Vazamento de Dados)\n**O Problema:** Seu modelo \"decorou\" as respostas do teste  \n**Analogia:** É tipo fazer prova tendo visto as respostas antes\n\n### 📊 2. Métricas que Mentem\n**O Problema:** BLEU alto não significa texto bom  \n**Exemplo:** \"Gato gato gato gato\" pode ter BLEU alto se a referência for \"O gato\"\n\n### 🎯 3. Overfitting nos Benchmarks\n**O Problema:** Modelo vai bem no GLUE mas falha na vida real  \n**Analogia:** Decorar questões do ENEM mas não saber aplicar na faculdade\n\n### 👥 4. Viés dos Avaliadores\n**O Problema:** Humanos têm preferências pessoais  \n**Solução:** Múltiplos avaliadores + guidelines claros\n\n### ⚖️ 5. Desbalanceamento de Classes\n**O Problema:** 90% das amostras são positivas  \n**Resultado:** Modelo \"aprende\" a sempre responder positivo\n\n**💡 Dica do Pedro:** Nunca confie numa métrica só! É tipo namorar baseado só numa foto do Instagram... você precisa conhecer a pessoa toda! 😂"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrando problemas comuns na avaliação\n",
        "\n",
        "# Problema 1: Desbalanceamento de classes\n",
        "print(\"🚨 DEMONSTRAÇÃO: PROBLEMAS NA AVALIAÇÃO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Simulando dataset desbalanceado\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# 90% positivo, 10% negativo (muito desbalanceado!)\n",
        "true_labels_unbalanced = np.random.choice(['positivo', 'negativo'], \n",
        "                                         n_samples, p=[0.9, 0.1])\n",
        "\n",
        "# Modelo \"preguiçoso\" que sempre prediz \"positivo\"\n",
        "lazy_predictions = ['positivo'] * n_samples\n",
        "\n",
        "# Calculando métricas\n",
        "accuracy_lazy = accuracy_score(true_labels_unbalanced, lazy_predictions)\n",
        "\n",
        "print(\"📊 CASO 1: Dataset Desbalanceado\")\n",
        "print(f\"Distribuição real: {np.unique(true_labels_unbalanced, return_counts=True)}\")\n",
        "print(f\"Modelo preguiçoso (sempre 'positivo'): {accuracy_lazy:.1%} de acurácia!\")\n",
        "print(\"❌ PROBLEMA: Alta acurácia mas modelo inútil!\")\n",
        "\n",
        "# Problema 2: Métricas enganosas\n",
        "print(\"\\n📊 CASO 2: Métrica Enganosa (BLEU)\")\n",
        "\n",
        "referencia = \"O gato subiu no telhado\"\n",
        "texto_repetitivo = \"gato gato gato gato gato\"\n",
        "texto_bom = \"O felino escalou o teto\"\n",
        "\n",
        "bleu_repetitivo = calculate_bleu_simple(referencia, texto_repetitivo)\n",
        "bleu_bom = calculate_bleu_simple(referencia, texto_bom)\n",
        "\n",
        "print(f\"Referência: '{referencia}'\")\n",
        "print(f\"Texto repetitivo: '{texto_repetitivo}' → BLEU: {bleu_repetitivo:.3f}\")\n",
        "print(f\"Texto bom: '{texto_bom}' → BLEU: {bleu_bom:.3f}\")\n",
        "print(\"❌ PROBLEMA: Texto repetitivo pode ter BLEU alto!\")\n",
        "\n",
        "# Problema 3: Falta de contexto\n",
        "print(\"\\n📊 CASO 3: Falta de Avaliação Contextual\")\n",
        "exemplos_contexto = [\n",
        "    {\n",
        "        'contexto': 'Cliente reclamando de produto quebrado',\n",
        "        'resposta_tecnica': 'Produto apresenta falha estrutural tipo B-304',\n",
        "        'resposta_humana': 'Que chato! Vamos resolver isso rapidinho para você'\n",
        "    },\n",
        "    {\n",
        "        'contexto': 'Dúvida sobre frete',\n",
        "        'resposta_tecnica': 'Modalidade de envio padrão demanda 3-5 dias úteis',\n",
        "        'resposta_humana': 'Seu pedido chega em 3-5 dias úteis pelo correio!'\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, ex in enumerate(exemplos_contexto, 1):\n",
        "    print(f\"\\nExemplo {i}: {ex['contexto']}\")\n",
        "    print(f\"  Técnica: {ex['resposta_tecnica']}\")\n",
        "    print(f\"  Humana:  {ex['resposta_humana']}\")\n",
        "\n",
        "print(\"\\n❌ PROBLEMA: Métricas automáticas não capturam adequação ao contexto!\")\n",
        "print(\"✅ SOLUÇÃO: Combinar métricas automáticas + avaliação humana contextual\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📈 Monitoramento Contínuo: Mantendo o Olho no Modelo\n\nLembram do módulo sobre prompting? Lá vimos como fazer o modelo funcionar. Agora precisamos garantir que ele CONTINUE funcionando!\n\nÉ tipo cuidar de um filho: não basta educar uma vez, tem que acompanhar sempre! 👶\n\n### 🔍 O que Monitorar?\n\n1. **Drift de Performance** - Modelo piorando com o tempo\n2. **Mudanças no Comportamento** - Respostas ficando estranhas\n3. **Feedback dos Usuários** - Satisfação caindo\n4. **Métricas Técnicas** - Latência aumentando\n5. **Custos** - Conta do OpenAI explodindo 💸\n\n### 🎯 Estratégias de Monitoramento\n\n**📊 Dashboards em Tempo Real**  \nGráficos bonitos que mostram tudo funcionando (ou não)\n\n**🚨 Alertas Automáticos**  \n\"Opa, algo tá estranho aqui!\"\n\n**📝 A/B Testing Contínuo**  \nSempre testando versões diferentes\n\n**👥 Feedback Loop**  \nUsuários ajudam a melhorar o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulando sistema de monitoramento contínuo\n",
        "import datetime\n",
        "\n",
        "# Gerando dados de monitoramento ao longo do tempo\n",
        "np.random.seed(42)\n",
        "days = 30\n",
        "dates = [datetime.datetime.now() - datetime.timedelta(days=x) for x in range(days, 0, -1)]\n",
        "\n",
        "# Simulando degradação gradual do modelo\n",
        "base_accuracy = 0.85\n",
        "degradation_factor = 0.001  # Piora 0.1% por dia\n",
        "\n",
        "monitoring_data = []\n",
        "for i, date in enumerate(dates):\n",
        "    # Performance degradando com o tempo + ruído aleatório\n",
        "    daily_accuracy = base_accuracy - (i * degradation_factor) + np.random.normal(0, 0.02)\n",
        "    daily_latency = 800 + (i * 10) + np.random.normal(0, 50)  # Latência aumentando\n",
        "    daily_cost = 100 + (i * 2) + np.random.normal(0, 10)      # Custo aumentando\n",
        "    daily_satisfaction = 8.5 - (i * 0.05) + np.random.normal(0, 0.3)  # Satisfação caindo\n",
        "    \n",
        "    monitoring_data.append({\n",
        "        'data': date,\n",
        "        'acuracia': max(0.6, min(1.0, daily_accuracy)),\n",
        "        'latencia_ms': max(500, daily_latency),\n",
        "        'custo_diario': max(50, daily_cost),\n",
        "        'satisfacao': max(1, min(10, daily_satisfaction))\n",
        "    })\n",
        "\n",
        "monitoring_df = pd.DataFrame(monitoring_data)\n",
        "\n",
        "print(\"📊 MONITORAMENTO CONTÍNUO - ÚLTIMOS 30 DIAS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Estatísticas resumo\n",
        "print(f\"📈 Acurácia: {monitoring_df['acuracia'].iloc[0]:.1%} → {monitoring_df['acuracia'].iloc[-1]:.1%}\")\n",
        "print(f\"⏱️  Latência: {monitoring_df['latencia_ms'].iloc[0]:.0f}ms → {monitoring_df['latencia_ms'].iloc[-1]:.0f}ms\")\n",
        "print(f\"💰 Custo: ${monitoring_df['custo_diario'].iloc[0]:.0f} → ${monitoring_df['custo_diario'].iloc[-1]:.0f}\")\n",
        "print(f\"😊 Satisfação: {monitoring_df['satisfacao'].iloc[0]:.1f} → {monitoring_df['satisfacao'].iloc[-1]:.1f}\")\n",
        "\n",
        "# Detectando problemas\n",
        "accuracy_drop = monitoring_df['acuracia'].iloc[0] - monitoring_df['acuracia'].iloc[-1]\n",
        "latency_increase = monitoring_df['latencia_ms'].iloc[-1] - monitoring_df['latencia_ms'].iloc[0]\n",
        "cost_increase = monitoring_df['custo_diario'].iloc[-1] - monitoring_df['custo_diario'].iloc[0]\n",
        "\n",
        "print(\"\\n🚨 ALERTAS DETECTADOS:\")\n",
        "if accuracy_drop > 0.05:\n",
        "    print(f\"⚠️  ALERTA: Queda de acurácia de {accuracy_drop:.1%}!\")\n",
        "if latency_increase > 200:\n",
        "    print(f\"⚠️  ALERTA: Aumento de latência de {latency_increase:.0f}ms!\")\n",
        "if cost_increase > 50:\n",
        "    print(f\"⚠️  ALERTA: Aumento de custo de ${cost_increase:.0f}!\")\n",
        "\n",
        "print(\"\\n💡 Modelo precisa de atenção!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando o monitoramento\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Gráfico 1: Acurácia ao longo do tempo\n",
        "ax1.plot(monitoring_df['data'], monitoring_df['acuracia'], 'b-', linewidth=2, marker='o')\n",
        "ax1.axhline(y=0.8, color='r', linestyle='--', alpha=0.7, label='Limite mínimo')\n",
        "ax1.set_title('📊 Acurácia ao Longo do Tempo')\n",
        "ax1.set_ylabel('Acurácia')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 2: Latência\n",
        "ax2.plot(monitoring_df['data'], monitoring_df['latencia_ms'], 'orange', linewidth=2, marker='s')\n",
        "ax2.axhline(y=1000, color='r', linestyle='--', alpha=0.7, label='Limite máximo')\n",
        "ax2.set_title('⏱️ Latência ao Longo do Tempo')\n",
        "ax2.set_ylabel('Latência (ms)')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 3: Custo\n",
        "ax3.plot(monitoring_df['data'], monitoring_df['custo_diario'], 'green', linewidth=2, marker='^')\n",
        "ax3.set_title('💰 Custo Diário')\n",
        "ax3.set_ylabel('Custo ($)')\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 4: Satisfação do usuário\n",
        "ax4.plot(monitoring_df['data'], monitoring_df['satisfacao'], 'purple', linewidth=2, marker='D')\n",
        "ax4.axhline(y=7.0, color='r', linestyle='--', alpha=0.7, label='Limite mínimo')\n",
        "ax4.set_title('😊 Satisfação dos Usuários')\n",
        "ax4.set_ylabel('Satisfação (1-10)')\n",
        "ax4.tick_params(axis='x', rotation=45)\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"📊 Dashboard de monitoramento gerado!\")\n",
        "print(\"🎯 Visualização clara das tendências ao longo do tempo\")\n",
        "print(\"🚨 Linhas vermelhas mostram limites de alerta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎓 EXERCÍCIO PRÁTICO 2: Detecção de Problemas\n\nAgora você é o \"médico\" do modelo! Analise os sintomas e detecte o problema.\n\n**📋 CENÁRIO:**  \nVocê monitora um modelo de classificação de sentimentos para reviews de e-commerce. Nos últimos dias, algo estranho está acontecendo...\n\n**🔍 SUA MISSÃO:**  \nAnalise os dados e identifique possíveis problemas!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÍCIO 2: Analise os dados e detecte problemas\n",
        "\n",
        "# Dados suspeitos dos últimos 7 dias\n",
        "dados_suspeitos = {\n",
        "    'dia': ['Segunda', 'Terça', 'Quarta', 'Quinta', 'Sexta', 'Sábado', 'Domingo'],\n",
        "    'total_classificacoes': [1000, 1200, 1100, 1300, 2800, 3200, 2900],\n",
        "    'predito_positivo': [600, 720, 660, 780, 2520, 2880, 2610],\n",
        "    'predito_negativo': [300, 360, 330, 390, 210, 240, 218],\n",
        "    'predito_neutro': [100, 120, 110, 130, 70, 80, 72],\n",
        "    'acuracia_reportada': [0.85, 0.83, 0.86, 0.84, 0.92, 0.94, 0.93],\n",
        "    'tempo_resposta_ms': [450, 480, 460, 470, 180, 160, 170]\n",
        "}\n",
        "\n",
        "suspeitos_df = pd.DataFrame(dados_suspeitos)\n",
        "\n",
        "print(\"🔍 DADOS DOS ÚLTIMOS 7 DIAS\")\n",
        "print(\"=\" * 50)\n",
        "print(suspeitos_df.to_string(index=False))\n",
        "\n",
        "# Calculando proporções\n",
        "suspeitos_df['prop_positivo'] = suspeitos_df['predito_positivo'] / suspeitos_df['total_classificacoes']\n",
        "suspeitos_df['prop_negativo'] = suspeitos_df['predito_negativo'] / suspeitos_df['total_classificacoes']\n",
        "suspeitos_df['prop_neutro'] = suspeitos_df['predito_neutro'] / suspeitos_df['total_classificacoes']\n",
        "\n",
        "print(\"\\n📊 PROPORÇÕES:\")\n",
        "print(suspeitos_df[['dia', 'prop_positivo', 'prop_negativo', 'prop_neutro']].round(3).to_string(index=False))\n",
        "\n",
        "# SUA ANÁLISE AQUI:\n",
        "print(\"\\n🤔 ANÁLISE - O que você observa?\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# TODO: Analise os dados e responda:\n",
        "# 1. O que aconteceu na sexta/sábado/domingo?\n",
        "# 2. Por que a acurácia subiu mas algo parece estranho?\n",
        "# 3. O que sugere sobre o comportamento do modelo?\n",
        "# 4. Qual seria sua recomendação?\n",
        "\n",
        "print(\"📝 SUAS OBSERVAÇÕES:\")\n",
        "print(\"1. Volume de classificações: \")\n",
        "print(\"2. Distribuição das classes: \")\n",
        "print(\"3. Acurácia vs comportamento: \")\n",
        "print(\"4. Tempo de resposta: \")\n",
        "print(\"\\n🎯 DIAGNÓSTICO PROVÁVEL:\")\n",
        "print(\"Problema detectado: \")\n",
        "print(\"Causa provável: \")\n",
        "print(\"Ação recomendada: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Conectando com o Restante do Curso\n\nLembram da nossa jornada até aqui? Vamos conectar os pontos! 🔗\n\n### 🔙 Módulos Anteriores que se Conectam:\n\n**📚 Módulo 2 - O que são LLMs**  \n→ Agora sabemos MEDIR se eles realmente \"entendem\"\n\n**🏗️ Módulo 3 - Arquitetura Transformer**  \n→ Diferentes arquiteturas têm diferentes pontos fortes nos benchmarks\n\n**🔤 Módulo 4 - Tokens**  \n→ Métricas como BLEU dependem de tokenização adequada\n\n**🎯 Módulo 8 - Prompting**  \n→ Avaliação nos mostra se nossos prompts estão funcionando!\n\n### 🔜 Preparando para os Próximos Módulos:\n\n**🛡️ Módulo 10 - Segurança**  \n→ Vamos avaliar se modelos são seguros e éticos\n\n**⚠️ Módulo 11 - Limitações**  \n→ Entender onde os modelos falham mesmo com boa avaliação\n\n**🚀 Módulo 12 - Projeto Final**  \n→ Aplicar tudo que aprendemos sobre avaliação!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumo visual das conexões do curso\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Dados para o mapa de conexões\n",
        "modulos = ['Setup', 'LLMs', 'Transformer', 'Tokens', 'Embeddings', \n",
        "          'Tipos', 'Treinamento', 'Prompting', 'AVALIAÇÃO', 'Segurança', \n",
        "          'Limitações', 'Projeto', 'Avançado']\n",
        "\n",
        "conexoes_avaliacao = [2, 4, 6, 8, 5, 7, 9, 10, 10, 8, 9, 9, 6]\n",
        "cores = ['lightgray' if i != 8 else 'red' for i in range(len(modulos))]\n",
        "\n",
        "bars = ax.barh(modulos, conexoes_avaliacao, color=cores)\n",
        "\n",
        "# Destacando o módulo atual\n",
        "ax.set_xlabel('Nível de Conexão com Avaliação')\n",
        "ax.set_title('🔗 Como \"Avaliação de Modelos\" se Conecta com Outros Módulos\\n', fontsize=14)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for i, bar in enumerate(bars):\n",
        "    width = bar.get_width()\n",
        "    emoji = '🎯' if i == 8 else '📚' if i < 8 else '🔜'\n",
        "    ax.text(width + 0.1, bar.get_y() + bar.get_height()/2, \n",
        "            f'{emoji} {width}', va='center')\n",
        "\n",
        "ax.set_xlim(0, 11)\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"🎯 MÓDULO ATUAL: Avaliação de Modelos\")\n",
        "print(\"📚 JÁ ESTUDADOS: Bases sólidas para entender avaliação\")\n",
        "print(\"🔜 PRÓXIMOS: Vamos usar avaliação para segurança e projetos\")\n",
        "print(\"\\n💡 Avaliação é o 'termômetro' do nosso conhecimento sobre LLMs!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎉 Resumão: O que Aprendemos Hoje\n\nBora fazer aquele resumão maroto do que rolou hoje! 📝\n\n### ✅ CONCEITOS FUNDAMENTAIS\n- **Avaliação é essencial** - Sem ela, não sabemos se o modelo funciona\n- **Multiple métricas** - Nunca confie numa métrica só!\n- **Contexto importa** - Métricas automáticas não capturam tudo\n- **Monitoramento contínuo** - Modelo pode degradar com o tempo\n\n### 🛠️ FERRAMENTAS PRÁTICAS\n- **Métricas clássicas:** Acurácia, Precisão, Recall, F1-Score\n- **Métricas para LLM:** BLEU, ROUGE, Perplexidade, BERTScore\n- **Benchmarks famosos:** GLUE, HellaSwag, SQuAD, GSM8K\n- **Avaliação humana:** Quando máquinas não bastam\n- **Monitoramento:** Dashboards, alertas, A/B testing\n\n### 🚨 ARMADILHAS A EVITAR\n- Data leakage (modelo \"decorou\" as respostas)\n- Métricas enganosas (BLEU alto ≠ texto bom)\n- Overfitting em benchmarks\n- Desbalanceamento de classes\n- Falta de contexto na avaliação\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdução-à-llms-modulo-09_img_04.png)\n\n### 🎯 DICA FINAL DO PEDRO\n**Avaliação de modelos é como cuidar da saúde:** você não vai no médico só quando está doente, né? Tem que fazer check-up regular!\n\nAssim é com LLMs: monitore sempre, use múltiplas métricas, e nunca esqueça que números não contam toda a história. Às vezes o modelo com menor BLEU é mais útil na prática!\n\n---\n\n**🚀 PRÓXIMO MÓDULO:** Segurança e Guardrails  \n*Onde vamos aprender a \"colocar cabresto\" nos nossos modelos para eles não saírem falando besteira por aí!* 😄\n\n**Até lá, pessoal! Bora avaliar uns modelos! 🎯**"
      ]
    }
  ]
}