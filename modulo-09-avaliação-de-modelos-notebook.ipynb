{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Avalia√ß√£o de Modelos LLM: Como Saber se Nosso \"Filho\" est√° Indo Bem na Escola da IA\n\n## M√≥dulo 9 - Curso Introdu√ß√£o √† LLMs\n\n**Instrutor:** Pedro Nunes Guth  \n**Tema:** M√©tricas, benchmarks e t√©cnicas para avaliar nossos LLMs\n\n---\n\nE a√≠, pessoal! üöÄ\n\nImagina que voc√™ √© pai/m√£e de um filho que est√° na escola. Como voc√™ sabe se ele est√° aprendendo direito? Voc√™ olha as notas, conversa com os professores, v√™ se ele consegue aplicar o que aprendeu no dia a dia, n√©?\n\nCom LLMs √© a MESMA COISA! Depois de treinar nossos modelos (lembram do m√≥dulo 7?), precisamos saber: **\"Ser√° que esse bichinho est√° funcionando direito?\"**\n\nBora descobrir as melhores formas de avaliar nossos modelos! üéØ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Vamos preparar nosso ambiente!\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√µes para gr√°ficos bonitos\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(\"üéØ Ambiente configurado! Bora avaliar alguns modelos!\")\n",
        "print(\"üìä Bibliotecas carregadas com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î T√°, mas o que √© Avalia√ß√£o de Modelos?\n\nGalera, avaliar um modelo √© como avaliar um chef de cozinha. Voc√™ n√£o vai s√≥ perguntar \"voc√™ sabe cozinhar?\". Voc√™ vai:\n\n1. **Testar receitas espec√≠ficas** (tarefas espec√≠ficas)\n2. **Ver a consist√™ncia** (ele faz o mesmo prato sempre igual?)\n3. **Medir o tempo** (efici√™ncia)\n4. **Pedir opini√£o de diferentes pessoas** (m√∫ltiplos avaliadores)\n5. **Comparar com outros chefs** (benchmarks)\n\n### Por que √© T√ÉO importante?\n\nLembram do m√≥dulo 8 sobre prompting? L√° vimos como **fazer** o modelo funcionar melhor. Agora precisamos **medir** se realmente funcionou!\n\n**Sem avalia√ß√£o adequada:**\n- üö´ Voc√™ n√£o sabe se seu modelo est√° melhorando\n- üö´ Pode estar gastando dinheiro √† toa\n- üö´ Usu√°rios podem ter experi√™ncias ruins\n- üö´ Seu chefe vai te perguntar \"t√°, mas funciona mesmo?\"\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-09_img_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Tipos de Avalia√ß√£o: O Card√°pio Completo\n\nAssim como num restaurante tem entrada, prato principal e sobremesa, na avalia√ß√£o de LLMs temos diferentes \"pratos\":\n\n### 1. Avalia√ß√£o Intr√≠nseca vs Extr√≠nseca\n\n**Intr√≠nseca** = Como o modelo se sai em tarefas gerais (tipo prova do ENEM)  \n**Extr√≠nseca** = Como ele se sai na SUA aplica√ß√£o espec√≠fica (tipo prova de medicina)\n\n### 2. Avalia√ß√£o Autom√°tica vs Humana\n\n**Autom√°tica** = Computador corrige (r√°pido, barato, mas √†s vezes \"burro\")  \n**Humana** = Gente real avalia (lento, caro, mas entende nuances)\n\n### 3. Avalia√ß√£o Online vs Offline\n\n**Offline** = Testa antes de colocar em produ√ß√£o  \n**Online** = Monitora enquanto usu√°rios reais usam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um exemplo simples de dados de avalia√ß√£o\n",
        "# Simulando respostas de um modelo de classifica√ß√£o de sentimentos\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulando dados de teste\n",
        "n_samples = 1000\n",
        "true_labels = np.random.choice(['positivo', 'negativo', 'neutro'], n_samples, p=[0.4, 0.3, 0.3])\n",
        "\n",
        "# Simulando predi√ß√µes de 3 modelos diferentes\n",
        "# Modelo A: Bem balanceado\n",
        "modelo_a_accuracy = 0.85\n",
        "modelo_a_preds = []\n",
        "\n",
        "for true_label in true_labels:\n",
        "    if np.random.random() < modelo_a_accuracy:\n",
        "        modelo_a_preds.append(true_label)\n",
        "    else:\n",
        "        # Erro aleat√≥rio\n",
        "        options = ['positivo', 'negativo', 'neutro']\n",
        "        options.remove(true_label)\n",
        "        modelo_a_preds.append(np.random.choice(options))\n",
        "\n",
        "# Criando DataFrame para facilitar an√°lise\n",
        "results_df = pd.DataFrame({\n",
        "    'true_label': true_labels,\n",
        "    'modelo_a_pred': modelo_a_preds\n",
        "})\n",
        "\n",
        "print(\"üìä Dataset de avalia√ß√£o criado!\")\n",
        "print(f\"üìù {len(results_df)} amostras de teste\")\n",
        "print(\"\\nüîç Primeiras 10 linhas:\")\n",
        "print(results_df.head(10))\n",
        "\n",
        "print(\"\\nüìà Distribui√ß√£o das classes:\")\n",
        "print(results_df['true_label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ M√©tricas Cl√°ssicas: Os \"Cl√°ssicos da MPB\" da Avalia√ß√£o\n\nAssim como na m√∫sica brasileira temos cl√°ssicos que todo mundo conhece, na avalia√ß√£o de modelos temos as m√©tricas fundamentais:\n\n### 1. Acur√°cia (Accuracy)\n**\"De 100 tentativas, quantas eu acertei?\"**  \n√â tipo aprova√ß√£o no vestibular: simples e direto!\n\n### 2. Precis√£o (Precision)\n**\"Das vezes que eu disse '√© positivo', quantas realmente eram?\"**  \n√â tipo um detector de mentiras: quando ele fala que √© verdade, geralmente √© mesmo?\n\n### 3. Recall (Revoca√ß√£o)\n**\"De todos os casos positivos, quantos eu consegui encontrar?\"**  \n√â tipo um detetive: consegue achar TODOS os criminosos?\n\n### 4. F1-Score\n**\"A m√©dia harm√¥nica entre precis√£o e recall\"**  \n√â tipo o \"meio termo\" entre ser certeiro e ser abrangente.\n\n**üí° Dica do Pedro:** Use F1 quando voc√™ quer balancear precis√£o e recall. √â tipo escolher entre ser um sniper (alta precis√£o) ou uma metralhadora (alto recall)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculando m√©tricas cl√°ssicas para nosso modelo\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Calculando m√©tricas b√°sicas\n",
        "accuracy = accuracy_score(results_df['true_label'], results_df['modelo_a_pred'])\n",
        "\n",
        "print(\"üéØ M√âTRICAS DO MODELO A\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"üìä Acur√°cia: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
        "print(\"\\nüìã Relat√≥rio Completo:\")\n",
        "print(classification_report(results_df['true_label'], results_df['modelo_a_pred']))\n",
        "\n",
        "# Matriz de Confus√£o\n",
        "print(\"\\nü§î Matriz de Confus√£o:\")\n",
        "cm = confusion_matrix(results_df['true_label'], results_df['modelo_a_pred'])\n",
        "print(cm)\n",
        "\n",
        "# Vamos interpretar\n",
        "print(\"\\nüß† INTERPRETA√á√ÉO:\")\n",
        "print(f\"‚úÖ O modelo acerta {accuracy*100:.1f}% das vezes\")\n",
        "print(\"üìä A matriz mostra onde ele mais erra\")\n",
        "print(\"üéØ F1-score nos d√° uma vis√£o balanceada por classe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a matriz de confus√£o de forma bonita\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Matriz de confus√£o normalizada\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_normalized, \n",
        "            annot=True, \n",
        "            fmt='.2f', \n",
        "            cmap='Blues',\n",
        "            xticklabels=['negativo', 'neutro', 'positivo'],\n",
        "            yticklabels=['negativo', 'neutro', 'positivo'])\n",
        "\n",
        "plt.title('üéØ Matriz de Confus√£o - Modelo A\\n(Valores Normalizados)', fontsize=14)\n",
        "plt.xlabel('Predi√ß√£o do Modelo', fontsize=12)\n",
        "plt.ylabel('Verdade Real', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Matriz de Confus√£o Visualizada!\")\n",
        "print(\"üí° Quanto mais azul escuro, melhor o modelo naquela classe\")\n",
        "print(\"üéØ Diagonal principal = acertos, resto = erros\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèÜ Benchmarks Famosos: O \"Top 10 da Billboard\" dos LLMs\n\nAssim como na m√∫sica temos charts para comparar artistas, no mundo dos LLMs temos benchmarks padr√£o:\n\n### üìö GLUE & SuperGLUE\n**\"O ENEM dos modelos de linguagem\"**  \n- Conjunto de tarefas variadas\n- Compreens√£o de texto, infer√™ncia, similaridade\n- Todo mundo usa para comparar modelos\n\n### üß† HellaSwag\n**\"Teste de senso comum\"**  \n- \"Maria abriu a geladeira e...\" (o que acontece depois?)\n- Testa se o modelo entende o mundo real\n\n### üìñ SQuAD\n**\"Interpreta√ß√£o de texto\"**  \n- L√™ um texto e responde perguntas\n- Tipo prova de portugu√™s do ensino m√©dio\n\n### üî¢ GSM8K\n**\"Matem√°tica de escola\"**  \n- Problemas matem√°ticos em linguagem natural\n- \"Jo√£o tem 5 ma√ß√£s e come 2, quantas sobraram?\"\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-09_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulando resultados de diferentes modelos em benchmarks famosos\n",
        "benchmarks_data = {\n",
        "    'Modelo': ['GPT-3.5', 'GPT-4', 'Claude-2', 'Llama-2-70B', 'Gemini Pro'],\n",
        "    'GLUE': [87.2, 91.4, 88.7, 84.3, 89.1],\n",
        "    'HellaSwag': [76.4, 85.2, 78.9, 73.1, 81.2],\n",
        "    'SQuAD': [88.1, 92.3, 89.7, 85.4, 90.8],\n",
        "    'GSM8K': [23.5, 67.1, 41.2, 28.7, 52.3],\n",
        "    'MMLU': [70.0, 86.4, 75.2, 68.9, 79.1]\n",
        "}\n",
        "\n",
        "benchmarks_df = pd.DataFrame(benchmarks_data)\n",
        "print(\"üèÜ LEADERBOARD DOS MODELOS\")\n",
        "print(\"=\" * 50)\n",
        "print(benchmarks_df.to_string(index=False))\n",
        "\n",
        "# Calculando score m√©dio\n",
        "numeric_cols = ['GLUE', 'HellaSwag', 'SQuAD', 'GSM8K', 'MMLU']\n",
        "benchmarks_df['Score_M√©dio'] = benchmarks_df[numeric_cols].mean(axis=1)\n",
        "\n",
        "print(\"\\nüéØ RANKING POR SCORE M√âDIO:\")\n",
        "ranking = benchmarks_df.sort_values('Score_M√©dio', ascending=False)[['Modelo', 'Score_M√©dio']]\n",
        "print(ranking.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando o desempenho nos benchmarks\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Gr√°fico 1: Compara√ß√£o por benchmark\n",
        "benchmarks_melted = benchmarks_df.melt(id_vars=['Modelo'], \n",
        "                                      value_vars=numeric_cols,\n",
        "                                      var_name='Benchmark', \n",
        "                                      value_name='Score')\n",
        "\n",
        "sns.barplot(data=benchmarks_melted, x='Benchmark', y='Score', hue='Modelo', ax=ax1)\n",
        "ax1.set_title('üèÜ Desempenho por Benchmark', fontsize=14)\n",
        "ax1.set_ylabel('Score (%)')\n",
        "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Gr√°fico 2: Score m√©dio geral\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']\n",
        "bars = ax2.bar(benchmarks_df['Modelo'], benchmarks_df['Score_M√©dio'], color=colors)\n",
        "ax2.set_title('üéØ Score M√©dio Geral', fontsize=14)\n",
        "ax2.set_ylabel('Score M√©dio (%)')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "             f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Gr√°ficos de benchmark gerados!\")\n",
        "print(\"üí° Note como diferentes modelos se saem melhor em diferentes tarefas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Fluxo de Avalia√ß√£o: A \"Receita de Bolo\" da Avalia√ß√£o\n\nToda avalia√ß√£o s√©ria segue um processo, tipo receita de bolo da vov√≥:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\ngraph TD\n    A[üìä Definir Objetivos] --> B[üìù Escolher M√©tricas]\n    B --> C[üéØ Preparar Datasets]\n    C --> D[üîÑ Executar Testes]\n    D --> E[üìà Calcular M√©tricas]\n    E --> F[ü§î Analisar Resultados]\n    F --> G{‚úÖ Satisfat√≥rio?}\n    G -->|N√£o| H[üîß Melhorar Modelo]\n    H --> D\n    G -->|Sim| I[üöÄ Deploy/Conclus√£o]\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Passo 1: Definir Objetivos\n**\"O que meu modelo precisa fazer MUITO bem?\"**\n\n- Chatbot de atendimento ‚Üí Compreens√£o + Cortesia\n- Tradutor ‚Üí Precis√£o + Flu√™ncia  \n- Resumidor ‚Üí Concis√£o + Relev√¢ncia\n\n### üìä Passo 2: Escolher M√©tricas\n**\"Como vou medir se est√° bom?\"**\n\n- **Objetivas:** BLEU, ROUGE, Perplexidade\n- **Subjetivas:** Avalia√ß√£o humana, A/B testing\n- **T√©cnicas:** Lat√™ncia, throughput, uso de mem√≥ria\n\n**üí° Dica do Pedro:** Nunca use s√≥ UMA m√©trica! √â tipo avaliar um namorado s√≥ pela apar√™ncia... voc√™ precisa de m√∫ltiplos crit√©rios! üòÑ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando m√©tricas espec√≠ficas para LLMs\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def calculate_bleu_simple(reference, candidate):\n",
        "    \"\"\"Implementa√ß√£o simplificada do BLEU score\"\"\"\n",
        "    ref_words = reference.lower().split()\n",
        "    cand_words = candidate.lower().split()\n",
        "    \n",
        "    if len(cand_words) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Contagem de palavras em comum (1-gram precision)\n",
        "    ref_counter = Counter(ref_words)\n",
        "    cand_counter = Counter(cand_words)\n",
        "    \n",
        "    overlap = sum((ref_counter & cand_counter).values())\n",
        "    precision = overlap / len(cand_words)\n",
        "    \n",
        "    # Brevity penalty (penaliza textos muito curtos)\n",
        "    bp = 1.0 if len(cand_words) >= len(ref_words) else math.exp(1 - len(ref_words)/len(cand_words))\n",
        "    \n",
        "    return bp * precision\n",
        "\n",
        "def calculate_perplexity_estimate(text_length, num_errors):\n",
        "    \"\"\"Estimativa simplificada de perplexidade\"\"\"\n",
        "    # Quanto menor, melhor\n",
        "    error_rate = num_errors / text_length if text_length > 0 else 1.0\n",
        "    return math.exp(error_rate)\n",
        "\n",
        "# Exemplo de avalia√ß√£o para tarefa de gera√ß√£o de texto\n",
        "evaluation_examples = [\n",
        "    {\n",
        "        'referencia': 'O gato subiu no telhado da casa',\n",
        "        'modelo_a': 'O gato escalou o telhado da resid√™ncia',\n",
        "        'modelo_b': 'O felino subiu no teto do lar'\n",
        "    },\n",
        "    {\n",
        "        'referencia': 'Python √© uma linguagem de programa√ß√£o',\n",
        "        'modelo_a': 'Python √© uma linguagem para programar',\n",
        "        'modelo_b': 'Python programa linguagem computador'\n",
        "    },\n",
        "    {\n",
        "        'referencia': 'Machine learning est√° revolucionando a tecnologia',\n",
        "        'modelo_a': 'Aprendizado de m√°quina est√° transformando a tecnologia',\n",
        "        'modelo_b': 'ML mudando tech muito'\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"üéØ AVALIA√á√ÉO DE GERA√á√ÉO DE TEXTO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, example in enumerate(evaluation_examples, 1):\n",
        "    print(f\"\\nüìù Exemplo {i}:\")\n",
        "    print(f\"Refer√™ncia: {example['referencia']}\")\n",
        "    print(f\"Modelo A: {example['modelo_a']}\")\n",
        "    print(f\"Modelo B: {example['modelo_b']}\")\n",
        "    \n",
        "    # BLEU scores\n",
        "    bleu_a = calculate_bleu_simple(example['referencia'], example['modelo_a'])\n",
        "    bleu_b = calculate_bleu_simple(example['referencia'], example['modelo_b'])\n",
        "    \n",
        "    print(f\"\\nüìä BLEU Score:\")\n",
        "    print(f\"  Modelo A: {bleu_a:.3f}\")\n",
        "    print(f\"  Modelo B: {bleu_b:.3f}\")\n",
        "    \n",
        "    winner = \"A\" if bleu_a > bleu_b else \"B\"\n",
        "    print(f\"üèÜ Vencedor: Modelo {winner}\")\n",
        "\n",
        "print(\"\\nüí° BLEU mede similaridade lexical com a refer√™ncia\")\n",
        "print(\"üìà Valores mais altos = mais similar ao texto esperado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ M√©tricas Espec√≠ficas para LLMs: Os \"Hits do Momento\"\n\nAgora vamos falar das m√©tricas que s√£o espec√≠ficas para modelos de linguagem grandes:\n\n### üî§ BLEU Score\n**\"Quantas palavras em comum com a resposta esperada?\"**  \nOriginalmente para tradu√ß√£o, agora usado em gera√ß√£o de texto.\n\n### üìÑ ROUGE Score\n**\"Bom para avaliar resumos\"**  \nMede overlap de n-gramas entre texto gerado e refer√™ncia.\n\n### üåä Perplexidade (Perplexity)\n**\"Qu√£o 'surpreso' o modelo fica com o texto?\"**  \nQuanto menor, melhor. √â tipo medir se o texto \"flui naturalmente\".\n\n### üéØ BERTScore\n**\"Usa embeddings para comparar significado\"**  \nMais sofisticado que BLEU, entende sin√¥nimos.\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-09_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparando diferentes m√©tricas para LLMs\n",
        "import random\n",
        "\n",
        "# Simulando dados de m√∫ltiplos modelos\n",
        "models_data = {\n",
        "    'GPT-3.5': {'bleu': 0.72, 'rouge': 0.68, 'perplexity': 12.4, 'bert_score': 0.81},\n",
        "    'GPT-4': {'bleu': 0.79, 'rouge': 0.75, 'perplexity': 8.9, 'bert_score': 0.87},\n",
        "    'Claude-2': {'bleu': 0.74, 'rouge': 0.71, 'perplexity': 10.2, 'bert_score': 0.83},\n",
        "    'Llama-2': {'bleu': 0.69, 'rouge': 0.66, 'perplexity': 14.1, 'bert_score': 0.78},\n",
        "    'Gemini': {'bleu': 0.76, 'rouge': 0.73, 'perplexity': 9.7, 'bert_score': 0.85}\n",
        "}\n",
        "\n",
        "# Convertendo para DataFrame\n",
        "metrics_df = pd.DataFrame(models_data).T\n",
        "print(\"üéØ COMPARA√á√ÉO DE M√âTRICAS ESPEC√çFICAS PARA LLMs\")\n",
        "print(\"=\" * 60)\n",
        "print(metrics_df.round(3))\n",
        "\n",
        "# Normalizando perplexity (invertendo porque menor √© melhor)\n",
        "max_perplexity = metrics_df['perplexity'].max()\n",
        "metrics_df['perplexity_norm'] = (max_perplexity - metrics_df['perplexity']) / max_perplexity\n",
        "\n",
        "# Calculando score composto\n",
        "score_cols = ['bleu', 'rouge', 'perplexity_norm', 'bert_score']\n",
        "metrics_df['score_composto'] = metrics_df[score_cols].mean(axis=1)\n",
        "\n",
        "print(\"\\nüèÜ RANKING FINAL (Score Composto):\")\n",
        "ranking = metrics_df.sort_values('score_composto', ascending=False)[['score_composto']]\n",
        "for i, (model, score) in enumerate(ranking.iterrows(), 1):\n",
        "    print(f\"{i}¬∫ lugar: {model} - {score['score_composto']:.3f}\")\n",
        "\n",
        "print(\"\\nüí° Score composto = m√©dia de todas as m√©tricas normalizadas\")\n",
        "print(\"üìä Perplexity foi invertida (menor √© melhor)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o radar das m√©tricas\n",
        "import math\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
        "\n",
        "# M√©tricas para o radar\n",
        "metrics = ['BLEU', 'ROUGE', 'Perplexity\\n(norm)', 'BERTScore']\n",
        "angles = [n / float(len(metrics)) * 2 * math.pi for n in range(len(metrics))]\n",
        "angles += angles[:1]  # Completar o c√≠rculo\n",
        "\n",
        "# Cores para cada modelo\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']\n",
        "\n",
        "# Plotar cada modelo\n",
        "for i, (model, color) in enumerate(zip(metrics_df.index, colors)):\n",
        "    values = [\n",
        "        metrics_df.loc[model, 'bleu'],\n",
        "        metrics_df.loc[model, 'rouge'], \n",
        "        metrics_df.loc[model, 'perplexity_norm'],\n",
        "        metrics_df.loc[model, 'bert_score']\n",
        "    ]\n",
        "    values += values[:1]  # Completar o c√≠rculo\n",
        "    \n",
        "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=color)\n",
        "    ax.fill(angles, values, alpha=0.1, color=color)\n",
        "\n",
        "# Configura√ß√µes do gr√°fico\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_title('üéØ Compara√ß√£o Radar das M√©tricas LLM\\n', size=16, pad=20)\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Gr√°fico radar criado!\")\n",
        "print(\"üéØ Quanto mais longe do centro, melhor o desempenho\")\n",
        "print(\"üí° Facilita visualizar pontos fortes e fracos de cada modelo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üë• Avalia√ß√£o Humana: Quando a M√°quina Precisa da Opini√£o Humana\n\nT√°, galera, nem tudo na vida pode ser medido por n√∫meros! Imaginem avaliar um poema s√≥ contando palavras... ü§î\n\n### Quando Precisamos de Humanos?\n\n1. **Criatividade** - \"Esse texto √© interessante?\"\n2. **Adequa√ß√£o Cultural** - \"Isso faz sentido no Brasil?\"\n3. **√âtica e Vieses** - \"Isso pode ofender algu√©m?\"\n4. **Nuances Sutis** - \"O tom est√° adequado?\"\n\n### üéØ M√©todos de Avalia√ß√£o Humana\n\n**üìä Escalas Likert**  \n\"De 1 a 5, qu√£o boa √© essa resposta?\"  \n\n**‚öñÔ∏è Compara√ß√£o Pareada**  \n\"Entre A e B, qual resposta √© melhor?\"  \n\n**üìù Avalia√ß√£o Qualitativa**  \n\"Descreva em suas palavras o que achou\"\n\n**üí° Dica do Pedro:** Avalia√ß√£o humana √© cara e demorada, mas √†s vezes √© a √öNICA forma de saber se seu modelo realmente est√° bom!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulando dados de avalia√ß√£o humana\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulando avalia√ß√µes humanas para diferentes aspectos\n",
        "human_eval_data = {\n",
        "    'Modelo': ['GPT-3.5', 'GPT-4', 'Claude-2', 'Llama-2', 'Gemini'],\n",
        "    'Flu√™ncia': np.random.normal(loc=[7.2, 8.5, 7.8, 6.9, 8.1], scale=0.5, size=5).clip(1, 10),\n",
        "    'Relev√¢ncia': np.random.normal(loc=[7.0, 8.3, 7.6, 6.7, 7.9], scale=0.6, size=5).clip(1, 10),\n",
        "    'Criatividade': np.random.normal(loc=[6.8, 8.1, 7.9, 6.5, 7.7], scale=0.7, size=5).clip(1, 10),\n",
        "    'Seguran√ßa': np.random.normal(loc=[8.1, 9.0, 8.6, 7.8, 8.4], scale=0.4, size=5).clip(1, 10),\n",
        "    'Satisfa√ß√£o_Geral': np.random.normal(loc=[7.1, 8.4, 7.8, 6.8, 8.0], scale=0.5, size=5).clip(1, 10)\n",
        "}\n",
        "\n",
        "human_df = pd.DataFrame(human_eval_data)\n",
        "human_df.iloc[:, 1:] = human_df.iloc[:, 1:].round(1)\n",
        "\n",
        "print(\"üë• AVALIA√á√ÉO HUMANA DOS MODELOS\")\n",
        "print(\"=\" * 50)\n",
        "print(\"üìä Escala: 1-10 (onde 10 = excelente)\")\n",
        "print(\"\\n\", human_df.to_string(index=False))\n",
        "\n",
        "# Calculando score m√©dio da avalia√ß√£o humana\n",
        "human_metrics = ['Flu√™ncia', 'Relev√¢ncia', 'Criatividade', 'Seguran√ßa']\n",
        "human_df['Score_Humano'] = human_df[human_metrics].mean(axis=1)\n",
        "\n",
        "print(\"\\nüèÜ RANKING POR AVALIA√á√ÉO HUMANA:\")\n",
        "human_ranking = human_df.sort_values('Score_Humano', ascending=False)[['Modelo', 'Score_Humano', 'Satisfa√ß√£o_Geral']]\n",
        "for i, row in human_ranking.iterrows():\n",
        "    print(f\"{list(human_ranking.index).index(i)+1}¬∫ {row['Modelo']}: {row['Score_Humano']:.1f} (Satisfa√ß√£o: {row['Satisfa√ß√£o_Geral']:.1f})\")\n",
        "\n",
        "print(\"\\nüí° Note que a ordem pode ser diferente das m√©tricas autom√°ticas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparando avalia√ß√£o autom√°tica vs humana\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico 1: Avalia√ß√£o Humana por aspecto\n",
        "human_melted = human_df.melt(id_vars=['Modelo'], \n",
        "                            value_vars=human_metrics,\n",
        "                            var_name='Aspecto', \n",
        "                            value_name='Score')\n",
        "\n",
        "sns.boxplot(data=human_melted, x='Aspecto', y='Score', ax=ax1)\n",
        "ax1.set_title('üë• Distribui√ß√£o das Avalia√ß√µes Humanas', fontsize=14)\n",
        "ax1.set_ylabel('Score (1-10)')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Gr√°fico 2: Correla√ß√£o entre avalia√ß√£o autom√°tica e humana\n",
        "# Combinando dados para compara√ß√£o\n",
        "combined_scores = pd.DataFrame({\n",
        "    'Modelo': metrics_df.index,\n",
        "    'Score_Autom√°tico': metrics_df['score_composto'].values,\n",
        "    'Score_Humano': human_df.set_index('Modelo')['Score_Humano'].values\n",
        "})\n",
        "\n",
        "ax2.scatter(combined_scores['Score_Autom√°tico'], combined_scores['Score_Humano'], \n",
        "           s=100, alpha=0.7, c=colors)\n",
        "\n",
        "# Adicionando labels dos modelos\n",
        "for i, row in combined_scores.iterrows():\n",
        "    ax2.annotate(row['Modelo'], \n",
        "                (row['Score_Autom√°tico'], row['Score_Humano']),\n",
        "                xytext=(5, 5), textcoords='offset points')\n",
        "\n",
        "ax2.set_xlabel('Score Autom√°tico')\n",
        "ax2.set_ylabel('Score Humano')\n",
        "ax2.set_title('ü§ñ vs üë• Correla√ß√£o Auto x Humano', fontsize=14)\n",
        "\n",
        "# Linha de tend√™ncia\n",
        "z = np.polyfit(combined_scores['Score_Autom√°tico'], combined_scores['Score_Humano'], 1)\n",
        "p = np.poly1d(z)\n",
        "ax2.plot(combined_scores['Score_Autom√°tico'], p(combined_scores['Score_Autom√°tico']), \"r--\", alpha=0.8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculando correla√ß√£o\n",
        "correlation = combined_scores['Score_Autom√°tico'].corr(combined_scores['Score_Humano'])\n",
        "print(f\"üìä Correla√ß√£o entre avalia√ß√£o autom√°tica e humana: {correlation:.3f}\")\n",
        "print(f\"üí° Correla√ß√£o de {correlation:.1f} indica {'forte' if abs(correlation) > 0.7 else 'moderada' if abs(correlation) > 0.5 else 'fraca'} concord√¢ncia\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Avalia√ß√£o de Performance: Velocidade e Efici√™ncia\n\nN√£o adianta ter um modelo super inteligente se ele demora 1 hora para responder \"oi\"! üòÖ\n\n### M√©tricas T√©cnicas Importantes:\n\n**‚è±Ô∏è Lat√™ncia**  \nTempo entre fazer a pergunta e receber a resposta\n\n**üöÄ Throughput**  \nQuantas requisi√ß√µes por segundo consegue processar\n\n**üíæ Uso de Mem√≥ria**  \nQuanto de RAM/VRAM consome\n\n**‚ö° Tokens por Segundo**  \nVelocidade de gera√ß√£o de texto\n\n**üí∞ Custo por Token**  \nQuanto custa cada palavra gerada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulando m√©tricas de performance\n",
        "import time\n",
        "\n",
        "performance_data = {\n",
        "    'Modelo': ['GPT-3.5-Turbo', 'GPT-4', 'Claude-2', 'Llama-2-70B', 'Gemini-Pro'],\n",
        "    'Lat√™ncia_ms': [850, 1200, 950, 2100, 750],\n",
        "    'Throughput_req_s': [45, 25, 38, 12, 52],\n",
        "    'Tokens_por_segundo': [120, 80, 95, 35, 140],\n",
        "    'Mem√≥ria_GB': [12, 28, 16, 140, 20],\n",
        "    'Custo_por_1k_tokens': [0.002, 0.06, 0.008, 0.001, 0.003]\n",
        "}\n",
        "\n",
        "perf_df = pd.DataFrame(performance_data)\n",
        "\n",
        "print(\"‚ö° M√âTRICAS DE PERFORMANCE\")\n",
        "print(\"=\" * 50)\n",
        "print(perf_df.to_string(index=False))\n",
        "\n",
        "# Calculando score de performance normalizado\n",
        "# Para m√©tricas onde MENOR √© melhor (lat√™ncia, custo, mem√≥ria)\n",
        "perf_df['latencia_score'] = (perf_df['Lat√™ncia_ms'].max() - perf_df['Lat√™ncia_ms']) / perf_df['Lat√™ncia_ms'].max()\n",
        "perf_df['memoria_score'] = (perf_df['Mem√≥ria_GB'].max() - perf_df['Mem√≥ria_GB']) / perf_df['Mem√≥ria_GB'].max()\n",
        "perf_df['custo_score'] = (perf_df['Custo_por_1k_tokens'].max() - perf_df['Custo_por_1k_tokens']) / perf_df['Custo_por_1k_tokens'].max()\n",
        "\n",
        "# Para m√©tricas onde MAIOR √© melhor (throughput, tokens/s)\n",
        "perf_df['throughput_score'] = perf_df['Throughput_req_s'] / perf_df['Throughput_req_s'].max()\n",
        "perf_df['velocidade_score'] = perf_df['Tokens_por_segundo'] / perf_df['Tokens_por_segundo'].max()\n",
        "\n",
        "# Score composto de performance\n",
        "perf_metrics = ['latencia_score', 'throughput_score', 'velocidade_score', 'memoria_score', 'custo_score']\n",
        "perf_df['performance_score'] = perf_df[perf_metrics].mean(axis=1)\n",
        "\n",
        "print(\"\\nüèÜ RANKING DE PERFORMANCE:\")\n",
        "perf_ranking = perf_df.sort_values('performance_score', ascending=False)[['Modelo', 'performance_score']]\n",
        "for i, (_, row) in enumerate(perf_ranking.iterrows(), 1):\n",
        "    print(f\"{i}¬∫ {row['Modelo']}: {row['performance_score']:.3f}\")\n",
        "\n",
        "print(\"\\nüí° Score considera velocidade, efici√™ncia e custo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì EXERC√çCIO PR√ÅTICO 1: Monte Sua Pr√≥pria Avalia√ß√£o\n\nAgora √© sua vez! Vamos criar uma avalia√ß√£o personalizada para um caso espec√≠fico.\n\n**üìù CEN√ÅRIO:**  \nVoc√™ precisa escolher um LLM para um chatbot de atendimento ao cliente de uma loja online brasileira.\n\n**üéØ CRIT√âRIOS IMPORTANTES:**\n1. Entendimento de portugu√™s brasileiro\n2. Velocidade de resposta (cliente n√£o gosta de esperar)\n3. Cortesia e tom adequado\n4. Custo operacional\n5. Capacidade de lidar com reclama√ß√µes\n\n**üìä SUA TAREFA:**\nComplete o c√≥digo abaixo definindo pesos para cada crit√©rio e implementando a fun√ß√£o de avalia√ß√£o:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 1: Complete o c√≥digo para criar sua avalia√ß√£o personalizada\n",
        "\n",
        "def avaliar_modelo_chatbot(modelo_name, criterios_scores, pesos):\n",
        "    \"\"\"\n",
        "    Avalia um modelo para chatbot baseado em crit√©rios espec√≠ficos\n",
        "    \n",
        "    Args:\n",
        "        modelo_name: Nome do modelo\n",
        "        criterios_scores: Dict com scores de cada crit√©rio (0-10)\n",
        "        pesos: Dict com pesos de cada crit√©rio (devem somar 1.0)\n",
        "    \n",
        "    Returns:\n",
        "        Score final ponderado\n",
        "    \"\"\"\n",
        "    # TODO: Implemente a l√≥gica de c√°lculo do score ponderado\n",
        "    score_final = 0\n",
        "    \n",
        "    # DICA: Para cada crit√©rio, multiplique o score pelo peso\n",
        "    # SEU C√ìDIGO AQUI:\n",
        "    \n",
        "    \n",
        "    return score_final\n",
        "\n",
        "# Dados dos modelos (j√° preenchidos)\n",
        "modelos_chatbot = {\n",
        "    'GPT-3.5-Turbo': {\n",
        "        'portugues_br': 8.5,\n",
        "        'velocidade': 9.0,\n",
        "        'cortesia': 8.0,\n",
        "        'custo': 8.5,\n",
        "        'reclamacoes': 7.5\n",
        "    },\n",
        "    'GPT-4': {\n",
        "        'portugues_br': 9.5,\n",
        "        'velocidade': 7.0,\n",
        "        'cortesia': 9.0,\n",
        "        'custo': 5.0,\n",
        "        'reclamacoes': 9.5\n",
        "    },\n",
        "    'Claude-2': {\n",
        "        'portugues_br': 8.0,\n",
        "        'velocidade': 8.0,\n",
        "        'cortesia': 9.5,\n",
        "        'custo': 7.0,\n",
        "        'reclamacoes': 8.5\n",
        "    }\n",
        "}\n",
        "\n",
        "# TODO: Defina os pesos (devem somar 1.0)\n",
        "# Pense: o que √© mais importante para um chatbot?\n",
        "pesos = {\n",
        "    'portugues_br': 0.0,    # SEU PESO AQUI (ex: 0.25 = 25%)\n",
        "    'velocidade': 0.0,      # SEU PESO AQUI\n",
        "    'cortesia': 0.0,       # SEU PESO AQUI\n",
        "    'custo': 0.0,          # SEU PESO AQUI\n",
        "    'reclamacoes': 0.0     # SEU PESO AQUI\n",
        "    # TOTAL DEVE SER 1.0!\n",
        "}\n",
        "\n",
        "print(\"ü§ñ AVALIA√á√ÉO DE MODELOS PARA CHATBOT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Testando sua implementa√ß√£o\n",
        "if sum(pesos.values()) == 1.0:\n",
        "    for modelo, scores in modelos_chatbot.items():\n",
        "        score = avaliar_modelo_chatbot(modelo, scores, pesos)\n",
        "        print(f\"{modelo}: {score:.2f}\")\n",
        "else:\n",
        "    print(\"‚ùå ERRO: Os pesos devem somar 1.0!\")\n",
        "    print(f\"Soma atual: {sum(pesos.values()):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üö® Problemas Comuns na Avalia√ß√£o: Os \"Pegadinhas\" que Todo Mundo Cai\n\nGalera, avaliar LLMs tem suas armadilhas! √â tipo dirigir no Rio... tem que conhecer os \"furos\" üòÑ\n\n### üé≠ 1. Data Leakage (Vazamento de Dados)\n**O Problema:** Seu modelo \"decorou\" as respostas do teste  \n**Analogia:** √â tipo fazer prova tendo visto as respostas antes\n\n### üìä 2. M√©tricas que Mentem\n**O Problema:** BLEU alto n√£o significa texto bom  \n**Exemplo:** \"Gato gato gato gato\" pode ter BLEU alto se a refer√™ncia for \"O gato\"\n\n### üéØ 3. Overfitting nos Benchmarks\n**O Problema:** Modelo vai bem no GLUE mas falha na vida real  \n**Analogia:** Decorar quest√µes do ENEM mas n√£o saber aplicar na faculdade\n\n### üë• 4. Vi√©s dos Avaliadores\n**O Problema:** Humanos t√™m prefer√™ncias pessoais  \n**Solu√ß√£o:** M√∫ltiplos avaliadores + guidelines claros\n\n### ‚öñÔ∏è 5. Desbalanceamento de Classes\n**O Problema:** 90% das amostras s√£o positivas  \n**Resultado:** Modelo \"aprende\" a sempre responder positivo\n\n**üí° Dica do Pedro:** Nunca confie numa m√©trica s√≥! √â tipo namorar baseado s√≥ numa foto do Instagram... voc√™ precisa conhecer a pessoa toda! üòÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrando problemas comuns na avalia√ß√£o\n",
        "\n",
        "# Problema 1: Desbalanceamento de classes\n",
        "print(\"üö® DEMONSTRA√á√ÉO: PROBLEMAS NA AVALIA√á√ÉO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Simulando dataset desbalanceado\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# 90% positivo, 10% negativo (muito desbalanceado!)\n",
        "true_labels_unbalanced = np.random.choice(['positivo', 'negativo'], \n",
        "                                         n_samples, p=[0.9, 0.1])\n",
        "\n",
        "# Modelo \"pregui√ßoso\" que sempre prediz \"positivo\"\n",
        "lazy_predictions = ['positivo'] * n_samples\n",
        "\n",
        "# Calculando m√©tricas\n",
        "accuracy_lazy = accuracy_score(true_labels_unbalanced, lazy_predictions)\n",
        "\n",
        "print(\"üìä CASO 1: Dataset Desbalanceado\")\n",
        "print(f\"Distribui√ß√£o real: {np.unique(true_labels_unbalanced, return_counts=True)}\")\n",
        "print(f\"Modelo pregui√ßoso (sempre 'positivo'): {accuracy_lazy:.1%} de acur√°cia!\")\n",
        "print(\"‚ùå PROBLEMA: Alta acur√°cia mas modelo in√∫til!\")\n",
        "\n",
        "# Problema 2: M√©tricas enganosas\n",
        "print(\"\\nüìä CASO 2: M√©trica Enganosa (BLEU)\")\n",
        "\n",
        "referencia = \"O gato subiu no telhado\"\n",
        "texto_repetitivo = \"gato gato gato gato gato\"\n",
        "texto_bom = \"O felino escalou o teto\"\n",
        "\n",
        "bleu_repetitivo = calculate_bleu_simple(referencia, texto_repetitivo)\n",
        "bleu_bom = calculate_bleu_simple(referencia, texto_bom)\n",
        "\n",
        "print(f\"Refer√™ncia: '{referencia}'\")\n",
        "print(f\"Texto repetitivo: '{texto_repetitivo}' ‚Üí BLEU: {bleu_repetitivo:.3f}\")\n",
        "print(f\"Texto bom: '{texto_bom}' ‚Üí BLEU: {bleu_bom:.3f}\")\n",
        "print(\"‚ùå PROBLEMA: Texto repetitivo pode ter BLEU alto!\")\n",
        "\n",
        "# Problema 3: Falta de contexto\n",
        "print(\"\\nüìä CASO 3: Falta de Avalia√ß√£o Contextual\")\n",
        "exemplos_contexto = [\n",
        "    {\n",
        "        'contexto': 'Cliente reclamando de produto quebrado',\n",
        "        'resposta_tecnica': 'Produto apresenta falha estrutural tipo B-304',\n",
        "        'resposta_humana': 'Que chato! Vamos resolver isso rapidinho para voc√™'\n",
        "    },\n",
        "    {\n",
        "        'contexto': 'D√∫vida sobre frete',\n",
        "        'resposta_tecnica': 'Modalidade de envio padr√£o demanda 3-5 dias √∫teis',\n",
        "        'resposta_humana': 'Seu pedido chega em 3-5 dias √∫teis pelo correio!'\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, ex in enumerate(exemplos_contexto, 1):\n",
        "    print(f\"\\nExemplo {i}: {ex['contexto']}\")\n",
        "    print(f\"  T√©cnica: {ex['resposta_tecnica']}\")\n",
        "    print(f\"  Humana:  {ex['resposta_humana']}\")\n",
        "\n",
        "print(\"\\n‚ùå PROBLEMA: M√©tricas autom√°ticas n√£o capturam adequa√ß√£o ao contexto!\")\n",
        "print(\"‚úÖ SOLU√á√ÉO: Combinar m√©tricas autom√°ticas + avalia√ß√£o humana contextual\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Monitoramento Cont√≠nuo: Mantendo o Olho no Modelo\n\nLembram do m√≥dulo sobre prompting? L√° vimos como fazer o modelo funcionar. Agora precisamos garantir que ele CONTINUE funcionando!\n\n√â tipo cuidar de um filho: n√£o basta educar uma vez, tem que acompanhar sempre! üë∂\n\n### üîç O que Monitorar?\n\n1. **Drift de Performance** - Modelo piorando com o tempo\n2. **Mudan√ßas no Comportamento** - Respostas ficando estranhas\n3. **Feedback dos Usu√°rios** - Satisfa√ß√£o caindo\n4. **M√©tricas T√©cnicas** - Lat√™ncia aumentando\n5. **Custos** - Conta do OpenAI explodindo üí∏\n\n### üéØ Estrat√©gias de Monitoramento\n\n**üìä Dashboards em Tempo Real**  \nGr√°ficos bonitos que mostram tudo funcionando (ou n√£o)\n\n**üö® Alertas Autom√°ticos**  \n\"Opa, algo t√° estranho aqui!\"\n\n**üìù A/B Testing Cont√≠nuo**  \nSempre testando vers√µes diferentes\n\n**üë• Feedback Loop**  \nUsu√°rios ajudam a melhorar o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulando sistema de monitoramento cont√≠nuo\n",
        "import datetime\n",
        "\n",
        "# Gerando dados de monitoramento ao longo do tempo\n",
        "np.random.seed(42)\n",
        "days = 30\n",
        "dates = [datetime.datetime.now() - datetime.timedelta(days=x) for x in range(days, 0, -1)]\n",
        "\n",
        "# Simulando degrada√ß√£o gradual do modelo\n",
        "base_accuracy = 0.85\n",
        "degradation_factor = 0.001  # Piora 0.1% por dia\n",
        "\n",
        "monitoring_data = []\n",
        "for i, date in enumerate(dates):\n",
        "    # Performance degradando com o tempo + ru√≠do aleat√≥rio\n",
        "    daily_accuracy = base_accuracy - (i * degradation_factor) + np.random.normal(0, 0.02)\n",
        "    daily_latency = 800 + (i * 10) + np.random.normal(0, 50)  # Lat√™ncia aumentando\n",
        "    daily_cost = 100 + (i * 2) + np.random.normal(0, 10)      # Custo aumentando\n",
        "    daily_satisfaction = 8.5 - (i * 0.05) + np.random.normal(0, 0.3)  # Satisfa√ß√£o caindo\n",
        "    \n",
        "    monitoring_data.append({\n",
        "        'data': date,\n",
        "        'acuracia': max(0.6, min(1.0, daily_accuracy)),\n",
        "        'latencia_ms': max(500, daily_latency),\n",
        "        'custo_diario': max(50, daily_cost),\n",
        "        'satisfacao': max(1, min(10, daily_satisfaction))\n",
        "    })\n",
        "\n",
        "monitoring_df = pd.DataFrame(monitoring_data)\n",
        "\n",
        "print(\"üìä MONITORAMENTO CONT√çNUO - √öLTIMOS 30 DIAS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Estat√≠sticas resumo\n",
        "print(f\"üìà Acur√°cia: {monitoring_df['acuracia'].iloc[0]:.1%} ‚Üí {monitoring_df['acuracia'].iloc[-1]:.1%}\")\n",
        "print(f\"‚è±Ô∏è  Lat√™ncia: {monitoring_df['latencia_ms'].iloc[0]:.0f}ms ‚Üí {monitoring_df['latencia_ms'].iloc[-1]:.0f}ms\")\n",
        "print(f\"üí∞ Custo: ${monitoring_df['custo_diario'].iloc[0]:.0f} ‚Üí ${monitoring_df['custo_diario'].iloc[-1]:.0f}\")\n",
        "print(f\"üòä Satisfa√ß√£o: {monitoring_df['satisfacao'].iloc[0]:.1f} ‚Üí {monitoring_df['satisfacao'].iloc[-1]:.1f}\")\n",
        "\n",
        "# Detectando problemas\n",
        "accuracy_drop = monitoring_df['acuracia'].iloc[0] - monitoring_df['acuracia'].iloc[-1]\n",
        "latency_increase = monitoring_df['latencia_ms'].iloc[-1] - monitoring_df['latencia_ms'].iloc[0]\n",
        "cost_increase = monitoring_df['custo_diario'].iloc[-1] - monitoring_df['custo_diario'].iloc[0]\n",
        "\n",
        "print(\"\\nüö® ALERTAS DETECTADOS:\")\n",
        "if accuracy_drop > 0.05:\n",
        "    print(f\"‚ö†Ô∏è  ALERTA: Queda de acur√°cia de {accuracy_drop:.1%}!\")\n",
        "if latency_increase > 200:\n",
        "    print(f\"‚ö†Ô∏è  ALERTA: Aumento de lat√™ncia de {latency_increase:.0f}ms!\")\n",
        "if cost_increase > 50:\n",
        "    print(f\"‚ö†Ô∏è  ALERTA: Aumento de custo de ${cost_increase:.0f}!\")\n",
        "\n",
        "print(\"\\nüí° Modelo precisa de aten√ß√£o!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando o monitoramento\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Gr√°fico 1: Acur√°cia ao longo do tempo\n",
        "ax1.plot(monitoring_df['data'], monitoring_df['acuracia'], 'b-', linewidth=2, marker='o')\n",
        "ax1.axhline(y=0.8, color='r', linestyle='--', alpha=0.7, label='Limite m√≠nimo')\n",
        "ax1.set_title('üìä Acur√°cia ao Longo do Tempo')\n",
        "ax1.set_ylabel('Acur√°cia')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 2: Lat√™ncia\n",
        "ax2.plot(monitoring_df['data'], monitoring_df['latencia_ms'], 'orange', linewidth=2, marker='s')\n",
        "ax2.axhline(y=1000, color='r', linestyle='--', alpha=0.7, label='Limite m√°ximo')\n",
        "ax2.set_title('‚è±Ô∏è Lat√™ncia ao Longo do Tempo')\n",
        "ax2.set_ylabel('Lat√™ncia (ms)')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 3: Custo\n",
        "ax3.plot(monitoring_df['data'], monitoring_df['custo_diario'], 'green', linewidth=2, marker='^')\n",
        "ax3.set_title('üí∞ Custo Di√°rio')\n",
        "ax3.set_ylabel('Custo ($)')\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 4: Satisfa√ß√£o do usu√°rio\n",
        "ax4.plot(monitoring_df['data'], monitoring_df['satisfacao'], 'purple', linewidth=2, marker='D')\n",
        "ax4.axhline(y=7.0, color='r', linestyle='--', alpha=0.7, label='Limite m√≠nimo')\n",
        "ax4.set_title('üòä Satisfa√ß√£o dos Usu√°rios')\n",
        "ax4.set_ylabel('Satisfa√ß√£o (1-10)')\n",
        "ax4.tick_params(axis='x', rotation=45)\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Dashboard de monitoramento gerado!\")\n",
        "print(\"üéØ Visualiza√ß√£o clara das tend√™ncias ao longo do tempo\")\n",
        "print(\"üö® Linhas vermelhas mostram limites de alerta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì EXERC√çCIO PR√ÅTICO 2: Detec√ß√£o de Problemas\n\nAgora voc√™ √© o \"m√©dico\" do modelo! Analise os sintomas e detecte o problema.\n\n**üìã CEN√ÅRIO:**  \nVoc√™ monitora um modelo de classifica√ß√£o de sentimentos para reviews de e-commerce. Nos √∫ltimos dias, algo estranho est√° acontecendo...\n\n**üîç SUA MISS√ÉO:**  \nAnalise os dados e identifique poss√≠veis problemas!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 2: Analise os dados e detecte problemas\n",
        "\n",
        "# Dados suspeitos dos √∫ltimos 7 dias\n",
        "dados_suspeitos = {\n",
        "    'dia': ['Segunda', 'Ter√ßa', 'Quarta', 'Quinta', 'Sexta', 'S√°bado', 'Domingo'],\n",
        "    'total_classificacoes': [1000, 1200, 1100, 1300, 2800, 3200, 2900],\n",
        "    'predito_positivo': [600, 720, 660, 780, 2520, 2880, 2610],\n",
        "    'predito_negativo': [300, 360, 330, 390, 210, 240, 218],\n",
        "    'predito_neutro': [100, 120, 110, 130, 70, 80, 72],\n",
        "    'acuracia_reportada': [0.85, 0.83, 0.86, 0.84, 0.92, 0.94, 0.93],\n",
        "    'tempo_resposta_ms': [450, 480, 460, 470, 180, 160, 170]\n",
        "}\n",
        "\n",
        "suspeitos_df = pd.DataFrame(dados_suspeitos)\n",
        "\n",
        "print(\"üîç DADOS DOS √öLTIMOS 7 DIAS\")\n",
        "print(\"=\" * 50)\n",
        "print(suspeitos_df.to_string(index=False))\n",
        "\n",
        "# Calculando propor√ß√µes\n",
        "suspeitos_df['prop_positivo'] = suspeitos_df['predito_positivo'] / suspeitos_df['total_classificacoes']\n",
        "suspeitos_df['prop_negativo'] = suspeitos_df['predito_negativo'] / suspeitos_df['total_classificacoes']\n",
        "suspeitos_df['prop_neutro'] = suspeitos_df['predito_neutro'] / suspeitos_df['total_classificacoes']\n",
        "\n",
        "print(\"\\nüìä PROPOR√á√ïES:\")\n",
        "print(suspeitos_df[['dia', 'prop_positivo', 'prop_negativo', 'prop_neutro']].round(3).to_string(index=False))\n",
        "\n",
        "# SUA AN√ÅLISE AQUI:\n",
        "print(\"\\nü§î AN√ÅLISE - O que voc√™ observa?\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# TODO: Analise os dados e responda:\n",
        "# 1. O que aconteceu na sexta/s√°bado/domingo?\n",
        "# 2. Por que a acur√°cia subiu mas algo parece estranho?\n",
        "# 3. O que sugere sobre o comportamento do modelo?\n",
        "# 4. Qual seria sua recomenda√ß√£o?\n",
        "\n",
        "print(\"üìù SUAS OBSERVA√á√ïES:\")\n",
        "print(\"1. Volume de classifica√ß√µes: \")\n",
        "print(\"2. Distribui√ß√£o das classes: \")\n",
        "print(\"3. Acur√°cia vs comportamento: \")\n",
        "print(\"4. Tempo de resposta: \")\n",
        "print(\"\\nüéØ DIAGN√ìSTICO PROV√ÅVEL:\")\n",
        "print(\"Problema detectado: \")\n",
        "print(\"Causa prov√°vel: \")\n",
        "print(\"A√ß√£o recomendada: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Conectando com o Restante do Curso\n\nLembram da nossa jornada at√© aqui? Vamos conectar os pontos! üîó\n\n### üîô M√≥dulos Anteriores que se Conectam:\n\n**üìö M√≥dulo 2 - O que s√£o LLMs**  \n‚Üí Agora sabemos MEDIR se eles realmente \"entendem\"\n\n**üèóÔ∏è M√≥dulo 3 - Arquitetura Transformer**  \n‚Üí Diferentes arquiteturas t√™m diferentes pontos fortes nos benchmarks\n\n**üî§ M√≥dulo 4 - Tokens**  \n‚Üí M√©tricas como BLEU dependem de tokeniza√ß√£o adequada\n\n**üéØ M√≥dulo 8 - Prompting**  \n‚Üí Avalia√ß√£o nos mostra se nossos prompts est√£o funcionando!\n\n### üîú Preparando para os Pr√≥ximos M√≥dulos:\n\n**üõ°Ô∏è M√≥dulo 10 - Seguran√ßa**  \n‚Üí Vamos avaliar se modelos s√£o seguros e √©ticos\n\n**‚ö†Ô∏è M√≥dulo 11 - Limita√ß√µes**  \n‚Üí Entender onde os modelos falham mesmo com boa avalia√ß√£o\n\n**üöÄ M√≥dulo 12 - Projeto Final**  \n‚Üí Aplicar tudo que aprendemos sobre avalia√ß√£o!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumo visual das conex√µes do curso\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Dados para o mapa de conex√µes\n",
        "modulos = ['Setup', 'LLMs', 'Transformer', 'Tokens', 'Embeddings', \n",
        "          'Tipos', 'Treinamento', 'Prompting', 'AVALIA√á√ÉO', 'Seguran√ßa', \n",
        "          'Limita√ß√µes', 'Projeto', 'Avan√ßado']\n",
        "\n",
        "conexoes_avaliacao = [2, 4, 6, 8, 5, 7, 9, 10, 10, 8, 9, 9, 6]\n",
        "cores = ['lightgray' if i != 8 else 'red' for i in range(len(modulos))]\n",
        "\n",
        "bars = ax.barh(modulos, conexoes_avaliacao, color=cores)\n",
        "\n",
        "# Destacando o m√≥dulo atual\n",
        "ax.set_xlabel('N√≠vel de Conex√£o com Avalia√ß√£o')\n",
        "ax.set_title('üîó Como \"Avalia√ß√£o de Modelos\" se Conecta com Outros M√≥dulos\\n', fontsize=14)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for i, bar in enumerate(bars):\n",
        "    width = bar.get_width()\n",
        "    emoji = 'üéØ' if i == 8 else 'üìö' if i < 8 else 'üîú'\n",
        "    ax.text(width + 0.1, bar.get_y() + bar.get_height()/2, \n",
        "            f'{emoji} {width}', va='center')\n",
        "\n",
        "ax.set_xlim(0, 11)\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ M√ìDULO ATUAL: Avalia√ß√£o de Modelos\")\n",
        "print(\"üìö J√Å ESTUDADOS: Bases s√≥lidas para entender avalia√ß√£o\")\n",
        "print(\"üîú PR√ìXIMOS: Vamos usar avalia√ß√£o para seguran√ßa e projetos\")\n",
        "print(\"\\nüí° Avalia√ß√£o √© o 'term√¥metro' do nosso conhecimento sobre LLMs!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Resum√£o: O que Aprendemos Hoje\n\nBora fazer aquele resum√£o maroto do que rolou hoje! üìù\n\n### ‚úÖ CONCEITOS FUNDAMENTAIS\n- **Avalia√ß√£o √© essencial** - Sem ela, n√£o sabemos se o modelo funciona\n- **Multiple m√©tricas** - Nunca confie numa m√©trica s√≥!\n- **Contexto importa** - M√©tricas autom√°ticas n√£o capturam tudo\n- **Monitoramento cont√≠nuo** - Modelo pode degradar com o tempo\n\n### üõ†Ô∏è FERRAMENTAS PR√ÅTICAS\n- **M√©tricas cl√°ssicas:** Acur√°cia, Precis√£o, Recall, F1-Score\n- **M√©tricas para LLM:** BLEU, ROUGE, Perplexidade, BERTScore\n- **Benchmarks famosos:** GLUE, HellaSwag, SQuAD, GSM8K\n- **Avalia√ß√£o humana:** Quando m√°quinas n√£o bastam\n- **Monitoramento:** Dashboards, alertas, A/B testing\n\n### üö® ARMADILHAS A EVITAR\n- Data leakage (modelo \"decorou\" as respostas)\n- M√©tricas enganosas (BLEU alto ‚â† texto bom)\n- Overfitting em benchmarks\n- Desbalanceamento de classes\n- Falta de contexto na avalia√ß√£o\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-09_img_04.png)\n\n### üéØ DICA FINAL DO PEDRO\n**Avalia√ß√£o de modelos √© como cuidar da sa√∫de:** voc√™ n√£o vai no m√©dico s√≥ quando est√° doente, n√©? Tem que fazer check-up regular!\n\nAssim √© com LLMs: monitore sempre, use m√∫ltiplas m√©tricas, e nunca esque√ßa que n√∫meros n√£o contam toda a hist√≥ria. √Äs vezes o modelo com menor BLEU √© mais √∫til na pr√°tica!\n\n---\n\n**üöÄ PR√ìXIMO M√ìDULO:** Seguran√ßa e Guardrails  \n*Onde vamos aprender a \"colocar cabresto\" nos nossos modelos para eles n√£o sa√≠rem falando besteira por a√≠!* üòÑ\n\n**At√© l√°, pessoal! Bora avaliar uns modelos! üéØ**"
      ]
    }
  ]
}