{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 Embeddings e Representações: Transformando Palavras em Números Mágicos\n\n**Módulo 5 de 13 - Introdução à LLMs**\n\nPor: Pedro Nunes Guth 🚀\n\n---\n\nBora mergulhar no mundo dos embeddings! Se você chegou até aqui, já entendeu sobre tokens e tokenização no módulo anterior. Agora vamos descobrir como transformar essas palavrinhas em números que fazem sentido para os computadores!\n\n**O que você vai aprender:**\n- O que são embeddings (spoiler: não é comida!)\n- Como representar palavras em vetores\n- Word2Vec, GloVe e embeddings contextuais\n- Como os Transformers usam embeddings\n- Na prática: implementando embeddings do zero!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bora começar importando as bibliotecas que vamos usar!\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurações visuais\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"🎉 Bibliotecas carregadas! Bora para os embeddings!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤔 Tá, mas o que são Embeddings?\n\nImagina que você é um tradutor tentando explicar para um alienígena o que significa \"cachorro\". Como você faria isso?\n\nVocê poderia dizer:\n- \"É um animal que late\"\n- \"Tem 4 patas\"\n- \"É peludo\"\n- \"É amigo dos humanos\"\n\n**Embeddings fazem exatamente isso!** Eles pegam uma palavra e a representam através de números que capturam suas características e significados.\n\n### Por que precisamos disso?\n\nComputadores não entendem \"cachorro\" ou \"gato\". Eles só entendem números! Então precisamos converter palavras em vetores numéricos que preservem o significado semântico.\n\n**Dica do Pedro:** Think of embeddings como o \"DNA numérico\" das palavras! 🧬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Visualizando o Conceito de Embeddings\n\nVamos começar com um exemplo visual para entender como palavras podem ser representadas em um espaço vetorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar embeddings simples para algumas palavras\n",
        "# Imagine que temos apenas 2 dimensões: \"animal\" e \"tamanho\"\n",
        "\n",
        "palavras = ['cachorro', 'gato', 'elefante', 'rato', 'leão', 'peixe']\n",
        "\n",
        "# Embeddings 2D simples [dimensão_animal, dimensão_tamanho]\n",
        "embeddings_2d = {\n",
        "    'cachorro': [0.8, 0.6],   # Muito animal, tamanho médio\n",
        "    'gato': [0.9, 0.4],       # Muito animal, pequeno\n",
        "    'elefante': [0.7, 0.9],   # Animal, muito grande\n",
        "    'rato': [0.6, 0.1],       # Pouco animal, muito pequeno\n",
        "    'leão': [0.85, 0.8],      # Muito animal, grande\n",
        "    'peixe': [0.5, 0.3]       # Pouco animal (diferente), pequeno\n",
        "}\n",
        "\n",
        "# Plotando os embeddings\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "for palavra, (x, y) in embeddings_2d.items():\n",
        "    ax.scatter(x, y, s=200, alpha=0.7)\n",
        "    ax.annotate(palavra, (x, y), xytext=(5, 5), textcoords='offset points', \n",
        "                fontsize=12, fontweight='bold')\n",
        "\n",
        "ax.set_xlabel('Dimensão: Animalidade', fontsize=14)\n",
        "ax.set_ylabel('Dimensão: Tamanho', fontsize=14)\n",
        "ax.set_title('Embeddings 2D Simplificados\\n(Palavras no Espaço Vetorial)', fontsize=16)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"🎯 Repare como palavras similares ficam próximas no espaço!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdução-à-llms-modulo-05_img_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 A Evolução dos Embeddings\n\nOs embeddings evoluíram muito ao longo do tempo. Vamos entender essa jornada:\n\n```mermaid\ngraph TD\n    A[One-Hot Encoding] --> B[Word2Vec]\n    B --> C[GloVe]\n    C --> D[FastText]\n    D --> E[Embeddings Contextuais]\n    E --> F[BERT/GPT Embeddings]\n    \n    A --> A1[\"Problema: Vetores enormes e esparsos\"]\n    B --> B1[\"Solução: Vetores densos e semânticos\"]\n    C --> C1[\"Melhoria: Considera estatísticas globais\"]\n    D --> D1[\"Inovação: Considera subpalavras\"]\n    E --> E1[\"Revolução: Contexto importa!\"]\n    F --> F1[\"Estado da arte: Transformers\"]\n```\n\n### 1. One-Hot Encoding (O Início Ingênuo)\n\nEra como dar um CPF único para cada palavra, mas sem nenhuma relação semântica:\n- Cachorro: [1, 0, 0, 0, 0]\n- Gato: [0, 1, 0, 0, 0]\n\n**Problema:** Vetores gigantes e sem significado semântico!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo de One-Hot Encoding\n",
        "vocabulario = ['cachorro', 'gato', 'elefante', 'rato', 'leão']\n",
        "\n",
        "def one_hot_encoding(palavra, vocab):\n",
        "    \"\"\"Cria one-hot encoding para uma palavra\"\"\"\n",
        "    vector = [0] * len(vocab)\n",
        "    if palavra in vocab:\n",
        "        index = vocab.index(palavra)\n",
        "        vector[index] = 1\n",
        "    return vector\n",
        "\n",
        "# Testando\n",
        "print(\"One-Hot Encoding:\")\n",
        "for palavra in vocabulario[:3]:\n",
        "    encoding = one_hot_encoding(palavra, vocabulario)\n",
        "    print(f\"{palavra}: {encoding}\")\n",
        "\n",
        "# Calculando similaridade (spoiler: sempre será 0!)\n",
        "cachorro_vec = one_hot_encoding('cachorro', vocabulario)\n",
        "gato_vec = one_hot_encoding('gato', vocabulario)\n",
        "\n",
        "similarity = cosine_similarity([cachorro_vec], [gato_vec])[0][0]\n",
        "print(f\"\\n🤔 Similaridade entre 'cachorro' e 'gato': {similarity}\")\n",
        "print(\"Isso não faz sentido! Eles são ambos animais domésticos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Word2Vec: A Revolução dos Embeddings\n\nEm 2013, o Google lançou o Word2Vec e mudou tudo! A ideia genial foi:\n\n**\"Me diga com quem andas e te direi quem és\"**\n\nPalavras que aparecem em contextos similares têm significados similares!\n\n### Como funciona?\n\nExistem duas abordagens:\n1. **CBOW (Continuous Bag of Words):** Prediz a palavra central dado o contexto\n2. **Skip-gram:** Prediz o contexto dada a palavra central\n\n**Dica do Pedro:** É como aprender português ouvindo conversas. Se você sempre ouve \"cachorro\" perto de \"late\", \"peludo\", \"animal\", você aprende que eles estão relacionados! 🐕"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdução-à-llms-modulo-05_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos implementar um Word2Vec simplificado do zero!\n",
        "# (Versão didática para entender o conceito)\n",
        "\n",
        "class SimpleWord2Vec:\n",
        "    def __init__(self, vector_size=100, window=2, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.vocab = {}\n",
        "        self.word_vectors = {}\n",
        "        \n",
        "    def build_vocab(self, sentences):\n",
        "        \"\"\"Constrói o vocabulário a partir das sentenças\"\"\"\n",
        "        word_count = {}\n",
        "        \n",
        "        # Conta frequência das palavras\n",
        "        for sentence in sentences:\n",
        "            for word in sentence.split():\n",
        "                word = word.lower().strip('.,!?')\n",
        "                word_count[word] = word_count.get(word, 0) + 1\n",
        "        \n",
        "        # Filtra palavras por frequência mínima\n",
        "        self.vocab = {word: idx for idx, (word, count) in enumerate(word_count.items()) \n",
        "                     if count >= self.min_count}\n",
        "        \n",
        "        # Inicializa vetores aleatórios\n",
        "        np.random.seed(42)\n",
        "        for word in self.vocab:\n",
        "            self.word_vectors[word] = np.random.normal(0, 0.1, self.vector_size)\n",
        "            \n",
        "        print(f\"📚 Vocabulário construído com {len(self.vocab)} palavras!\")\n",
        "        \n",
        "    def get_vector(self, word):\n",
        "        \"\"\"Retorna o vetor de uma palavra\"\"\"\n",
        "        return self.word_vectors.get(word.lower(), None)\n",
        "    \n",
        "    def similarity(self, word1, word2):\n",
        "        \"\"\"Calcula similaridade cosseno entre duas palavras\"\"\"\n",
        "        vec1 = self.get_vector(word1)\n",
        "        vec2 = self.get_vector(word2)\n",
        "        \n",
        "        if vec1 is None or vec2 is None:\n",
        "            return 0\n",
        "            \n",
        "        return cosine_similarity([vec1], [vec2])[0][0]\n",
        "\n",
        "# Testando nosso Word2Vec simples\n",
        "sentences = [\n",
        "    \"o cachorro late muito alto\",\n",
        "    \"o gato mia baixinho\",\n",
        "    \"cachorros e gatos são animais domésticos\",\n",
        "    \"animais domésticos precisam de cuidados\",\n",
        "    \"o elefante é um animal gigante\",\n",
        "    \"elefantes vivem na savana africana\",\n",
        "    \"gatos gostam de pescar peixes\",\n",
        "    \"cachorros adoram brincar no parque\"\n",
        "]\n",
        "\n",
        "model = SimpleWord2Vec(vector_size=50)\n",
        "model.build_vocab(sentences)\n",
        "\n",
        "print(f\"\\n🔍 Palavras no vocabulário: {list(model.vocab.keys())[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📈 Visualizando Embeddings em Ação\n\nAgora vamos ver como os embeddings capturam relações semânticas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar embeddings mais realistas para demonstração\n",
        "np.random.seed(42)\n",
        "\n",
        "# Embeddings pré-definidos que capturam relações semânticas\n",
        "embeddings_realistas = {\n",
        "    # Animais domésticos\n",
        "    'cachorro': np.array([0.8, 0.6, 0.9, 0.7, 0.5]),\n",
        "    'gato': np.array([0.9, 0.5, 0.8, 0.6, 0.4]),\n",
        "    \n",
        "    # Animais selvagens\n",
        "    'leão': np.array([0.7, 0.9, 0.6, 0.8, 0.9]),\n",
        "    'elefante': np.array([0.6, 0.8, 0.5, 0.9, 0.8]),\n",
        "    \n",
        "    # Objetos\n",
        "    'carro': np.array([0.2, 0.1, 0.3, 0.2, 0.1]),\n",
        "    'casa': np.array([0.1, 0.2, 0.1, 0.3, 0.2]),\n",
        "    \n",
        "    # Comidas\n",
        "    'pizza': np.array([0.3, 0.2, 0.4, 0.1, 0.3]),\n",
        "    'hamburguer': np.array([0.4, 0.3, 0.5, 0.2, 0.4])\n",
        "}\n",
        "\n",
        "# Calculando matriz de similaridades\n",
        "palavras = list(embeddings_realistas.keys())\n",
        "vectors = list(embeddings_realistas.values())\n",
        "\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "# Criando heatmap de similaridades\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "im = ax.imshow(similarity_matrix, cmap='RdYlBu_r', aspect='auto')\n",
        "\n",
        "# Configurações do plot\n",
        "ax.set_xticks(range(len(palavras)))\n",
        "ax.set_yticks(range(len(palavras)))\n",
        "ax.set_xticklabels(palavras, rotation=45, ha='right')\n",
        "ax.set_yticklabels(palavras)\n",
        "\n",
        "# Adicionando valores na matriz\n",
        "for i in range(len(palavras)):\n",
        "    for j in range(len(palavras)):\n",
        "        text = ax.text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
        "                      ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "\n",
        "ax.set_title('Matriz de Similaridade dos Embeddings\\n(Quanto mais vermelho, mais similar)', \n",
        "             fontsize=14, pad=20)\n",
        "\n",
        "# Colorbar\n",
        "cbar = plt.colorbar(im, ax=ax)\n",
        "cbar.set_label('Similaridade Cosseno', rotation=270, labelpad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"🎯 Repare como:\")\n",
        "print(\"- Cachorro e gato têm alta similaridade (ambos animais domésticos)\")\n",
        "print(\"- Leão e elefante também são similares (animais selvagens)\")\n",
        "print(\"- Pizza e hambúrguer têm similaridade (ambos comidas)\")\n",
        "print(\"- Animais vs objetos têm baixa similaridade\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdução-à-llms-modulo-05_img_03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧠 Embeddings Contextuais: A Nova Era\n\nAqui vem a parada mais liiinda! Os embeddings contextuais (como os usados em BERT, GPT, etc.) não dão apenas UM vetor por palavra. Eles criam vetores diferentes dependendo do CONTEXTO!\n\n### Por que isso é revolucionário?\n\nPense na palavra \"banco\":\n- \"Vou ao **banco** sacar dinheiro\" (instituição financeira)\n- \"Sentei no **banco** da praça\" (assento)\n\nEmbeddings tradicionais dariam o mesmo vetor para ambos. Embeddings contextuais dão vetores diferentes!\n\n**Dica do Pedro:** É como ter um tradutor que entende o contexto da conversa, não só palavras isoladas! 🎯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulando embeddings contextuais\n",
        "# (Na prática, isso vem de modelos como BERT/GPT)\n",
        "\n",
        "def gerar_embedding_contextual(palavra, contexto, dimensoes=5):\n",
        "    \"\"\"\n",
        "    Simula como embeddings contextuais funcionam\n",
        "    \"\"\"\n",
        "    np.random.seed(hash(contexto) % 1000)  # Seed baseada no contexto\n",
        "    \n",
        "    # Base embedding da palavra\n",
        "    base_embedding = np.random.normal(0, 0.5, dimensoes)\n",
        "    \n",
        "    # Modificação contextual\n",
        "    context_modification = np.random.normal(0, 0.3, dimensoes)\n",
        "    \n",
        "    return base_embedding + context_modification\n",
        "\n",
        "# Testando com a palavra \"banco\" em contextos diferentes\n",
        "contextos = {\n",
        "    \"financeiro\": \"Vou ao banco sacar dinheiro para pagar as contas\",\n",
        "    \"assento\": \"Sentei no banco da praça para descansar um pouco\",\n",
        "    \"dados\": \"O banco de dados do sistema está funcionando perfeitamente\"\n",
        "}\n",
        "\n",
        "embeddings_contextuais = {}\n",
        "\n",
        "print(\"🎭 Diferentes embeddings para 'banco' conforme o contexto:\\n\")\n",
        "\n",
        "for tipo, contexto in contextos.items():\n",
        "    embedding = gerar_embedding_contextual(\"banco\", contexto)\n",
        "    embeddings_contextuais[tipo] = embedding\n",
        "    print(f\"{tipo.upper()}:\")\n",
        "    print(f\"Contexto: {contexto}\")\n",
        "    print(f\"Embedding: {embedding.round(3)}\\n\")\n",
        "\n",
        "# Calculando similaridades\n",
        "tipos = list(embeddings_contextuais.keys())\n",
        "print(\"🔍 Similaridades entre os diferentes contextos:\")\n",
        "\n",
        "for i, tipo1 in enumerate(tipos):\n",
        "    for tipo2 in tipos[i+1:]:\n",
        "        sim = cosine_similarity([embeddings_contextuais[tipo1]], \n",
        "                               [embeddings_contextuais[tipo2]])[0][0]\n",
        "        print(f\"{tipo1} vs {tipo2}: {sim:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Como os Transformers Usam Embeddings\n\nLembra dos Transformers que estudamos no Módulo 3? Eles usam três tipos de embeddings:\n\n```mermaid\ngraph LR\n    A[Token] --> B[Token Embedding]\n    A --> C[Position Embedding] \n    A --> D[Segment Embedding]\n    \n    B --> E[Soma]\n    C --> E\n    D --> E\n    \n    E --> F[Input para Transformer]\n    \n    B --> B1[\"Significado da palavra\"]\n    C --> C1[\"Posição na sequência\"]\n    D --> D1[\"Qual sentença (BERT)\"]\n```\n\n### 1. Token Embeddings\nO significado semântico da palavra/token\n\n### 2. Position Embeddings  \nA posição da palavra na sequência (muito importante!)\n\n### 3. Segment Embeddings\nUsado em modelos como BERT para distinguir diferentes sentenças"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando os três tipos de embeddings usados em Transformers\n",
        "\n",
        "class TransformerEmbeddings:\n",
        "    def __init__(self, vocab_size=1000, embed_dim=128, max_length=512):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Inicializando embeddings aleatórios (na prática, são aprendidos)\n",
        "        np.random.seed(42)\n",
        "        self.token_embeddings = np.random.normal(0, 0.1, (vocab_size, embed_dim))\n",
        "        self.position_embeddings = np.random.normal(0, 0.1, (max_length, embed_dim))\n",
        "        self.segment_embeddings = np.random.normal(0, 0.1, (2, embed_dim))  # 2 segmentos\n",
        "        \n",
        "    def get_embeddings(self, token_ids, positions, segment_ids):\n",
        "        \"\"\"\n",
        "        Combina os três tipos de embeddings\n",
        "        \"\"\"\n",
        "        # Token embeddings\n",
        "        token_emb = self.token_embeddings[token_ids]\n",
        "        \n",
        "        # Position embeddings\n",
        "        pos_emb = self.position_embeddings[positions]\n",
        "        \n",
        "        # Segment embeddings\n",
        "        seg_emb = self.segment_embeddings[segment_ids]\n",
        "        \n",
        "        # Soma tudo (como fazem os Transformers)\n",
        "        final_embeddings = token_emb + pos_emb + seg_emb\n",
        "        \n",
        "        return {\n",
        "            'token': token_emb,\n",
        "            'position': pos_emb, \n",
        "            'segment': seg_emb,\n",
        "            'final': final_embeddings\n",
        "        }\n",
        "\n",
        "# Testando\n",
        "embedder = TransformerEmbeddings(embed_dim=8)  # Dimensão pequena para visualizar\n",
        "\n",
        "# Exemplo: \"O gato subiu no telhado\"\n",
        "token_ids = [1, 45, 234, 12, 567]  # IDs dos tokens\n",
        "positions = [0, 1, 2, 3, 4]        # Posições\n",
        "segment_ids = [0, 0, 0, 0, 0]      # Todos da mesma sentença\n",
        "\n",
        "embeddings = embedder.get_embeddings(token_ids, positions, segment_ids)\n",
        "\n",
        "print(\"🔧 Embeddings dos Transformers:\")\n",
        "print(f\"\\nToken embeddings shape: {embeddings['token'].shape}\")\n",
        "print(f\"Position embeddings shape: {embeddings['position'].shape}\")\n",
        "print(f\"Segment embeddings shape: {embeddings['segment'].shape}\")\n",
        "print(f\"\\nFinal embeddings (primeiro token): {embeddings['final'][0].round(3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📏 Métricas e Avaliação de Embeddings\n\nComo sabemos se nossos embeddings são bons? Existem várias métricas:\n\n### 1. Similaridade Cosseno\nMede o ângulo entre dois vetores (0 = ortogonais, 1 = idênticos)\n\n### 2. Analogias\n\"Rei\" - \"Homem\" + \"Mulher\" ≈ \"Rainha\"\n\n### 3. Clustering\nPalavras similares devem formar clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando avaliação de embeddings\n",
        "\n",
        "def avaliar_analogias(embeddings_dict):\n",
        "    \"\"\"\n",
        "    Testa analogias do tipo: A - B + C ≈ D\n",
        "    \"\"\"\n",
        "    analogias = [\n",
        "        ('rei', 'homem', 'mulher', 'rainha'),\n",
        "        ('paris', 'frança', 'brasil', 'brasilia'),\n",
        "        ('grande', 'maior', 'pequeno', 'menor')\n",
        "    ]\n",
        "    \n",
        "    print(\"🧮 Testando analogias (conceitual):\")\n",
        "    \n",
        "    for a, b, c, d_esperado in analogias:\n",
        "        if all(word in embeddings_dict for word in [a, b, c]):\n",
        "            # Calcula: A - B + C\n",
        "            resultado = (embeddings_dict[a] - embeddings_dict[b] + embeddings_dict[c])\n",
        "            \n",
        "            # Encontra a palavra mais próxima\n",
        "            max_sim = -1\n",
        "            palavra_mais_proxima = None\n",
        "            \n",
        "            for palavra, embedding in embeddings_dict.items():\n",
        "                if palavra not in [a, b, c]:  # Exclui palavras da entrada\n",
        "                    sim = cosine_similarity([resultado], [embedding])[0][0]\n",
        "                    if sim > max_sim:\n",
        "                        max_sim = sim\n",
        "                        palavra_mais_proxima = palavra\n",
        "            \n",
        "            print(f\"{a} - {b} + {c} = {palavra_mais_proxima} (esperado: {d_esperado})\")\n",
        "            print(f\"Similaridade: {max_sim:.3f}\\n\")\n",
        "\n",
        "# Criando embeddings de exemplo para teste\n",
        "embeddings_teste = {\n",
        "    'rei': np.array([0.8, 0.9, 0.7, 0.6]),\n",
        "    'rainha': np.array([0.9, 0.8, 0.6, 0.7]), \n",
        "    'homem': np.array([0.7, 0.5, 0.8, 0.4]),\n",
        "    'mulher': np.array([0.8, 0.4, 0.7, 0.5]),\n",
        "    'paris': np.array([0.3, 0.8, 0.9, 0.2]),\n",
        "    'brasilia': np.array([0.4, 0.7, 0.8, 0.3]),\n",
        "    'frança': np.array([0.2, 0.9, 0.8, 0.1]),\n",
        "    'brasil': np.array([0.3, 0.8, 0.7, 0.2])\n",
        "}\n",
        "\n",
        "avaliar_analogias(embeddings_teste)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎨 Visualização em 2D com PCA\n\nEmbeddings geralmente têm centenas de dimensões. Para visualizar, usamos PCA (Análise de Componentes Principais) para reduzir para 2D:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando embeddings mais ricos para visualização\n",
        "np.random.seed(42)\n",
        "\n",
        "# Categorias de palavras com embeddings similares dentro da categoria\n",
        "categorias = {\n",
        "    'animais': ['cachorro', 'gato', 'leão', 'tigre', 'elefante'],\n",
        "    'comidas': ['pizza', 'hamburguer', 'sushi', 'lasanha', 'salada'],\n",
        "    'veiculos': ['carro', 'moto', 'avião', 'barco', 'bicicleta'],\n",
        "    'cores': ['vermelho', 'azul', 'verde', 'amarelo', 'roxo']\n",
        "}\n",
        "\n",
        "# Gerando embeddings agrupados por categoria\n",
        "embeddings_categorias = {}\n",
        "cores_plot = ['red', 'blue', 'green', 'orange']\n",
        "cores_mapa = {}\n",
        "\n",
        "for i, (categoria, palavras) in enumerate(categorias.items()):\n",
        "    # Centro da categoria\n",
        "    centro = np.random.normal(i*3, 1, 10)  # 10 dimensões\n",
        "    \n",
        "    for palavra in palavras:\n",
        "        # Adiciona ruído ao centro para criar cluster\n",
        "        embedding = centro + np.random.normal(0, 0.5, 10)\n",
        "        embeddings_categorias[palavra] = embedding\n",
        "        cores_mapa[palavra] = cores_plot[i]\n",
        "\n",
        "# Aplicando PCA para reduzir para 2D\n",
        "palavras = list(embeddings_categorias.keys())\n",
        "vectors = list(embeddings_categorias.values())\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(vectors)\n",
        "\n",
        "# Plotando\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "# Plot por categoria\n",
        "for i, (categoria, palavras_cat) in enumerate(categorias.items()):\n",
        "    indices = [palavras.index(p) for p in palavras_cat]\n",
        "    x_coords = [embeddings_2d[idx][0] for idx in indices]\n",
        "    y_coords = [embeddings_2d[idx][1] for idx in indices]\n",
        "    \n",
        "    ax.scatter(x_coords, y_coords, c=cores_plot[i], label=categoria.title(), \n",
        "              s=200, alpha=0.7, edgecolors='black', linewidth=1)\n",
        "    \n",
        "    # Anotações\n",
        "    for idx, palavra in zip(indices, palavras_cat):\n",
        "        ax.annotate(palavra, (embeddings_2d[idx][0], embeddings_2d[idx][1]), \n",
        "                   xytext=(5, 5), textcoords='offset points', \n",
        "                   fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} da variância)', fontsize=12)\n",
        "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} da variância)', fontsize=12)\n",
        "ax.set_title('Visualização de Embeddings em 2D\\n(Redução dimensional com PCA)', fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"🎯 PCA explica {sum(pca.explained_variance_ratio_):.2%} da variância total\")\n",
        "print(\"Repare como palavras da mesma categoria ficam agrupadas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdução-à-llms-modulo-05_img_04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔗 Conectando com os Próximos Módulos\n\nOs embeddings que aprendemos hoje são fundamentais para entender:\n\n### Módulo 6 - Tipos de Modelos\n- Como diferentes arquiteturas (BERT, GPT, T5) usam embeddings\n- Embeddings especializados para tarefas específicas\n\n### Módulo 7 - Treinamento\n- Como os embeddings são aprendidos durante o treinamento\n- Técnicas de fine-tuning de embeddings\n\n### Módulo 8 - Prompting\n- Como prompts são convertidos em embeddings\n- Engenharia de prompts no espaço vetorial\n\n**Dica do Pedro:** Embeddings são como o \"sistema circulatório\" dos LLMs - eles transportam significado por toda a rede! 🩸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🏋️ Exercício Prático 1: Construindo seu Próprio Sistema de Embeddings\n\nAgora é sua vez! Vamos implementar um sistema completo de embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÍCIO 1: Complete o código abaixo\n",
        "\n",
        "class MeuSistemaEmbeddings:\n",
        "    def __init__(self, dimensoes=50):\n",
        "        self.dimensoes = dimensoes\n",
        "        self.vocab = {}\n",
        "        self.embeddings = {}\n",
        "        self.treinado = False\n",
        "    \n",
        "    def treinar(self, textos):\n",
        "        \"\"\"\n",
        "        TODO: Implemente o treinamento dos embeddings\n",
        "        \n",
        "        Passos:\n",
        "        1. Construa o vocabulário a partir dos textos\n",
        "        2. Inicialize embeddings aleatórios para cada palavra\n",
        "        3. (Bonus) Implemente uma versão simples de co-ocorrência\n",
        "        \"\"\"\n",
        "        pass  # Substitua por sua implementação\n",
        "    \n",
        "    def obter_embedding(self, palavra):\n",
        "        \"\"\"\n",
        "        TODO: Retorne o embedding de uma palavra\n",
        "        \"\"\"\n",
        "        pass  # Substitua por sua implementação\n",
        "    \n",
        "    def similaridade(self, palavra1, palavra2):\n",
        "        \"\"\"\n",
        "        TODO: Calcule a similaridade cosseno entre duas palavras\n",
        "        \"\"\"\n",
        "        pass  # Substitua por sua implementação\n",
        "    \n",
        "    def palavras_similares(self, palavra, top_k=3):\n",
        "        \"\"\"\n",
        "        TODO: Encontre as k palavras mais similares\n",
        "        \"\"\"\n",
        "        pass  # Substitua por sua implementação\n",
        "\n",
        "# Dados para teste\n",
        "textos_treino = [\n",
        "    \"cachorro late muito alto no quintal\",\n",
        "    \"gato mia baixinho pela manhã\", \n",
        "    \"carro vermelho anda na estrada\",\n",
        "    \"bicicleta azul está na garagem\",\n",
        "    \"pizza deliciosa com queijo\",\n",
        "    \"hamburguer saboroso com batata\"\n",
        "]\n",
        "\n",
        "# Teste seu sistema aqui\n",
        "# meu_sistema = MeuSistemaEmbeddings()\n",
        "# meu_sistema.treinar(textos_treino)\n",
        "# print(meu_sistema.similaridade('cachorro', 'gato'))\n",
        "\n",
        "print(\"🎯 Complete o código acima e teste seu sistema de embeddings!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧪 Exercício Prático 2: Análise de Bias em Embeddings\n\nEmbeddings podem carregar preconceitos dos dados de treino. Vamos investigar isso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÍCIO 2: Detectando bias em embeddings\n",
        "\n",
        "# Embeddings com bias simulado\n",
        "embeddings_com_bias = {\n",
        "    'médico': np.array([0.8, 0.7, 0.9, 0.6]),\n",
        "    'enfermeira': np.array([0.3, 0.8, 0.4, 0.9]),\n",
        "    'engenheiro': np.array([0.9, 0.6, 0.8, 0.5]),\n",
        "    'professor': np.array([0.5, 0.7, 0.6, 0.8]),\n",
        "    'homem': np.array([0.9, 0.5, 0.8, 0.4]),\n",
        "    'mulher': np.array([0.2, 0.9, 0.3, 0.8]),\n",
        "    'forte': np.array([0.8, 0.4, 0.9, 0.3]),\n",
        "    'delicada': np.array([0.2, 0.8, 0.3, 0.9])\n",
        "}\n",
        "\n",
        "def analisar_bias(embeddings_dict, profissoes, generos, adjetivos):\n",
        "    \"\"\"\n",
        "    TODO: Analise associações entre profissões, gêneros e adjetivos\n",
        "    \n",
        "    Calcule:\n",
        "    1. Similaridade entre profissões e gêneros\n",
        "    2. Similaridade entre gêneros e adjetivos\n",
        "    3. Identifique possíveis vieses\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"🔍 ANÁLISE DE BIAS EM EMBEDDINGS\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # TODO: Implemente sua análise aqui\n",
        "    \n",
        "    # Exemplo de estrutura:\n",
        "    # for profissao in profissoes:\n",
        "    #     for genero in generos:\n",
        "    #         sim = calcular_similaridade(profissao, genero)\n",
        "    #         print(f\"{profissao} <-> {genero}: {sim:.3f}\")\n",
        "    \n",
        "    pass\n",
        "\n",
        "# Teste sua análise\n",
        "profissoes = ['médico', 'enfermeira', 'engenheiro', 'professor']\n",
        "generos = ['homem', 'mulher']\n",
        "adjetivos = ['forte', 'delicada']\n",
        "\n",
        "# analisar_bias(embeddings_com_bias, profissoes, generos, adjetivos)\n",
        "\n",
        "print(\"\\n💡 Dicas para sua implementação:\")\n",
        "print(\"- Use similaridade cosseno\")\n",
        "print(\"- Procure por associações problemáticas\")\n",
        "print(\"- Pense em como mitigar esses vieses\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdução-à-llms-modulo-05_img_05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Comparando Diferentes Tipos de Embeddings\n\nVamos fazer uma comparação final entre os principais tipos de embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparação final entre tipos de embeddings\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Criando tabela comparativa\n",
        "comparacao = {\n",
        "    'Tipo': ['One-Hot', 'Word2Vec', 'GloVe', 'FastText', 'BERT/GPT'],\n",
        "    'Ano': [1990, 2013, 2014, 2017, 2018],\n",
        "    'Dimensões': ['Vocabulário', '50-300', '50-300', '100-300', '768-1024'],\n",
        "    'Contextual': ['Não', 'Não', 'Não', 'Não', 'Sim'],\n",
        "    'Subpalavras': ['Não', 'Não', 'Não', 'Sim', 'Sim'],\n",
        "    'Vantagem Principal': [\n",
        "        'Simplicidade',\n",
        "        'Semântica capturada',\n",
        "        'Estatísticas globais',\n",
        "        'Palavras raras/OOV',\n",
        "        'Contexto dinâmico'\n",
        "    ],\n",
        "    'Desvantagem': [\n",
        "        'Sem semântica',\n",
        "        'Sem contexto',\n",
        "        'Computacionalmente caro',\n",
        "        'Treino complexo',\n",
        "        'Muito pesado'\n",
        "    ]\n",
        "}\n",
        "\n",
        "df_comparacao = pd.DataFrame(comparacao)\n",
        "print(\"📊 EVOLUÇÃO DOS EMBEDDINGS\")\n",
        "print(\"=\" * 50)\n",
        "print(df_comparacao.to_string(index=False))\n",
        "\n",
        "# Gráfico da evolução temporal\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Timeline\n",
        "anos = df_comparacao['Ano'].values\n",
        "tipos = df_comparacao['Tipo'].values\n",
        "cores = ['red', 'orange', 'yellow', 'lightgreen', 'darkgreen']\n",
        "\n",
        "ax1.scatter(anos, range(len(tipos)), c=cores, s=200, alpha=0.7)\n",
        "for i, (ano, tipo) in enumerate(zip(anos, tipos)):\n",
        "    ax1.annotate(tipo, (ano, i), xytext=(10, 0), \n",
        "                textcoords='offset points', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax1.set_xlabel('Ano', fontsize=12)\n",
        "ax1.set_ylabel('Evolução', fontsize=12)\n",
        "ax1.set_title('Timeline dos Embeddings', fontsize=14)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_yticks([])\n",
        "\n",
        "# Complexidade vs Performance (conceitual)\n",
        "complexidade = [1, 3, 4, 5, 8]\n",
        "performance = [1, 6, 7, 8, 10]\n",
        "\n",
        "ax2.scatter(complexidade, performance, c=cores, s=200, alpha=0.7)\n",
        "for i, tipo in enumerate(tipos):\n",
        "    ax2.annotate(tipo, (complexidade[i], performance[i]), \n",
        "                xytext=(5, 5), textcoords='offset points', \n",
        "                fontsize=10, fontweight='bold')\n",
        "\n",
        "ax2.set_xlabel('Complexidade', fontsize=12)\n",
        "ax2.set_ylabel('Performance', fontsize=12)\n",
        "ax2.set_title('Complexidade vs Performance', fontsize=14)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n🎯 Lições principais:\")\n",
        "print(\"• Embeddings evoluíram de simples para sofisticados\")\n",
        "print(\"• Trade-off entre simplicidade e performance\")\n",
        "print(\"• Contexto fez toda a diferença!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdução-à-llms-modulo-05_img_06.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎓 Resumo do Módulo: Embeddings Dominados!\n\nLiiindo! Você chegou ao final do Módulo 5! Vamos recapitular o que aprendemos:\n\n### 🧠 Conceitos Principais\n- **Embeddings**: Representações numéricas que capturam significado semântico\n- **Evolução**: De One-Hot → Word2Vec → Embeddings Contextuais\n- **Dimensionalidade**: Como reduzir dimensões mantendo informação\n- **Similaridade**: Medindo relações entre palavras no espaço vetorial\n\n### 🔧 Técnicas Aprendidas\n- Implementação de embeddings simples\n- Visualização com PCA\n- Avaliação de qualidade (analogias, similaridade)\n- Detecção de bias\n\n### 🚀 Próximos Passos\nNo **Módulo 6 - Tipos de Modelos**, veremos como diferentes arquiteturas (BERT, GPT, T5) implementam e utilizam embeddings de formas específicas.\n\n**Dica Final do Pedro:** Embeddings são a base de tudo em NLP moderno. Dominar esse conceito é como ter a chave do reino dos LLMs! 🗝️\n\n### 📚 Para Praticar Mais\n- Experimente com datasets maiores\n- Teste embeddings pré-treinados (Word2Vec do Google, GloVe)\n- Explore bibliotecas como Gensim e Hugging Face\n- Analise embeddings de diferentes domínios (médico, jurídico, etc.)\n\n---\n\n**Bora para o próximo módulo! 🚀**"
      ]
    }
  ]
}