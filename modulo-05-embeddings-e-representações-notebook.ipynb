{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Embeddings e Representa√ß√µes: Transformando Palavras em N√∫meros M√°gicos\n\n**M√≥dulo 5 de 13 - Introdu√ß√£o √† LLMs**\n\nPor: Pedro Nunes Guth üöÄ\n\n---\n\nBora mergulhar no mundo dos embeddings! Se voc√™ chegou at√© aqui, j√° entendeu sobre tokens e tokeniza√ß√£o no m√≥dulo anterior. Agora vamos descobrir como transformar essas palavrinhas em n√∫meros que fazem sentido para os computadores!\n\n**O que voc√™ vai aprender:**\n- O que s√£o embeddings (spoiler: n√£o √© comida!)\n- Como representar palavras em vetores\n- Word2Vec, GloVe e embeddings contextuais\n- Como os Transformers usam embeddings\n- Na pr√°tica: implementando embeddings do zero!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bora come√ßar importando as bibliotecas que vamos usar!\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√µes visuais\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"üéâ Bibliotecas carregadas! Bora para os embeddings!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î T√°, mas o que s√£o Embeddings?\n\nImagina que voc√™ √© um tradutor tentando explicar para um alien√≠gena o que significa \"cachorro\". Como voc√™ faria isso?\n\nVoc√™ poderia dizer:\n- \"√â um animal que late\"\n- \"Tem 4 patas\"\n- \"√â peludo\"\n- \"√â amigo dos humanos\"\n\n**Embeddings fazem exatamente isso!** Eles pegam uma palavra e a representam atrav√©s de n√∫meros que capturam suas caracter√≠sticas e significados.\n\n### Por que precisamos disso?\n\nComputadores n√£o entendem \"cachorro\" ou \"gato\". Eles s√≥ entendem n√∫meros! Ent√£o precisamos converter palavras em vetores num√©ricos que preservem o significado sem√¢ntico.\n\n**Dica do Pedro:** Think of embeddings como o \"DNA num√©rico\" das palavras! üß¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visualizando o Conceito de Embeddings\n\nVamos come√ßar com um exemplo visual para entender como palavras podem ser representadas em um espa√ßo vetorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar embeddings simples para algumas palavras\n",
        "# Imagine que temos apenas 2 dimens√µes: \"animal\" e \"tamanho\"\n",
        "\n",
        "palavras = ['cachorro', 'gato', 'elefante', 'rato', 'le√£o', 'peixe']\n",
        "\n",
        "# Embeddings 2D simples [dimens√£o_animal, dimens√£o_tamanho]\n",
        "embeddings_2d = {\n",
        "    'cachorro': [0.8, 0.6],   # Muito animal, tamanho m√©dio\n",
        "    'gato': [0.9, 0.4],       # Muito animal, pequeno\n",
        "    'elefante': [0.7, 0.9],   # Animal, muito grande\n",
        "    'rato': [0.6, 0.1],       # Pouco animal, muito pequeno\n",
        "    'le√£o': [0.85, 0.8],      # Muito animal, grande\n",
        "    'peixe': [0.5, 0.3]       # Pouco animal (diferente), pequeno\n",
        "}\n",
        "\n",
        "# Plotando os embeddings\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "for palavra, (x, y) in embeddings_2d.items():\n",
        "    ax.scatter(x, y, s=200, alpha=0.7)\n",
        "    ax.annotate(palavra, (x, y), xytext=(5, 5), textcoords='offset points', \n",
        "                fontsize=12, fontweight='bold')\n",
        "\n",
        "ax.set_xlabel('Dimens√£o: Animalidade', fontsize=14)\n",
        "ax.set_ylabel('Dimens√£o: Tamanho', fontsize=14)\n",
        "ax.set_title('Embeddings 2D Simplificados\\n(Palavras no Espa√ßo Vetorial)', fontsize=16)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Repare como palavras similares ficam pr√≥ximas no espa√ßo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-05_img_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ A Evolu√ß√£o dos Embeddings\n\nOs embeddings evolu√≠ram muito ao longo do tempo. Vamos entender essa jornada:\n\n```mermaid\ngraph TD\n    A[One-Hot Encoding] --> B[Word2Vec]\n    B --> C[GloVe]\n    C --> D[FastText]\n    D --> E[Embeddings Contextuais]\n    E --> F[BERT/GPT Embeddings]\n    \n    A --> A1[\"Problema: Vetores enormes e esparsos\"]\n    B --> B1[\"Solu√ß√£o: Vetores densos e sem√¢nticos\"]\n    C --> C1[\"Melhoria: Considera estat√≠sticas globais\"]\n    D --> D1[\"Inova√ß√£o: Considera subpalavras\"]\n    E --> E1[\"Revolu√ß√£o: Contexto importa!\"]\n    F --> F1[\"Estado da arte: Transformers\"]\n```\n\n### 1. One-Hot Encoding (O In√≠cio Ing√™nuo)\n\nEra como dar um CPF √∫nico para cada palavra, mas sem nenhuma rela√ß√£o sem√¢ntica:\n- Cachorro: [1, 0, 0, 0, 0]\n- Gato: [0, 1, 0, 0, 0]\n\n**Problema:** Vetores gigantes e sem significado sem√¢ntico!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo de One-Hot Encoding\n",
        "vocabulario = ['cachorro', 'gato', 'elefante', 'rato', 'le√£o']\n",
        "\n",
        "def one_hot_encoding(palavra, vocab):\n",
        "    \"\"\"Cria one-hot encoding para uma palavra\"\"\"\n",
        "    vector = [0] * len(vocab)\n",
        "    if palavra in vocab:\n",
        "        index = vocab.index(palavra)\n",
        "        vector[index] = 1\n",
        "    return vector\n",
        "\n",
        "# Testando\n",
        "print(\"One-Hot Encoding:\")\n",
        "for palavra in vocabulario[:3]:\n",
        "    encoding = one_hot_encoding(palavra, vocabulario)\n",
        "    print(f\"{palavra}: {encoding}\")\n",
        "\n",
        "# Calculando similaridade (spoiler: sempre ser√° 0!)\n",
        "cachorro_vec = one_hot_encoding('cachorro', vocabulario)\n",
        "gato_vec = one_hot_encoding('gato', vocabulario)\n",
        "\n",
        "similarity = cosine_similarity([cachorro_vec], [gato_vec])[0][0]\n",
        "print(f\"\\nü§î Similaridade entre 'cachorro' e 'gato': {similarity}\")\n",
        "print(\"Isso n√£o faz sentido! Eles s√£o ambos animais dom√©sticos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Word2Vec: A Revolu√ß√£o dos Embeddings\n\nEm 2013, o Google lan√ßou o Word2Vec e mudou tudo! A ideia genial foi:\n\n**\"Me diga com quem andas e te direi quem √©s\"**\n\nPalavras que aparecem em contextos similares t√™m significados similares!\n\n### Como funciona?\n\nExistem duas abordagens:\n1. **CBOW (Continuous Bag of Words):** Prediz a palavra central dado o contexto\n2. **Skip-gram:** Prediz o contexto dada a palavra central\n\n**Dica do Pedro:** √â como aprender portugu√™s ouvindo conversas. Se voc√™ sempre ouve \"cachorro\" perto de \"late\", \"peludo\", \"animal\", voc√™ aprende que eles est√£o relacionados! üêï"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-05_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos implementar um Word2Vec simplificado do zero!\n",
        "# (Vers√£o did√°tica para entender o conceito)\n",
        "\n",
        "class SimpleWord2Vec:\n",
        "    def __init__(self, vector_size=100, window=2, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.vocab = {}\n",
        "        self.word_vectors = {}\n",
        "        \n",
        "    def build_vocab(self, sentences):\n",
        "        \"\"\"Constr√≥i o vocabul√°rio a partir das senten√ßas\"\"\"\n",
        "        word_count = {}\n",
        "        \n",
        "        # Conta frequ√™ncia das palavras\n",
        "        for sentence in sentences:\n",
        "            for word in sentence.split():\n",
        "                word = word.lower().strip('.,!?')\n",
        "                word_count[word] = word_count.get(word, 0) + 1\n",
        "        \n",
        "        # Filtra palavras por frequ√™ncia m√≠nima\n",
        "        self.vocab = {word: idx for idx, (word, count) in enumerate(word_count.items()) \n",
        "                     if count >= self.min_count}\n",
        "        \n",
        "        # Inicializa vetores aleat√≥rios\n",
        "        np.random.seed(42)\n",
        "        for word in self.vocab:\n",
        "            self.word_vectors[word] = np.random.normal(0, 0.1, self.vector_size)\n",
        "            \n",
        "        print(f\"üìö Vocabul√°rio constru√≠do com {len(self.vocab)} palavras!\")\n",
        "        \n",
        "    def get_vector(self, word):\n",
        "        \"\"\"Retorna o vetor de uma palavra\"\"\"\n",
        "        return self.word_vectors.get(word.lower(), None)\n",
        "    \n",
        "    def similarity(self, word1, word2):\n",
        "        \"\"\"Calcula similaridade cosseno entre duas palavras\"\"\"\n",
        "        vec1 = self.get_vector(word1)\n",
        "        vec2 = self.get_vector(word2)\n",
        "        \n",
        "        if vec1 is None or vec2 is None:\n",
        "            return 0\n",
        "            \n",
        "        return cosine_similarity([vec1], [vec2])[0][0]\n",
        "\n",
        "# Testando nosso Word2Vec simples\n",
        "sentences = [\n",
        "    \"o cachorro late muito alto\",\n",
        "    \"o gato mia baixinho\",\n",
        "    \"cachorros e gatos s√£o animais dom√©sticos\",\n",
        "    \"animais dom√©sticos precisam de cuidados\",\n",
        "    \"o elefante √© um animal gigante\",\n",
        "    \"elefantes vivem na savana africana\",\n",
        "    \"gatos gostam de pescar peixes\",\n",
        "    \"cachorros adoram brincar no parque\"\n",
        "]\n",
        "\n",
        "model = SimpleWord2Vec(vector_size=50)\n",
        "model.build_vocab(sentences)\n",
        "\n",
        "print(f\"\\nüîç Palavras no vocabul√°rio: {list(model.vocab.keys())[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Visualizando Embeddings em A√ß√£o\n\nAgora vamos ver como os embeddings capturam rela√ß√µes sem√¢nticas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar embeddings mais realistas para demonstra√ß√£o\n",
        "np.random.seed(42)\n",
        "\n",
        "# Embeddings pr√©-definidos que capturam rela√ß√µes sem√¢nticas\n",
        "embeddings_realistas = {\n",
        "    # Animais dom√©sticos\n",
        "    'cachorro': np.array([0.8, 0.6, 0.9, 0.7, 0.5]),\n",
        "    'gato': np.array([0.9, 0.5, 0.8, 0.6, 0.4]),\n",
        "    \n",
        "    # Animais selvagens\n",
        "    'le√£o': np.array([0.7, 0.9, 0.6, 0.8, 0.9]),\n",
        "    'elefante': np.array([0.6, 0.8, 0.5, 0.9, 0.8]),\n",
        "    \n",
        "    # Objetos\n",
        "    'carro': np.array([0.2, 0.1, 0.3, 0.2, 0.1]),\n",
        "    'casa': np.array([0.1, 0.2, 0.1, 0.3, 0.2]),\n",
        "    \n",
        "    # Comidas\n",
        "    'pizza': np.array([0.3, 0.2, 0.4, 0.1, 0.3]),\n",
        "    'hamburguer': np.array([0.4, 0.3, 0.5, 0.2, 0.4])\n",
        "}\n",
        "\n",
        "# Calculando matriz de similaridades\n",
        "palavras = list(embeddings_realistas.keys())\n",
        "vectors = list(embeddings_realistas.values())\n",
        "\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "# Criando heatmap de similaridades\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "im = ax.imshow(similarity_matrix, cmap='RdYlBu_r', aspect='auto')\n",
        "\n",
        "# Configura√ß√µes do plot\n",
        "ax.set_xticks(range(len(palavras)))\n",
        "ax.set_yticks(range(len(palavras)))\n",
        "ax.set_xticklabels(palavras, rotation=45, ha='right')\n",
        "ax.set_yticklabels(palavras)\n",
        "\n",
        "# Adicionando valores na matriz\n",
        "for i in range(len(palavras)):\n",
        "    for j in range(len(palavras)):\n",
        "        text = ax.text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
        "                      ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "\n",
        "ax.set_title('Matriz de Similaridade dos Embeddings\\n(Quanto mais vermelho, mais similar)', \n",
        "             fontsize=14, pad=20)\n",
        "\n",
        "# Colorbar\n",
        "cbar = plt.colorbar(im, ax=ax)\n",
        "cbar.set_label('Similaridade Cosseno', rotation=270, labelpad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Repare como:\")\n",
        "print(\"- Cachorro e gato t√™m alta similaridade (ambos animais dom√©sticos)\")\n",
        "print(\"- Le√£o e elefante tamb√©m s√£o similares (animais selvagens)\")\n",
        "print(\"- Pizza e hamb√∫rguer t√™m similaridade (ambos comidas)\")\n",
        "print(\"- Animais vs objetos t√™m baixa similaridade\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-05_img_03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Embeddings Contextuais: A Nova Era\n\nAqui vem a parada mais liiinda! Os embeddings contextuais (como os usados em BERT, GPT, etc.) n√£o d√£o apenas UM vetor por palavra. Eles criam vetores diferentes dependendo do CONTEXTO!\n\n### Por que isso √© revolucion√°rio?\n\nPense na palavra \"banco\":\n- \"Vou ao **banco** sacar dinheiro\" (institui√ß√£o financeira)\n- \"Sentei no **banco** da pra√ßa\" (assento)\n\nEmbeddings tradicionais dariam o mesmo vetor para ambos. Embeddings contextuais d√£o vetores diferentes!\n\n**Dica do Pedro:** √â como ter um tradutor que entende o contexto da conversa, n√£o s√≥ palavras isoladas! üéØ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulando embeddings contextuais\n",
        "# (Na pr√°tica, isso vem de modelos como BERT/GPT)\n",
        "\n",
        "def gerar_embedding_contextual(palavra, contexto, dimensoes=5):\n",
        "    \"\"\"\n",
        "    Simula como embeddings contextuais funcionam\n",
        "    \"\"\"\n",
        "    np.random.seed(hash(contexto) % 1000)  # Seed baseada no contexto\n",
        "    \n",
        "    # Base embedding da palavra\n",
        "    base_embedding = np.random.normal(0, 0.5, dimensoes)\n",
        "    \n",
        "    # Modifica√ß√£o contextual\n",
        "    context_modification = np.random.normal(0, 0.3, dimensoes)\n",
        "    \n",
        "    return base_embedding + context_modification\n",
        "\n",
        "# Testando com a palavra \"banco\" em contextos diferentes\n",
        "contextos = {\n",
        "    \"financeiro\": \"Vou ao banco sacar dinheiro para pagar as contas\",\n",
        "    \"assento\": \"Sentei no banco da pra√ßa para descansar um pouco\",\n",
        "    \"dados\": \"O banco de dados do sistema est√° funcionando perfeitamente\"\n",
        "}\n",
        "\n",
        "embeddings_contextuais = {}\n",
        "\n",
        "print(\"üé≠ Diferentes embeddings para 'banco' conforme o contexto:\\n\")\n",
        "\n",
        "for tipo, contexto in contextos.items():\n",
        "    embedding = gerar_embedding_contextual(\"banco\", contexto)\n",
        "    embeddings_contextuais[tipo] = embedding\n",
        "    print(f\"{tipo.upper()}:\")\n",
        "    print(f\"Contexto: {contexto}\")\n",
        "    print(f\"Embedding: {embedding.round(3)}\\n\")\n",
        "\n",
        "# Calculando similaridades\n",
        "tipos = list(embeddings_contextuais.keys())\n",
        "print(\"üîç Similaridades entre os diferentes contextos:\")\n",
        "\n",
        "for i, tipo1 in enumerate(tipos):\n",
        "    for tipo2 in tipos[i+1:]:\n",
        "        sim = cosine_similarity([embeddings_contextuais[tipo1]], \n",
        "                               [embeddings_contextuais[tipo2]])[0][0]\n",
        "        print(f\"{tipo1} vs {tipo2}: {sim:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Como os Transformers Usam Embeddings\n\nLembra dos Transformers que estudamos no M√≥dulo 3? Eles usam tr√™s tipos de embeddings:\n\n```mermaid\ngraph LR\n    A[Token] --> B[Token Embedding]\n    A --> C[Position Embedding] \n    A --> D[Segment Embedding]\n    \n    B --> E[Soma]\n    C --> E\n    D --> E\n    \n    E --> F[Input para Transformer]\n    \n    B --> B1[\"Significado da palavra\"]\n    C --> C1[\"Posi√ß√£o na sequ√™ncia\"]\n    D --> D1[\"Qual senten√ßa (BERT)\"]\n```\n\n### 1. Token Embeddings\nO significado sem√¢ntico da palavra/token\n\n### 2. Position Embeddings  \nA posi√ß√£o da palavra na sequ√™ncia (muito importante!)\n\n### 3. Segment Embeddings\nUsado em modelos como BERT para distinguir diferentes senten√ßas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando os tr√™s tipos de embeddings usados em Transformers\n",
        "\n",
        "class TransformerEmbeddings:\n",
        "    def __init__(self, vocab_size=1000, embed_dim=128, max_length=512):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Inicializando embeddings aleat√≥rios (na pr√°tica, s√£o aprendidos)\n",
        "        np.random.seed(42)\n",
        "        self.token_embeddings = np.random.normal(0, 0.1, (vocab_size, embed_dim))\n",
        "        self.position_embeddings = np.random.normal(0, 0.1, (max_length, embed_dim))\n",
        "        self.segment_embeddings = np.random.normal(0, 0.1, (2, embed_dim))  # 2 segmentos\n",
        "        \n",
        "    def get_embeddings(self, token_ids, positions, segment_ids):\n",
        "        \"\"\"\n",
        "        Combina os tr√™s tipos de embeddings\n",
        "        \"\"\"\n",
        "        # Token embeddings\n",
        "        token_emb = self.token_embeddings[token_ids]\n",
        "        \n",
        "        # Position embeddings\n",
        "        pos_emb = self.position_embeddings[positions]\n",
        "        \n",
        "        # Segment embeddings\n",
        "        seg_emb = self.segment_embeddings[segment_ids]\n",
        "        \n",
        "        # Soma tudo (como fazem os Transformers)\n",
        "        final_embeddings = token_emb + pos_emb + seg_emb\n",
        "        \n",
        "        return {\n",
        "            'token': token_emb,\n",
        "            'position': pos_emb, \n",
        "            'segment': seg_emb,\n",
        "            'final': final_embeddings\n",
        "        }\n",
        "\n",
        "# Testando\n",
        "embedder = TransformerEmbeddings(embed_dim=8)  # Dimens√£o pequena para visualizar\n",
        "\n",
        "# Exemplo: \"O gato subiu no telhado\"\n",
        "token_ids = [1, 45, 234, 12, 567]  # IDs dos tokens\n",
        "positions = [0, 1, 2, 3, 4]        # Posi√ß√µes\n",
        "segment_ids = [0, 0, 0, 0, 0]      # Todos da mesma senten√ßa\n",
        "\n",
        "embeddings = embedder.get_embeddings(token_ids, positions, segment_ids)\n",
        "\n",
        "print(\"üîß Embeddings dos Transformers:\")\n",
        "print(f\"\\nToken embeddings shape: {embeddings['token'].shape}\")\n",
        "print(f\"Position embeddings shape: {embeddings['position'].shape}\")\n",
        "print(f\"Segment embeddings shape: {embeddings['segment'].shape}\")\n",
        "print(f\"\\nFinal embeddings (primeiro token): {embeddings['final'][0].round(3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìè M√©tricas e Avalia√ß√£o de Embeddings\n\nComo sabemos se nossos embeddings s√£o bons? Existem v√°rias m√©tricas:\n\n### 1. Similaridade Cosseno\nMede o √¢ngulo entre dois vetores (0 = ortogonais, 1 = id√™nticos)\n\n### 2. Analogias\n\"Rei\" - \"Homem\" + \"Mulher\" ‚âà \"Rainha\"\n\n### 3. Clustering\nPalavras similares devem formar clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando avalia√ß√£o de embeddings\n",
        "\n",
        "def avaliar_analogias(embeddings_dict):\n",
        "    \"\"\"\n",
        "    Testa analogias do tipo: A - B + C ‚âà D\n",
        "    \"\"\"\n",
        "    analogias = [\n",
        "        ('rei', 'homem', 'mulher', 'rainha'),\n",
        "        ('paris', 'fran√ßa', 'brasil', 'brasilia'),\n",
        "        ('grande', 'maior', 'pequeno', 'menor')\n",
        "    ]\n",
        "    \n",
        "    print(\"üßÆ Testando analogias (conceitual):\")\n",
        "    \n",
        "    for a, b, c, d_esperado in analogias:\n",
        "        if all(word in embeddings_dict for word in [a, b, c]):\n",
        "            # Calcula: A - B + C\n",
        "            resultado = (embeddings_dict[a] - embeddings_dict[b] + embeddings_dict[c])\n",
        "            \n",
        "            # Encontra a palavra mais pr√≥xima\n",
        "            max_sim = -1\n",
        "            palavra_mais_proxima = None\n",
        "            \n",
        "            for palavra, embedding in embeddings_dict.items():\n",
        "                if palavra not in [a, b, c]:  # Exclui palavras da entrada\n",
        "                    sim = cosine_similarity([resultado], [embedding])[0][0]\n",
        "                    if sim > max_sim:\n",
        "                        max_sim = sim\n",
        "                        palavra_mais_proxima = palavra\n",
        "            \n",
        "            print(f\"{a} - {b} + {c} = {palavra_mais_proxima} (esperado: {d_esperado})\")\n",
        "            print(f\"Similaridade: {max_sim:.3f}\\n\")\n",
        "\n",
        "# Criando embeddings de exemplo para teste\n",
        "embeddings_teste = {\n",
        "    'rei': np.array([0.8, 0.9, 0.7, 0.6]),\n",
        "    'rainha': np.array([0.9, 0.8, 0.6, 0.7]), \n",
        "    'homem': np.array([0.7, 0.5, 0.8, 0.4]),\n",
        "    'mulher': np.array([0.8, 0.4, 0.7, 0.5]),\n",
        "    'paris': np.array([0.3, 0.8, 0.9, 0.2]),\n",
        "    'brasilia': np.array([0.4, 0.7, 0.8, 0.3]),\n",
        "    'fran√ßa': np.array([0.2, 0.9, 0.8, 0.1]),\n",
        "    'brasil': np.array([0.3, 0.8, 0.7, 0.2])\n",
        "}\n",
        "\n",
        "avaliar_analogias(embeddings_teste)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Visualiza√ß√£o em 2D com PCA\n\nEmbeddings geralmente t√™m centenas de dimens√µes. Para visualizar, usamos PCA (An√°lise de Componentes Principais) para reduzir para 2D:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando embeddings mais ricos para visualiza√ß√£o\n",
        "np.random.seed(42)\n",
        "\n",
        "# Categorias de palavras com embeddings similares dentro da categoria\n",
        "categorias = {\n",
        "    'animais': ['cachorro', 'gato', 'le√£o', 'tigre', 'elefante'],\n",
        "    'comidas': ['pizza', 'hamburguer', 'sushi', 'lasanha', 'salada'],\n",
        "    'veiculos': ['carro', 'moto', 'avi√£o', 'barco', 'bicicleta'],\n",
        "    'cores': ['vermelho', 'azul', 'verde', 'amarelo', 'roxo']\n",
        "}\n",
        "\n",
        "# Gerando embeddings agrupados por categoria\n",
        "embeddings_categorias = {}\n",
        "cores_plot = ['red', 'blue', 'green', 'orange']\n",
        "cores_mapa = {}\n",
        "\n",
        "for i, (categoria, palavras) in enumerate(categorias.items()):\n",
        "    # Centro da categoria\n",
        "    centro = np.random.normal(i*3, 1, 10)  # 10 dimens√µes\n",
        "    \n",
        "    for palavra in palavras:\n",
        "        # Adiciona ru√≠do ao centro para criar cluster\n",
        "        embedding = centro + np.random.normal(0, 0.5, 10)\n",
        "        embeddings_categorias[palavra] = embedding\n",
        "        cores_mapa[palavra] = cores_plot[i]\n",
        "\n",
        "# Aplicando PCA para reduzir para 2D\n",
        "palavras = list(embeddings_categorias.keys())\n",
        "vectors = list(embeddings_categorias.values())\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(vectors)\n",
        "\n",
        "# Plotando\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "# Plot por categoria\n",
        "for i, (categoria, palavras_cat) in enumerate(categorias.items()):\n",
        "    indices = [palavras.index(p) for p in palavras_cat]\n",
        "    x_coords = [embeddings_2d[idx][0] for idx in indices]\n",
        "    y_coords = [embeddings_2d[idx][1] for idx in indices]\n",
        "    \n",
        "    ax.scatter(x_coords, y_coords, c=cores_plot[i], label=categoria.title(), \n",
        "              s=200, alpha=0.7, edgecolors='black', linewidth=1)\n",
        "    \n",
        "    # Anota√ß√µes\n",
        "    for idx, palavra in zip(indices, palavras_cat):\n",
        "        ax.annotate(palavra, (embeddings_2d[idx][0], embeddings_2d[idx][1]), \n",
        "                   xytext=(5, 5), textcoords='offset points', \n",
        "                   fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} da vari√¢ncia)', fontsize=12)\n",
        "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} da vari√¢ncia)', fontsize=12)\n",
        "ax.set_title('Visualiza√ß√£o de Embeddings em 2D\\n(Redu√ß√£o dimensional com PCA)', fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üéØ PCA explica {sum(pca.explained_variance_ratio_):.2%} da vari√¢ncia total\")\n",
        "print(\"Repare como palavras da mesma categoria ficam agrupadas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-05_img_04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó Conectando com os Pr√≥ximos M√≥dulos\n\nOs embeddings que aprendemos hoje s√£o fundamentais para entender:\n\n### M√≥dulo 6 - Tipos de Modelos\n- Como diferentes arquiteturas (BERT, GPT, T5) usam embeddings\n- Embeddings especializados para tarefas espec√≠ficas\n\n### M√≥dulo 7 - Treinamento\n- Como os embeddings s√£o aprendidos durante o treinamento\n- T√©cnicas de fine-tuning de embeddings\n\n### M√≥dulo 8 - Prompting\n- Como prompts s√£o convertidos em embeddings\n- Engenharia de prompts no espa√ßo vetorial\n\n**Dica do Pedro:** Embeddings s√£o como o \"sistema circulat√≥rio\" dos LLMs - eles transportam significado por toda a rede! ü©∏"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è Exerc√≠cio Pr√°tico 1: Construindo seu Pr√≥prio Sistema de Embeddings\n\nAgora √© sua vez! Vamos implementar um sistema completo de embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 1: Complete o c√≥digo abaixo\n",
        "\n",
        "class MeuSistemaEmbeddings:\n",
        "    def __init__(self, dimensoes=50):\n",
        "        self.dimensoes = dimensoes\n",
        "        self.vocab = {}\n",
        "        self.embeddings = {}\n",
        "        self.treinado = False\n",
        "    \n",
        "    def treinar(self, textos):\n",
        "        \"\"\"\n",
        "        TODO: Implemente o treinamento dos embeddings\n",
        "        \n",
        "        Passos:\n",
        "        1. Construa o vocabul√°rio a partir dos textos\n",
        "        2. Inicialize embeddings aleat√≥rios para cada palavra\n",
        "        3. (Bonus) Implemente uma vers√£o simples de co-ocorr√™ncia\n",
        "        \"\"\"\n",
        "        pass  # Substitua por sua implementa√ß√£o\n",
        "    \n",
        "    def obter_embedding(self, palavra):\n",
        "        \"\"\"\n",
        "        TODO: Retorne o embedding de uma palavra\n",
        "        \"\"\"\n",
        "        pass  # Substitua por sua implementa√ß√£o\n",
        "    \n",
        "    def similaridade(self, palavra1, palavra2):\n",
        "        \"\"\"\n",
        "        TODO: Calcule a similaridade cosseno entre duas palavras\n",
        "        \"\"\"\n",
        "        pass  # Substitua por sua implementa√ß√£o\n",
        "    \n",
        "    def palavras_similares(self, palavra, top_k=3):\n",
        "        \"\"\"\n",
        "        TODO: Encontre as k palavras mais similares\n",
        "        \"\"\"\n",
        "        pass  # Substitua por sua implementa√ß√£o\n",
        "\n",
        "# Dados para teste\n",
        "textos_treino = [\n",
        "    \"cachorro late muito alto no quintal\",\n",
        "    \"gato mia baixinho pela manh√£\", \n",
        "    \"carro vermelho anda na estrada\",\n",
        "    \"bicicleta azul est√° na garagem\",\n",
        "    \"pizza deliciosa com queijo\",\n",
        "    \"hamburguer saboroso com batata\"\n",
        "]\n",
        "\n",
        "# Teste seu sistema aqui\n",
        "# meu_sistema = MeuSistemaEmbeddings()\n",
        "# meu_sistema.treinar(textos_treino)\n",
        "# print(meu_sistema.similaridade('cachorro', 'gato'))\n",
        "\n",
        "print(\"üéØ Complete o c√≥digo acima e teste seu sistema de embeddings!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Exerc√≠cio Pr√°tico 2: An√°lise de Bias em Embeddings\n\nEmbeddings podem carregar preconceitos dos dados de treino. Vamos investigar isso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 2: Detectando bias em embeddings\n",
        "\n",
        "# Embeddings com bias simulado\n",
        "embeddings_com_bias = {\n",
        "    'm√©dico': np.array([0.8, 0.7, 0.9, 0.6]),\n",
        "    'enfermeira': np.array([0.3, 0.8, 0.4, 0.9]),\n",
        "    'engenheiro': np.array([0.9, 0.6, 0.8, 0.5]),\n",
        "    'professor': np.array([0.5, 0.7, 0.6, 0.8]),\n",
        "    'homem': np.array([0.9, 0.5, 0.8, 0.4]),\n",
        "    'mulher': np.array([0.2, 0.9, 0.3, 0.8]),\n",
        "    'forte': np.array([0.8, 0.4, 0.9, 0.3]),\n",
        "    'delicada': np.array([0.2, 0.8, 0.3, 0.9])\n",
        "}\n",
        "\n",
        "def analisar_bias(embeddings_dict, profissoes, generos, adjetivos):\n",
        "    \"\"\"\n",
        "    TODO: Analise associa√ß√µes entre profiss√µes, g√™neros e adjetivos\n",
        "    \n",
        "    Calcule:\n",
        "    1. Similaridade entre profiss√µes e g√™neros\n",
        "    2. Similaridade entre g√™neros e adjetivos\n",
        "    3. Identifique poss√≠veis vieses\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"üîç AN√ÅLISE DE BIAS EM EMBEDDINGS\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # TODO: Implemente sua an√°lise aqui\n",
        "    \n",
        "    # Exemplo de estrutura:\n",
        "    # for profissao in profissoes:\n",
        "    #     for genero in generos:\n",
        "    #         sim = calcular_similaridade(profissao, genero)\n",
        "    #         print(f\"{profissao} <-> {genero}: {sim:.3f}\")\n",
        "    \n",
        "    pass\n",
        "\n",
        "# Teste sua an√°lise\n",
        "profissoes = ['m√©dico', 'enfermeira', 'engenheiro', 'professor']\n",
        "generos = ['homem', 'mulher']\n",
        "adjetivos = ['forte', 'delicada']\n",
        "\n",
        "# analisar_bias(embeddings_com_bias, profissoes, generos, adjetivos)\n",
        "\n",
        "print(\"\\nüí° Dicas para sua implementa√ß√£o:\")\n",
        "print(\"- Use similaridade cosseno\")\n",
        "print(\"- Procure por associa√ß√µes problem√°ticas\")\n",
        "print(\"- Pense em como mitigar esses vieses\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-05_img_05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Comparando Diferentes Tipos de Embeddings\n\nVamos fazer uma compara√ß√£o final entre os principais tipos de embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compara√ß√£o final entre tipos de embeddings\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Criando tabela comparativa\n",
        "comparacao = {\n",
        "    'Tipo': ['One-Hot', 'Word2Vec', 'GloVe', 'FastText', 'BERT/GPT'],\n",
        "    'Ano': [1990, 2013, 2014, 2017, 2018],\n",
        "    'Dimens√µes': ['Vocabul√°rio', '50-300', '50-300', '100-300', '768-1024'],\n",
        "    'Contextual': ['N√£o', 'N√£o', 'N√£o', 'N√£o', 'Sim'],\n",
        "    'Subpalavras': ['N√£o', 'N√£o', 'N√£o', 'Sim', 'Sim'],\n",
        "    'Vantagem Principal': [\n",
        "        'Simplicidade',\n",
        "        'Sem√¢ntica capturada',\n",
        "        'Estat√≠sticas globais',\n",
        "        'Palavras raras/OOV',\n",
        "        'Contexto din√¢mico'\n",
        "    ],\n",
        "    'Desvantagem': [\n",
        "        'Sem sem√¢ntica',\n",
        "        'Sem contexto',\n",
        "        'Computacionalmente caro',\n",
        "        'Treino complexo',\n",
        "        'Muito pesado'\n",
        "    ]\n",
        "}\n",
        "\n",
        "df_comparacao = pd.DataFrame(comparacao)\n",
        "print(\"üìä EVOLU√á√ÉO DOS EMBEDDINGS\")\n",
        "print(\"=\" * 50)\n",
        "print(df_comparacao.to_string(index=False))\n",
        "\n",
        "# Gr√°fico da evolu√ß√£o temporal\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Timeline\n",
        "anos = df_comparacao['Ano'].values\n",
        "tipos = df_comparacao['Tipo'].values\n",
        "cores = ['red', 'orange', 'yellow', 'lightgreen', 'darkgreen']\n",
        "\n",
        "ax1.scatter(anos, range(len(tipos)), c=cores, s=200, alpha=0.7)\n",
        "for i, (ano, tipo) in enumerate(zip(anos, tipos)):\n",
        "    ax1.annotate(tipo, (ano, i), xytext=(10, 0), \n",
        "                textcoords='offset points', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax1.set_xlabel('Ano', fontsize=12)\n",
        "ax1.set_ylabel('Evolu√ß√£o', fontsize=12)\n",
        "ax1.set_title('Timeline dos Embeddings', fontsize=14)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_yticks([])\n",
        "\n",
        "# Complexidade vs Performance (conceitual)\n",
        "complexidade = [1, 3, 4, 5, 8]\n",
        "performance = [1, 6, 7, 8, 10]\n",
        "\n",
        "ax2.scatter(complexidade, performance, c=cores, s=200, alpha=0.7)\n",
        "for i, tipo in enumerate(tipos):\n",
        "    ax2.annotate(tipo, (complexidade[i], performance[i]), \n",
        "                xytext=(5, 5), textcoords='offset points', \n",
        "                fontsize=10, fontweight='bold')\n",
        "\n",
        "ax2.set_xlabel('Complexidade', fontsize=12)\n",
        "ax2.set_ylabel('Performance', fontsize=12)\n",
        "ax2.set_title('Complexidade vs Performance', fontsize=14)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ Li√ß√µes principais:\")\n",
        "print(\"‚Ä¢ Embeddings evolu√≠ram de simples para sofisticados\")\n",
        "print(\"‚Ä¢ Trade-off entre simplicidade e performance\")\n",
        "print(\"‚Ä¢ Contexto fez toda a diferen√ßa!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-05_img_06.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Resumo do M√≥dulo: Embeddings Dominados!\n\nLiiindo! Voc√™ chegou ao final do M√≥dulo 5! Vamos recapitular o que aprendemos:\n\n### üß† Conceitos Principais\n- **Embeddings**: Representa√ß√µes num√©ricas que capturam significado sem√¢ntico\n- **Evolu√ß√£o**: De One-Hot ‚Üí Word2Vec ‚Üí Embeddings Contextuais\n- **Dimensionalidade**: Como reduzir dimens√µes mantendo informa√ß√£o\n- **Similaridade**: Medindo rela√ß√µes entre palavras no espa√ßo vetorial\n\n### üîß T√©cnicas Aprendidas\n- Implementa√ß√£o de embeddings simples\n- Visualiza√ß√£o com PCA\n- Avalia√ß√£o de qualidade (analogias, similaridade)\n- Detec√ß√£o de bias\n\n### üöÄ Pr√≥ximos Passos\nNo **M√≥dulo 6 - Tipos de Modelos**, veremos como diferentes arquiteturas (BERT, GPT, T5) implementam e utilizam embeddings de formas espec√≠ficas.\n\n**Dica Final do Pedro:** Embeddings s√£o a base de tudo em NLP moderno. Dominar esse conceito √© como ter a chave do reino dos LLMs! üóùÔ∏è\n\n### üìö Para Praticar Mais\n- Experimente com datasets maiores\n- Teste embeddings pr√©-treinados (Word2Vec do Google, GloVe)\n- Explore bibliotecas como Gensim e Hugging Face\n- Analise embeddings de diferentes dom√≠nios (m√©dico, jur√≠dico, etc.)\n\n---\n\n**Bora para o pr√≥ximo m√≥dulo! üöÄ**"
      ]
    }
  ]
}