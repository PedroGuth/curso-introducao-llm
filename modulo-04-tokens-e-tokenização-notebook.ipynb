{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ M√≥dulo 4: Desvendando os Tokens - O DNA das LLMs!\n\n**Curso: Introdu√ß√£o √† LLMs**\n\n*By Pedro Nunes Guth*\n\n---\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-04_img_01.png)\n\nE a√≠, pessoal! Bora mergulhar no mundo dos **tokens**? üöÄ\n\nLembra quando falamos sobre a arquitetura Transformer no m√≥dulo anterior? Pois √©, chegou a hora de entender como as LLMs realmente \"enxergam\" o texto que voc√™ manda pra elas!\n\nT√°, mas o que √© um token afinal? ü§î"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† O que s√£o Tokens?\n\n**Token** √© basicamente a \"moeda\" que as LLMs usam pra processar texto. Imagina que voc√™ tem um sandu√≠che gigante e precisa cortar ele em peda√ßos menores pra conseguir comer. Os tokens s√£o esses peda√ßos!\n\n### Por que n√£o processar palavra por palavra?\n\nPensa comigo:\n- E se eu escrever \"anti-inflamat√≥rio\"? S√£o 2 palavras ou 1?\n- E \"Jo√£o\"? Como a IA vai saber que √© um nome?\n- E emojis? üòéüî•\n\nPor isso a tokeniza√ß√£o √© **FUNDAMENTAL** pras LLMs!\n\n### Tipos de Tokens:\n1. **Subword tokens** - Peda√ßos de palavras\n2. **Word tokens** - Palavras completas\n3. **Character tokens** - Caracteres individuais\n4. **Tokens especiais** - [CLS], [SEP], etc.\n\n**Dica do Pedro:** A maioria das LLMs modernas usa subword tokenization. √â tipo cortar o sandu√≠che em peda√ßos que fazem sentido! ü•™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos come√ßar com os imports necess√°rios!\n# Bora instalar e importar as bibliotecas que vamos usar\n\n!pip install transformers tiktoken matplotlib seaborn wordcloud\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport re\nimport json\nfrom wordcloud import WordCloud\n\n# Configura√ß√µes visuais\nplt.style.use('default')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\nprint(\"üìö Bibliotecas carregadas com sucesso!\")\nprint(\"üöÄ Bora tokenizar!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Processo de Tokeniza√ß√£o\n\nA tokeniza√ß√£o acontece em algumas etapas. Vou explicar usando uma analogia bem brasileira:\n\n```mermaid\ngraph TD\n    A[\"Texto: 'Vai, Brasil! üáßüá∑'\"] --> B[Pr√©-processamento]\n    B --> C[Normaliza√ß√£o]\n    C --> D[Aplicar Algoritmo]\n    D --> E[\"Tokens: ['Vai', ',', 'Brasil', '!', 'üáßüá∑']\"]\n    E --> F[IDs Num√©ricos]\n    F --> G[\"[1234, 45, 789, 23, 9876]\"]\n```\n\n### Etapas da Tokeniza√ß√£o:\n\n1. **Pr√©-processamento**: Limpar o texto\n2. **Normaliza√ß√£o**: Converter mai√∫sculas, remover acentos (√†s vezes)\n3. **Aplicar algoritmo**: BPE, WordPiece, SentencePiece\n4. **Converter para IDs**: Cada token vira um n√∫mero\n\n**Importante**: Cada modelo tem seu pr√≥prio vocabul√°rio de tokens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um tokenizador simples pra entender o conceito\n\nclass TokenizadorSimples:\n    def __init__(self):\n        self.vocab = {}\n        self.token_to_id = {}\n        self.id_to_token = {}\n        \n    def treinar(self, textos):\n        \"\"\"Treina o tokenizador com uma lista de textos\"\"\"\n        print(\"üèãÔ∏è Treinando o tokenizador...\")\n        \n        # Coleta todos os caracteres √∫nicos\n        chars = set()\n        for texto in textos:\n            chars.update(list(texto.lower()))\n        \n        # Cria vocabul√°rio\n        tokens_especiais = ['<pad>', '<unk>', '<start>', '<end>']\n        self.vocab = tokens_especiais + sorted(list(chars))\n        \n        # Cria mapeamentos\n        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}\n        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}\n        \n        print(f\"‚úÖ Vocabul√°rio criado com {len(self.vocab)} tokens!\")\n        print(f\"üìã Primeiros 10 tokens: {self.vocab[:10]}\")\n        \n    def tokenizar(self, texto):\n        \"\"\"Tokeniza um texto em caracteres\"\"\"\n        tokens = []\n        for char in texto.lower():\n            if char in self.token_to_id:\n                tokens.append(char)\n            else:\n                tokens.append('<unk>')  # Token desconhecido\n        return tokens\n    \n    def encode(self, texto):\n        \"\"\"Converte texto em IDs\"\"\"\n        tokens = self.tokenizar(texto)\n        return [self.token_to_id[token] for token in tokens]\n    \n    def decode(self, ids):\n        \"\"\"Converte IDs de volta para texto\"\"\"\n        tokens = [self.id_to_token[id] for id in ids]\n        return ''.join(tokens)\n\n# Teste do nosso tokenizador\ntextos_treino = [\n    \"Ol√°, mundo!\",\n    \"Python √© incr√≠vel\",\n    \"Intelig√™ncia Artificial\",\n    \"Tokens s√£o importantes\"\n]\n\ntokenizador = TokenizadorSimples()\ntokenizador.treinar(textos_treino)\n\n# Testando\ntexto_teste = \"Ol√°, IA!\"\nprint(f\"\\nüîç Texto original: '{texto_teste}'\")\nprint(f\"üéØ Tokens: {tokenizador.tokenizar(texto_teste)}\")\nprint(f\"üî¢ IDs: {tokenizador.encode(texto_teste)}\")\nprint(f\"üîÑ Decodificado: '{tokenizador.decode(tokenizador.encode(texto_teste))}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß¨ Algoritmos de Tokeniza√ß√£o\n\nAgora vamos falar dos algoritmos que as LLMs de verdade usam! S√£o mais sofisticados que o nosso exemplo anterior.\n\n### 1. **Byte Pair Encoding (BPE)**\nUsado pelo GPT! Funciona assim:\n- Come√ßa com caracteres individuais\n- Vai juntando os pares mais frequentes\n- Tipo montar palavras com Lego! üß±\n\n### 2. **WordPiece**\nUsado pelo BERT! Similar ao BPE, mas:\n- Usa probabilidade pra decidir as jun√ß√µes\n- Prefere subwords que maximizam a verossimilhan√ßa\n\n### 3. **SentencePiece**\nUsado pelo T5! O mais moderno:\n- Trata o texto como sequ√™ncia de bytes\n- N√£o precisa de pr√©-processamento\n- Funciona com qualquer idioma!\n\n**Dica do Pedro:** Cada algoritmo tem suas vantagens. O SentencePiece √© tipo o \"canivete su√≠√ßo\" da tokeniza√ß√£o! üîß"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos implementar um BPE simplificado pra entender o conceito!\n\nclass BPESimplificado:\n    def __init__(self, num_merges=100):\n        self.num_merges = num_merges\n        self.vocab = set()\n        self.merges = []\n        \n    def get_stats(self, vocab):\n        \"\"\"Conta frequ√™ncia de pares consecutivos\"\"\"\n        pairs = {}\n        for word, freq in vocab.items():\n            symbols = word.split()\n            for i in range(len(symbols) - 1):\n                pair = (symbols[i], symbols[i + 1])\n                pairs[pair] = pairs.get(pair, 0) + freq\n        return pairs\n    \n    def merge_vocab(self, pair, v_in):\n        \"\"\"Faz o merge de um par no vocabul√°rio\"\"\"\n        v_out = {}\n        bigram = ' '.join(pair)\n        replacement = ''.join(pair)\n        \n        for word in v_in:\n            new_word = word.replace(bigram, replacement)\n            v_out[new_word] = v_in[word]\n        return v_out\n    \n    def treinar(self, textos):\n        \"\"\"Treina o BPE\"\"\"\n        print(\"üöÄ Iniciando treinamento BPE...\")\n        \n        # Prepara vocabul√°rio inicial (palavras como caracteres separados)\n        vocab = {}\n        for texto in textos:\n            palavras = texto.lower().split()\n            for palavra in palavras:\n                # Adiciona espa√ßo entre caracteres + </w> no final\n                palavra_tokenizada = ' '.join(list(palavra)) + ' </w>'\n                vocab[palavra_tokenizada] = vocab.get(palavra_tokenizada, 0) + 1\n        \n        print(f\"üìä Vocabul√°rio inicial: {len(vocab)} palavras\")\n        \n        # Executa merges\n        for i in range(self.num_merges):\n            pairs = self.get_stats(vocab)\n            if not pairs:\n                break\n                \n            # Pega o par mais frequente\n            best_pair = max(pairs, key=pairs.get)\n            vocab = self.merge_vocab(best_pair, vocab)\n            self.merges.append(best_pair)\n            \n            if i % 20 == 0:\n                print(f\"üîÑ Merge {i}: {best_pair} (freq: {pairs[best_pair]})\")\n        \n        # Extrai vocabul√°rio final\n        self.vocab = set()\n        for word in vocab:\n            self.vocab.update(word.split())\n            \n        print(f\"‚úÖ Treinamento conclu√≠do!\")\n        print(f\"üìà Vocabul√°rio final: {len(self.vocab)} tokens\")\n        print(f\"üîß Merges realizados: {len(self.merges)}\")\n        \n        return self.vocab, self.merges\n\n# Teste do BPE\ntextos_bpe = [\n    \"intelig√™ncia artificial √© incr√≠vel\",\n    \"processamento de linguagem natural\",\n    \"transformers revolucionaram a √°rea\",\n    \"tokeniza√ß√£o √© fundamental\",\n    \"artificial intelligence rocks\"\n]\n\nbpe = BPESimplificado(num_merges=50)\nvocab_final, merges = bpe.treinar(textos_bpe)\n\nprint(f\"\\nüéØ Alguns tokens aprendidos:\")\nfor i, token in enumerate(sorted(list(vocab_final))[:15]):\n    print(f\"  {i+1}. '{token}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos visualizar como diferentes tokenizadores funcionam na pr√°tica!\n\nfrom transformers import AutoTokenizer\nimport tiktoken\n\n# Carrega diferentes tokenizadores\nprint(\"üì• Carregando tokenizadores famosos...\")\n\n# BERT (WordPiece)\nbert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\n# GPT-2 (BPE)\ngpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n\n# GPT-4 (tiktoken)\ntry:\n    gpt4_tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\nexcept:\n    gpt4_tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Fallback\n\nprint(\"‚úÖ Tokenizadores carregados!\")\n\n# Texto de teste em portugu√™s\ntexto_br = \"Intelig√™ncia artificial est√° revolucionando o Brasil! üáßüá∑\"\n\nprint(f\"\\nüéØ Texto de teste: '{texto_br}'\\n\")\n\n# Compara tokeniza√ß√µes\nprint(\"üîç BERT (WordPiece):\")\nbert_tokens = bert_tokenizer.tokenize(texto_br)\nprint(f\"  Tokens: {bert_tokens}\")\nprint(f\"  Quantidade: {len(bert_tokens)} tokens\")\n\nprint(\"\\nüîç GPT-2 (BPE):\")\ngpt2_tokens = gpt2_tokenizer.tokenize(texto_br)\nprint(f\"  Tokens: {gpt2_tokens}\")\nprint(f\"  Quantidade: {len(gpt2_tokens)} tokens\")\n\nprint(\"\\nüîç GPT-4 (tiktoken):\")\ngpt4_tokens = [gpt4_tokenizer.decode([token]) for token in gpt4_tokenizer.encode(texto_br)]\nprint(f\"  Tokens: {gpt4_tokens}\")\nprint(f\"  Quantidade: {len(gpt4_tokens)} tokens\")\n\n# An√°lise dos resultados\nprint(\"\\nüìä An√°lise:\")\nprint(f\"  BERT quebrou em {len(bert_tokens)} peda√ßos\")\nprint(f\"  GPT-2 quebrou em {len(gpt2_tokens)} peda√ßos\")\nprint(f\"  GPT-4 quebrou em {len(gpt4_tokens)} peda√ßos\")\nprint(\"\\nüí° Cada modelo 'v√™' o texto de forma diferente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visualizando a Tokeniza√ß√£o\n\nVamos criar algumas visualiza√ß√µes massa pra entender melhor como funciona!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-04_img_02.png)\n\n**Dica do Pedro:** Visualizar dados √© sempre uma boa! Ajuda a \"ver\" o que t√° rolando nos algoritmos. üìà"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar visualiza√ß√µes incr√≠veis sobre tokeniza√ß√£o!\n\n# Preparando dados para compara√ß√£o\ntextos_teste = [\n    \"Ol√° mundo\",\n    \"Intelig√™ncia artificial\", \n    \"Processamento de linguagem natural\",\n    \"Transformers s√£o revolucion√°rios\",\n    \"Machine learning √© incr√≠vel\",\n    \"Deep learning mudou tudo\"\n]\n\n# Coleta dados de tokeniza√ß√£o\nresultados = []\nfor texto in textos_teste:\n    bert_count = len(bert_tokenizer.tokenize(texto))\n    gpt2_count = len(gpt2_tokenizer.tokenize(texto))\n    gpt4_count = len(gpt4_tokenizer.encode(texto))\n    \n    resultados.append({\n        'texto': texto,\n        'BERT': bert_count,\n        'GPT-2': gpt2_count,\n        'GPT-4': gpt4_count\n    })\n\n# Gr√°fico de barras comparativo\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Subplot 1: Compara√ß√£o de tokenizadores\ntextos_curtos = [t[:15] + '...' if len(t) > 15 else t for t in textos_teste]\nbert_counts = [r['BERT'] for r in resultados]\ngpt2_counts = [r['GPT-2'] for r in resultados]\ngpt4_counts = [r['GPT-4'] for r in resultados]\n\nx = np.arange(len(textos_curtos))\nwidth = 0.25\n\nax1.bar(x - width, bert_counts, width, label='BERT (WordPiece)', color='#FF6B6B', alpha=0.8)\nax1.bar(x, gpt2_counts, width, label='GPT-2 (BPE)', color='#4ECDC4', alpha=0.8)\nax1.bar(x + width, gpt4_counts, width, label='GPT-4 (tiktoken)', color='#45B7D1', alpha=0.8)\n\nax1.set_xlabel('Textos de Teste')\nax1.set_ylabel('N√∫mero de Tokens')\nax1.set_title('üîç Compara√ß√£o de Tokenizadores')\nax1.set_xticks(x)\nax1.set_xticklabels(textos_curtos, rotation=45, ha='right')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Subplot 2: Efici√™ncia por caractere\ncharacter_counts = [len(texto) for texto in textos_teste]\nbert_efficiency = [bert_counts[i]/character_counts[i] for i in range(len(textos_teste))]\ngpt2_efficiency = [gpt2_counts[i]/character_counts[i] for i in range(len(textos_teste))]\ngpt4_efficiency = [gpt4_counts[i]/character_counts[i] for i in range(len(textos_teste))]\n\nax2.plot(character_counts, bert_efficiency, 'o-', label='BERT', linewidth=2, markersize=8)\nax2.plot(character_counts, gpt2_efficiency, 's-', label='GPT-2', linewidth=2, markersize=8)\nax2.plot(character_counts, gpt4_efficiency, '^-', label='GPT-4', linewidth=2, markersize=8)\n\nax2.set_xlabel('N√∫mero de Caracteres')\nax2.set_ylabel('Tokens por Caractere')\nax2.set_title('üìà Efici√™ncia da Tokeniza√ß√£o')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üéØ Insights das visualiza√ß√µes:\")\nprint(\"  ‚Ä¢ Cada tokenizador tem comportamento diferente\")\nprint(\"  ‚Ä¢ Textos maiores geralmente s√£o mais eficientes\")\nprint(\"  ‚Ä¢ GPT-4 tende a ser mais eficiente (menos tokens)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma nuvem de palavras dos tokens mais comuns!\n\n# Analisa vocabul√°rio do GPT-2\nprint(\"üîç Analisando vocabul√°rio do GPT-2...\")\n\n# Pega alguns tokens do vocabul√°rio\nvocab_gpt2 = gpt2_tokenizer.get_vocab()\ntoken_ids = list(range(0, min(1000, len(vocab_gpt2))))  # Primeiros 1000 tokens\ntokens_sample = [gpt2_tokenizer.decode([id]) for id in token_ids]\n\n# Remove tokens especiais e muito pequenos\ntokens_limpos = []\nfor token in tokens_sample:\n    token_clean = token.strip()\n    if len(token_clean) > 1 and token_clean.isalpha():\n        tokens_limpos.append(token_clean)\n\n# Cria frequ√™ncias artificiais (em um caso real, usar√≠amos frequ√™ncias do corpus)\nnp.random.seed(42)\nfrequencias = {token: np.random.randint(10, 100) for token in tokens_limpos[:100]}\n\n# Gera nuvem de palavras\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n\n# Nuvem de palavras\nwordcloud = WordCloud(width=400, height=400, \n                     background_color='white',\n                     colormap='viridis',\n                     max_words=50).generate_from_frequencies(frequencias)\n\nax1.imshow(wordcloud, interpolation='bilinear')\nax1.axis('off')\nax1.set_title('‚òÅÔ∏è Nuvem de Tokens GPT-2', fontsize=16, pad=20)\n\n# Gr√°fico de barras dos mais frequentes\ntop_tokens = sorted(frequencias.items(), key=lambda x: x[1], reverse=True)[:15]\ntokens_top = [item[0] for item in top_tokens]\nfreqs_top = [item[1] for item in top_tokens]\n\nbars = ax2.barh(tokens_top, freqs_top, color=plt.cm.viridis(np.linspace(0, 1, len(tokens_top))))\nax2.set_xlabel('Frequ√™ncia (simulada)')\nax2.set_title('üìä Top 15 Tokens Mais Frequentes')\nax2.grid(True, alpha=0.3)\n\n# Adiciona valores nas barras\nfor i, (bar, freq) in enumerate(zip(bars, freqs_top)):\n    ax2.text(freq + 1, i, str(freq), va='center', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüìà Estat√≠sticas do vocabul√°rio GPT-2:\")\nprint(f\"  ‚Ä¢ Tamanho total: {len(vocab_gpt2):,} tokens\")\nprint(f\"  ‚Ä¢ Tokens analisados: {len(tokens_limpos)}\")\nprint(f\"  ‚Ä¢ Token mais 'frequente': {top_tokens[0][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Tokens Especiais\n\nT√°, mas al√©m das palavras normais, existem tokens especiais que s√£o **SUPER** importantes!\n\n### Principais Tokens Especiais:\n\n- **`<pad>`** - Padding (preenchimento)\n- **`<unk>`** - Unknown (desconhecido) \n- **`<start>/<s>`** - In√≠cio de sequ√™ncia\n- **`<end>/</s>`** - Fim de sequ√™ncia\n- **`<mask>`** - Mascaramento (BERT)\n- **`<cls>`** - Classifica√ß√£o\n- **`<sep>`** - Separador\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-04_img_03.png)\n\n### Por que s√£o importantes?\n\n1. **Estrutura**: Definem in√≠cio/fim de textos\n2. **Padding**: Fazem sequ√™ncias ficarem do mesmo tamanho\n3. **Mascaramento**: Permitem o treinamento do BERT\n4. **Separa√ß√£o**: Dividem diferentes partes do input\n\n**Dica do Pedro:** Os tokens especiais s√£o tipo a \"pontua√ß√£o\" do mundo das LLMs. Sem eles, seria bagun√ßa total! üé≠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos explorar os tokens especiais dos diferentes modelos!\n\nprint(\"üé≠ Explorando tokens especiais...\\n\")\n\n# BERT tokens especiais\nprint(\"üîç BERT (WordPiece):\")\nbert_especiais = {\n    'PAD': bert_tokenizer.pad_token,\n    'UNK': bert_tokenizer.unk_token, \n    'CLS': bert_tokenizer.cls_token,\n    'SEP': bert_tokenizer.sep_token,\n    'MASK': bert_tokenizer.mask_token\n}\n\nfor nome, token in bert_especiais.items():\n    if token:\n        token_id = bert_tokenizer.convert_tokens_to_ids(token)\n        print(f\"  {nome}: '{token}' (ID: {token_id})\")\n\n# GPT-2 tokens especiais  \nprint(\"\\nüîç GPT-2 (BPE):\")\ngpt2_especiais = {\n    'PAD': gpt2_tokenizer.pad_token,\n    'UNK': gpt2_tokenizer.unk_token,\n    'BOS': gpt2_tokenizer.bos_token,\n    'EOS': gpt2_tokenizer.eos_token\n}\n\nfor nome, token in gpt2_especiais.items():\n    if token:\n        token_id = gpt2_tokenizer.convert_tokens_to_ids(token)\n        print(f\"  {nome}: '{token}' (ID: {token_id})\")\n    else:\n        print(f\"  {nome}: None (n√£o definido)\")\n\n# Exemplo pr√°tico: como o BERT processa uma frase\nprint(\"\\nüõ†Ô∏è Exemplo pr√°tico - BERT processando:\")\nfrase = \"Ol√°! Como vai?\"\nprint(f\"\\nTexto: '{frase}'\")\n\n# Tokeniza√ß√£o completa do BERT\ntokens_bert = bert_tokenizer.tokenize(frase)\nprint(f\"Tokens: {tokens_bert}\")\n\n# Com tokens especiais\ninput_ids = bert_tokenizer.encode(frase)\ntokens_completos = bert_tokenizer.convert_ids_to_tokens(input_ids)\nprint(f\"Com especiais: {tokens_completos}\")\nprint(f\"IDs: {input_ids}\")\n\n# Explica√ß√£o\nprint(\"\\nüí° O que aconteceu:\")\nprint(f\"  ‚Ä¢ [CLS] foi adicionado no in√≠cio (ID: {input_ids[0]})\")\nprint(f\"  ‚Ä¢ [SEP] foi adicionado no final (ID: {input_ids[-1]})\")\nprint(f\"  ‚Ä¢ Total de tokens: {len(input_ids)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos simular como funciona o padding com tokens especiais!\n\nprint(\"üéØ Simulando padding com tokens especiais\\n\")\n\n# V√°rias frases de tamanhos diferentes\nfrases = [\n    \"Oi!\",\n    \"Como voc√™ est√° hoje?\", \n    \"Intelig√™ncia artificial √© incr√≠vel e est√° mudando o mundo\",\n    \"Tokens\"\n]\n\n# Tokeniza todas as frases\ntokens_por_frase = []\nids_por_frase = []\n\nfor i, frase in enumerate(frases):\n    tokens = bert_tokenizer.tokenize(frase)\n    ids = bert_tokenizer.encode(frase, add_special_tokens=True)\n    tokens_por_frase.append(tokens)\n    ids_por_frase.append(ids)\n    print(f\"Frase {i+1}: '{frase}'\")\n    print(f\"  Tokens: {tokens}\")\n    print(f\"  IDs: {ids}\")\n    print(f\"  Comprimento: {len(ids)} tokens\\n\")\n\n# Encontra o tamanho m√°ximo\nmax_length = max(len(ids) for ids in ids_por_frase)\nprint(f\"üìè Tamanho m√°ximo necess√°rio: {max_length} tokens\")\n\n# Aplica padding\nprint(\"\\nüîß Aplicando padding:\")\nids_padded = []\nattention_masks = []\n\nfor i, ids in enumerate(ids_por_frase):\n    # Padding\n    padding_length = max_length - len(ids)\n    ids_pad = ids + [bert_tokenizer.pad_token_id] * padding_length\n    \n    # Attention mask (1 para tokens reais, 0 para padding)\n    attention_mask = [1] * len(ids) + [0] * padding_length\n    \n    ids_padded.append(ids_pad)\n    attention_masks.append(attention_mask)\n    \n    print(f\"\\nFrase {i+1} (padded):\")\n    print(f\"  IDs: {ids_pad}\")\n    print(f\"  Mask: {attention_mask}\")\n    print(f\"  Comprimento: {len(ids_pad)} tokens\")\n\nprint(\"\\n‚úÖ Agora todas as frases t√™m o mesmo tamanho!\")\nprint(\"üí° O modelo usa a attention mask para ignorar os tokens de padding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Visualizando o Processo Completo\n\nVamos criar um diagrama que mostra todo o pipeline de tokeniza√ß√£o!\n\n```mermaid\ngraph TD\n    A[\"üìù Texto Original<br/>Intelig√™ncia Artificial\"] --> B[\"üßπ Pr√©-processamento<br/>Limpar, normalizar\"]\n    B --> C[\"‚ö° Tokeniza√ß√£o<br/>WordPiece/BPE/SentencePiece\"]\n    C --> D[\"üè∑Ô∏è Adicionar Tokens Especiais<br/>[CLS] + tokens + [SEP]\"]\n    D --> E[\"üî¢ Convers√£o para IDs<br/>[101, 1234, 5678, 102]\"]\n    E --> F[\"üìè Padding<br/>Mesmo tamanho\"]\n    F --> G[\"üé≠ Attention Masks<br/>1s e 0s\"]\n    G --> H[\"üöÄ Pronto para o Modelo!\"]\n```\n\n**Dica do Pedro:** Esse pipeline √© o cora√ß√£o do processamento de texto nas LLMs! Cada etapa √© crucial. üíñ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma visualiza√ß√£o do pipeline completo!\n\ndef visualizar_pipeline_tokenizacao(texto, tokenizer, modelo_nome):\n    \"\"\"Visualiza todo o pipeline de tokeniza√ß√£o\"\"\"\n    print(f\"\\nüé¨ Pipeline de Tokeniza√ß√£o - {modelo_nome}\")\n    print(\"=\" * 50)\n    \n    # Etapa 1: Texto original\n    print(f\"\\n1Ô∏è‚É£ Texto Original:\")\n    print(f\"   '{texto}'\")\n    print(f\"   üìä {len(texto)} caracteres\")\n    \n    # Etapa 2: Tokeniza√ß√£o b√°sica\n    print(f\"\\n2Ô∏è‚É£ Tokeniza√ß√£o:\")\n    tokens = tokenizer.tokenize(texto)\n    print(f\"   Tokens: {tokens}\")\n    print(f\"   üìä {len(tokens)} tokens\")\n    \n    # Etapa 3: Com tokens especiais\n    print(f\"\\n3Ô∏è‚É£ Com Tokens Especiais:\")\n    input_ids = tokenizer.encode(texto, add_special_tokens=True)\n    tokens_completos = tokenizer.convert_ids_to_tokens(input_ids)\n    print(f\"   Tokens: {tokens_completos}\")\n    print(f\"   üìä {len(tokens_completos)} tokens\")\n    \n    # Etapa 4: IDs num√©ricos\n    print(f\"\\n4Ô∏è‚É£ IDs Num√©ricos:\")\n    print(f\"   {input_ids}\")\n    \n    # Etapa 5: Com padding (simulado para tamanho 15)\n    print(f\"\\n5Ô∏è‚É£ Com Padding (max_length=15):\")\n    if len(input_ids) < 15:\n        padded_ids = input_ids + [tokenizer.pad_token_id] * (15 - len(input_ids))\n        attention_mask = [1] * len(input_ids) + [0] * (15 - len(input_ids))\n    else:\n        padded_ids = input_ids[:15]\n        attention_mask = [1] * 15\n    \n    print(f\"   IDs: {padded_ids}\")\n    print(f\"   Mask: {attention_mask}\")\n    \n    return {\n        'tokens': tokens,\n        'tokens_completos': tokens_completos,\n        'input_ids': input_ids,\n        'padded_ids': padded_ids,\n        'attention_mask': attention_mask\n    }\n\n# Testa com diferentes modelos\ntexto_exemplo = \"Transformers s√£o revolucion√°rios!\"\n\n# BERT\nresult_bert = visualizar_pipeline_tokenizacao(texto_exemplo, bert_tokenizer, \"BERT\")\n\n# GPT-2 \nresult_gpt2 = visualizar_pipeline_tokenizacao(texto_exemplo, gpt2_tokenizer, \"GPT-2\")\n\nprint(\"\\nüèÜ Resumo Comparativo:\")\nprint(f\"  BERT: {len(result_bert['input_ids'])} tokens\")\nprint(f\"  GPT-2: {len(result_gpt2['input_ids'])} tokens\")\nprint(\"\\nüí° Diferentes modelos = diferentes 'vis√µes' do mesmo texto!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí∞ Impacto dos Tokens nos Custos\n\nT√°, agora vem uma parte **MUITO** importante pra quem vai usar LLMs em produ√ß√£o: **CUSTOS**! üí∏\n\n### Por que tokens importam financeiramente?\n\n- **APIs cobram por token**: GPT-4, Claude, etc.\n- **Modelos t√™m limites**: Context window (janela de contexto)\n- **Processamento custa**: Mais tokens = mais compute\n\n### Exemplo de Custos (aproximados):\n- **GPT-4**: ~$0.03 por 1K tokens (input)\n- **GPT-3.5**: ~$0.002 por 1K tokens (input)  \n- **Claude**: Varia entre $0.008-0.024 por 1K tokens\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-04_img_04.png)\n\n**Dica do Pedro:** Sempre otimize seus prompts! Um prompt mal feito pode custar 3x mais que um otimizado. √â tipo escolher entre Uber e √¥nibus! üöåüí∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma calculadora de custos de tokens!\n\nclass CalculadoraCustoTokens:\n    def __init__(self):\n        # Pre√ßos aproximados por 1K tokens (USD) - Mar√ßo 2024\n        self.precos = {\n            'GPT-4': {'input': 0.03, 'output': 0.06},\n            'GPT-3.5-turbo': {'input': 0.0015, 'output': 0.002}, \n            'Claude-3-Opus': {'input': 0.015, 'output': 0.075},\n            'Claude-3-Sonnet': {'input': 0.003, 'output': 0.015}\n        }\n    \n    def calcular_custo(self, modelo, tokens_input, tokens_output=0):\n        \"\"\"Calcula o custo baseado no n√∫mero de tokens\"\"\"\n        if modelo not in self.precos:\n            return None\n            \n        custo_input = (tokens_input / 1000) * self.precos[modelo]['input']\n        custo_output = (tokens_output / 1000) * self.precos[modelo]['output']\n        \n        return {\n            'custo_input': custo_input,\n            'custo_output': custo_output,\n            'custo_total': custo_input + custo_output\n        }\n    \n    def comparar_modelos(self, tokens_input, tokens_output=0):\n        \"\"\"Compara custos entre diferentes modelos\"\"\"\n        resultados = {}\n        for modelo in self.precos.keys():\n            resultados[modelo] = self.calcular_custo(modelo, tokens_input, tokens_output)\n        return resultados\n\n# Exemplo pr√°tico: analisando custos\ncalculadora = CalculadoraCustoTokens()\n\n# Cen√°rios de uso\ncearios = [\n    {\"nome\": \"Prompt simples\", \"input\": 50, \"output\": 100},\n    {\"nome\": \"An√°lise de documento\", \"input\": 2000, \"output\": 500},\n    {\"nome\": \"Processamento em lote\", \"input\": 10000, \"output\": 3000},\n    {\"nome\": \"Chatbot di√°rio\", \"input\": 50000, \"output\": 25000}\n]\n\nprint(\"üí∞ Calculadora de Custos por Tokens\\n\")\n\nfor cenario in cearios:\n    print(f\"üéØ Cen√°rio: {cenario['nome']}\")\n    print(f\"   Input: {cenario['input']:,} tokens\")\n    print(f\"   Output: {cenario['output']:,} tokens\")\n    print(\"   Custos por modelo:\")\n    \n    comparacao = calculadora.comparar_modelos(cenario['input'], cenario['output'])\n    \n    for modelo, custos in comparacao.items():\n        total = custos['custo_total']\n        print(f\"     {modelo}: ${total:.4f}\")\n    \n    # Mostra o mais barato e mais caro\n    custos_ordenados = sorted(comparacao.items(), key=lambda x: x[1]['custo_total'])\n    mais_barato = custos_ordenados[0]\n    mais_caro = custos_ordenados[-1]\n    \n    diferenca = mais_caro[1]['custo_total'] / mais_barato[1]['custo_total']\n    \n    print(f\"   üí° Mais barato: {mais_barato[0]} (${mais_barato[1]['custo_total']:.4f})\")\n    print(f\"   üí∏ Diferen√ßa: {diferenca:.1f}x mais caro que o mais barato\\n\")\n\nprint(\"üìä Dicas para otimizar custos:\")\nprint(\"  ‚Ä¢ Use prompts concisos\")\nprint(\"  ‚Ä¢ Reutilize contexto quando poss√≠vel\")\nprint(\"  ‚Ä¢ Escolha o modelo adequado para a tarefa\")\nprint(\"  ‚Ä¢ Monitore uso de tokens em produ√ß√£o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um gr√°fico de custos por modelo!\n\n# Preparando dados para visualiza√ß√£o\nmodelos = list(calculadora.precos.keys())\ntokens_cenarios = [100, 500, 1000, 5000, 10000]\n\n# Calcula custos para cada cen√°rio (assumindo output = 50% do input)\ncustos_por_modelo = {modelo: [] for modelo in modelos}\n\nfor tokens in tokens_cenarios:\n    tokens_output = tokens // 2  # 50% do input\n    for modelo in modelos:\n        custo = calculadora.calcular_custo(modelo, tokens, tokens_output)\n        custos_por_modelo[modelo].append(custo['custo_total'])\n\n# Cria o gr√°fico\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Gr√°fico 1: Custos por n√∫mero de tokens\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\nfor i, (modelo, custos) in enumerate(custos_por_modelo.items()):\n    ax1.plot(tokens_cenarios, custos, marker='o', linewidth=2.5, \n            label=modelo, color=colors[i], markersize=8)\n\nax1.set_xlabel('N√∫mero de Tokens (Input)')\nax1.set_ylabel('Custo Total (USD)')\nax1.set_title('üí∞ Custos por Modelo vs N√∫mero de Tokens')\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.set_yscale('log')\n\n# Gr√°fico 2: Compara√ß√£o de pre√ßos por 1K tokens\nprecos_input = [calculadora.precos[modelo]['input'] for modelo in modelos]\nprecos_output = [calculadora.precos[modelo]['output'] for modelo in modelos]\n\nx_pos = np.arange(len(modelos))\nwidth = 0.35\n\nax2.bar(x_pos - width/2, precos_input, width, label='Input', \n       color='lightblue', alpha=0.8)\nax2.bar(x_pos + width/2, precos_output, width, label='Output', \n       color='lightcoral', alpha=0.8)\n\nax2.set_xlabel('Modelos')\nax2.set_ylabel('Pre√ßo por 1K Tokens (USD)')\nax2.set_title('üìä Pre√ßos Input vs Output por Modelo')\nax2.set_xticks(x_pos)\nax2.set_xticklabels([modelo.replace('-', '\\n') for modelo in modelos], rotation=45)\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Adiciona valores nas barras\nfor i, (inp, out) in enumerate(zip(precos_input, precos_output)):\n    ax2.text(i - width/2, inp + 0.001, f'${inp:.3f}', \n            ha='center', va='bottom', fontweight='bold', fontsize=9)\n    ax2.text(i + width/2, out + 0.001, f'${out:.3f}', \n            ha='center', va='bottom', fontweight='bold', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# Insights\nprint(\"üéØ Insights dos gr√°ficos:\")\nprint(\"  ‚Ä¢ Custos crescem linearmente com tokens\")\nprint(\"  ‚Ä¢ Output geralmente custa 2-5x mais que input\")\nprint(\"  ‚Ä¢ GPT-3.5 √© muito mais barato para tarefas simples\")\nprint(\"  ‚Ä¢ Diferen√ßas ficam enormes em grande escala!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Exerc√≠cio Pr√°tico 1: Analisador de Textos\n\n**Desafio:** Crie um analisador que compara diferentes aspectos da tokeniza√ß√£o!\n\n### Sua miss√£o:\n1. Receber um texto qualquer\n2. Analisar com 3 tokenizadores diferentes\n3. Calcular estat√≠sticas comparativas\n4. Gerar um relat√≥rio visual\n\n**Dica do Pedro:** Use tudo que aprendemos at√© agora! Seja criativo! üé®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ EXERC√çCIO PR√ÅTICO 1: Analisador de Textos\n# Complete o c√≥digo abaixo!\n\nclass AnalisadorTokenizacao:\n    def __init__(self):\n        # TODO: Inicializar tokenizadores\n        # self.bert_tokenizer = ???\n        # self.gpt2_tokenizer = ???\n        # (Use os tokenizadores j√° carregados)\n        pass\n    \n    def analisar_texto(self, texto):\n        \"\"\"Analisa um texto com m√∫ltiplos tokenizadores\"\"\"\n        # TODO: Implementar an√°lise completa\n        resultados = {\n            'texto_original': texto,\n            'caracteres': len(texto),\n            'palavras': len(texto.split()),\n            'bert': {\n                # TODO: Preencher com an√°lise BERT\n                'tokens': None,\n                'num_tokens': None,\n                'tokens_especiais': None\n            },\n            'gpt2': {\n                # TODO: Preencher com an√°lise GPT-2\n                'tokens': None,\n                'num_tokens': None,\n                'eficiencia': None  # tokens por palavra\n            }\n        }\n        \n        return resultados\n    \n    def gerar_relatorio(self, resultados):\n        \"\"\"Gera um relat√≥rio visual dos resultados\"\"\"\n        # TODO: Criar visualiza√ß√µes\n        # Dicas: gr√°fico de barras, estat√≠sticas, compara√ß√µes\n        pass\n\n# Seu c√≥digo aqui!\n# analisador = AnalisadorTokenizacao()\n# resultado = analisador.analisar_texto(\"Sua frase de teste aqui!\")\n# analisador.gerar_relatorio(resultado)\n\nprint(\"üéØ Complete o c√≥digo acima!\")\nprint(\"üí° Dicas:\")\nprint(\"  ‚Ä¢ Use os tokenizadores j√° carregados\")\nprint(\"  ‚Ä¢ Calcule estat√≠sticas interessantes\")\nprint(\"  ‚Ä¢ Crie gr√°ficos comparativos\")\nprint(\"  ‚Ä¢ Seja criativo com as an√°lises!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé≤ Exerc√≠cio Pr√°tico 2: Otimizador de Prompts\n\n**Desafio Avan√ßado:** Crie uma ferramenta que otimiza prompts para reduzir custos!\n\n### Objetivos:\n1. Receber um prompt \"verboso\"\n2. Sugerir otimiza√ß√µes\n3. Comparar custos antes/depois\n4. Manter a qualidade do prompt\n\n**T√©cnicas para usar:**\n- Remo√ß√£o de palavras desnecess√°rias\n- Substitui√ß√£o por sin√¥nimos mais curtos\n- Reorganiza√ß√£o da estrutura\n- Uso de abrevia√ß√µes quando apropriado\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ EXERC√çCIO PR√ÅTICO 2: Otimizador de Prompts\n# Implemente as fun√ß√µes abaixo!\n\nclass OtimizadorPrompts:\n    def __init__(self):\n        # Dicion√°rio de substitui√ß√µes para otimiza√ß√£o\n        self.substituicoes = {\n            'por favor': 'pls',\n            'voc√™ pode': 'pode',\n            'muito obrigado': 'obrigado',\n            'intelig√™ncia artificial': 'IA',\n            'aprendizado de m√°quina': 'ML',\n            'processamento de linguagem natural': 'NLP',\n            # TODO: Adicione mais substitui√ß√µes!\n        }\n        \n        # Palavras que podem ser removidas\n        self.palavras_removiveis = [\n            'realmente', 'absolutamente', 'definitivamente',\n            'por favor', 'tipo', 'meio que'\n            # TODO: Adicione mais palavras!\n        ]\n    \n    def otimizar_prompt(self, prompt_original):\n        \"\"\"Otimiza um prompt para reduzir tokens\"\"\"\n        # TODO: Implementar otimiza√ß√£o\n        # 1. Aplicar substitui√ß√µes\n        # 2. Remover palavras desnecess√°rias\n        # 3. Simplificar estruturas\n        \n        prompt_otimizado = prompt_original  # Placeholder\n        \n        return prompt_otimizado\n    \n    def comparar_custos(self, prompt_original, prompt_otimizado, modelo='GPT-4'):\n        \"\"\"Compara custos entre prompts\"\"\"\n        # TODO: Usar a calculadora de custos\n        # Calcular tokens e custos para ambos os prompts\n        pass\n    \n    def gerar_relatorio_otimizacao(self, prompt_original, prompt_otimizado):\n        \"\"\"Gera relat√≥rio da otimiza√ß√£o\"\"\"\n        # TODO: Criar visualiza√ß√£o comparativa\n        pass\n\n# Exemplo de uso:\nprompt_teste = \"\"\"\nOl√°! Por favor, voc√™ pode me ajudar a criar um texto sobre intelig√™ncia artificial? \nEu realmente preciso de algo muito detalhado e definitivamente completo sobre \naprendizado de m√°quina e processamento de linguagem natural. Muito obrigado!\n\"\"\"\n\n# Seu c√≥digo aqui!\n# otimizador = OtimizadorPrompts()\n# prompt_otimizado = otimizador.otimizar_prompt(prompt_teste)\n# otimizador.gerar_relatorio_otimizacao(prompt_teste, prompt_otimizado)\n\nprint(\"üéØ Implemente o otimizador de prompts!\")\nprint(\"\\nüí° Estrat√©gias de otimiza√ß√£o:\")\nprint(\"  ‚Ä¢ Substituir termos longos por abrevia√ß√µes\")\nprint(\"  ‚Ä¢ Remover palavras redundantes\")\nprint(\"  ‚Ä¢ Usar estruturas mais concisas\")\nprint(\"  ‚Ä¢ Manter o significado original\")\nprint(\"\\nüèÜ Meta: Reduzir 30-50% dos tokens!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó Conex√£o com Pr√≥ximos M√≥dulos\n\nAgora que voc√™ domina tokens, vamos ver como eles se conectam com o resto do curso!\n\n### üéØ M√≥dulo 5 - Embeddings e Representa√ß√µes:\n- **Tokens ‚Üí Embeddings**: Como tokens viram vetores\n- **Vocabulary embeddings**: Cada token tem sua representa√ß√£o\n- **Dimensionalidade**: Geralmente 512, 768 ou 1024 dimens√µes\n\n### üéØ M√≥dulo 6 - Tipos de Modelos:\n- **Encoder-only** (BERT): Usa tokens especiais como [CLS], [SEP]\n- **Decoder-only** (GPT): Gera tokens sequencialmente\n- **Encoder-decoder** (T5): Combina ambos os approaches\n\n### üéØ M√≥dulo 8 - Prompting:\n- **Prompt engineering**: Otimizar tokens para melhores respostas\n- **Context window**: Limite de tokens por intera√ß√£o\n- **Token budgeting**: Gerenciar tokens em aplica√ß√µes\n\n**Dica do Pedro:** Tokens s√£o a funda√ß√£o de tudo! Entender bem esse m√≥dulo vai te ajudar muito nos pr√≥ximos! üèóÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma pr√©via do que vem por a√≠!\n\nprint(\"üîÆ Pr√©via dos Pr√≥ximos M√≥dulos\\n\")\n\n# Simulando como tokens viram embeddings (M√≥dulo 5)\nprint(\"üéØ M√≥dulo 5 - De Tokens para Embeddings:\")\ntexto_exemplo = \"IA √© incr√≠vel\"\ntokens = bert_tokenizer.tokenize(texto_exemplo)\nprint(f\"  Tokens: {tokens}\")\nprint(f\"  Cada token vai virar um vetor de 768 dimens√µes!\")\nprint(f\"  Exemplo: 'IA' ‚Üí [0.1, -0.3, 0.7, ..., 0.2] (768 n√∫meros)\")\n\n# Context window (importante para M√≥dulo 8)\nprint(\"\\nüéØ M√≥dulo 8 - Context Window:\")\nmodelos_limits = {\n    'GPT-3.5': '4K tokens',\n    'GPT-4': '8K-32K tokens', \n    'Claude-3': '200K tokens',\n    'Gemini-Pro': '1M tokens'\n}\n\nfor modelo, limite in modelos_limits.items():\n    print(f\"  {modelo}: {limite}\")\n\nprint(\"\\nüí° Quanto mais tokens, mais 'mem√≥ria' o modelo tem!\")\n\n# Tipos de modelos (M√≥dulo 6)\nprint(\"\\nüéØ M√≥dulo 6 - Como diferentes arquiteturas usam tokens:\")\nprint(\"  ‚Ä¢ BERT (Encoder): V√™ todos os tokens simultaneamente\")\nprint(\"  ‚Ä¢ GPT (Decoder): Gera um token por vez\")\nprint(\"  ‚Ä¢ T5 (Encoder-Decoder): Combina ambas estrat√©gias\")\n\n# Exemplo de gera√ß√£o sequencial\nprint(\"\\nü§ñ Como o GPT geraria: 'A IA vai'\")\nsteps = [\n    \"Input: 'A IA vai' ‚Üí Pr√≥ximo token: 'revolucionar' (prob: 0.7)\",\n    \"Input: 'A IA vai revolucionar' ‚Üí Pr√≥ximo token: 'o' (prob: 0.8)\", \n    \"Input: 'A IA vai revolucionar o' ‚Üí Pr√≥ximo token: 'mundo' (prob: 0.6)\"\n]\n\nfor i, step in enumerate(steps, 1):\n    print(f\"  {i}. {step}\")\n    \nprint(\"\\nüöÄ Muito mais vem por a√≠! Tokens s√£o s√≥ o come√ßo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Resumo do M√≥dulo\n\n**Liiindo! Voc√™ agora √© um expert em tokens!** üèÜ\n\n### üß† O que aprendemos:\n\n1. **Conceitos Fundamentais**:\n   - O que s√£o tokens e por que existem\n   - Diferen√ßa entre caracteres, palavras e subwords\n   - Import√¢ncia da tokeniza√ß√£o para LLMs\n\n2. **Algoritmos de Tokeniza√ß√£o**:\n   - **BPE**: Usado pelo GPT\n   - **WordPiece**: Usado pelo BERT\n   - **SentencePiece**: Mais moderno e flex√≠vel\n\n3. **Tokens Especiais**:\n   - `<pad>`, `<unk>`, `<start>`, `<end>`\n   - Como estruturam o processamento\n   - Padding e attention masks\n\n4. **Aspectos Pr√°ticos**:\n   - **Custos**: Tokens = dinheiro em APIs\n   - **Otimiza√ß√£o**: Como reduzir tokens\n   - **Context window**: Limites dos modelos\n\n5. **Pipeline Completo**:\n   - Texto ‚Üí Tokens ‚Üí IDs ‚Üí Padding ‚Üí Modelo\n   - Cada etapa √© crucial!\n\n### üîë Pontos-chave para lembrar:\n\n- **Diferentes modelos = diferentes tokeniza√ß√µes**\n- **Mais tokens = mais custos**\n- **Tokens especiais s√£o fundamentais**\n- **Otimiza√ß√£o de prompts √© essencial**\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/introdu√ß√£o-√†-llms-modulo-04_img_06.png)\n\n### üöÄ Pr√≥ximos passos:\n\nNo **M√≥dulo 5**, vamos ver como esses tokens viram **embeddings** - representa√ß√µes num√©ricas que os modelos realmente entendem!\n\n**Dica final do Pedro:** Tokens s√£o a \"linguagem\" que voc√™ usa pra conversar com as LLMs. Dominar isso √© tipo aprender o \"dialeto\" da IA! ü§ñüí¨\n\n---\n\n**Parab√©ns por completar o M√≥dulo 4!** üéä\n\n*Continue praticando com os exerc√≠cios e nos vemos no pr√≥ximo m√≥dulo!*"
      ]
    }
  ]
}