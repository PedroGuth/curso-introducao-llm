{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎭 O Grande Teatro dos LLMs: Conhecendo os Diferentes Tipos de Modelos\n\n**Módulo 6 - Introdução à LLMs**\n\n*Por Pedro Nunes Guth*\n\n---\n\nBora! Chegou a hora de conhecer os diferentes \"personagens\" do mundo dos LLMs! 🎪\n\nTá, mas Pedro, por que \"teatro\"? Simples! Cada tipo de modelo tem sua especialidade, seu papel específico no palco da IA. É como uma companhia teatral: tem o protagonista, o coadjuvante, o especialista em comédia, o dramático... cada um brilha de um jeito!\n\nNeste módulo vamos explorar:\n- 🏗️ **Modelos Base vs. Modelos Especializados**\n- 🎯 **Decoder-Only, Encoder-Only e Encoder-Decoder**\n- 🚀 **Modelos Generativos vs. Discriminativos**\n- 🔧 **Fine-tuning e Especialização**\n- 📊 **Comparação de Performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Preparando nosso laboratório!\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import display, Markdown, HTML\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurações visuais\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"🎭 Ambiente configurado! Bora conhecer os tipos de modelos!\")\n",
        "print(\"📚 Lembra dos módulos anteriores:\")\n",
        "print(\"   • Transformers (Módulo 3)\")\n",
        "print(\"   • Tokens (Módulo 4)\")\n",
        "print(\"   • Embeddings (Módulo 5)\")\n",
        "print(\"Agora vamos ver como tudo isso se junta em diferentes tipos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🏗️ Modelos Base vs. Modelos Especializados\n\nTá, vamos começar com o básico! Imagina que você tem uma padaria:\n\n- **Modelo Base**: É como um padeiro que sabe fazer pão básico muito bem, mas ainda não se especializou em nada específico\n- **Modelo Especializado**: É o mesmo padeiro depois de fazer curso de confeitaria, agora ele faz bolos incríveis!\n\n### Modelos Base (Foundation Models)\n\nSão os modelos \"crus\", treinados em grandes quantidades de texto para aprender padrões gerais da linguagem. Eles sabem:\n- Completar frases\n- Entender contexto básico\n- Gerar texto coerente\n\n**Exemplos**: GPT-3 base, LLaMA base, PaLM base\n\n### Modelos Especializados (Fine-tuned Models)\n\nSão modelos base que passaram por treinamento adicional para tarefas específicas:\n- **Instrução**: ChatGPT, Claude (seguem comandos)\n- **Conversação**: Bard, Assistant models\n- **Código**: CodeT5, GitHub Copilot\n- **Domínio específico**: BioBERT (medicina), FinBERT (finanças)\n\n**🎯 Dica do Pedro**: Pense assim - modelo base é como saber português, modelo especializado é como ser jornalista, advogado ou poeta!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma visualização dos tipos de modelos!\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle, FancyBboxPatch\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "# Modelo Base (Foundation)\n",
        "base_box = FancyBboxPatch((1, 7), 4, 2, \n",
        "                         boxstyle=\"round,pad=0.1\", \n",
        "                         facecolor='lightblue', \n",
        "                         edgecolor='navy', linewidth=2)\n",
        "ax.add_patch(base_box)\n",
        "ax.text(3, 8, 'MODELO BASE\\n(Foundation)', ha='center', va='center', \n",
        "        fontsize=12, fontweight='bold')\n",
        "\n",
        "# Modelos Especializados\n",
        "specializations = [\n",
        "    ('Chat/Instrução\\n(ChatGPT)', 0.5, 4, 'lightgreen'),\n",
        "    ('Código\\n(Copilot)', 2.5, 4, 'lightyellow'),\n",
        "    ('Domínio\\n(BioBERT)', 4.5, 4, 'lightcoral'),\n",
        "    ('Classificação\\n(Sentiment)', 6.5, 4, 'lightpink')\n",
        "]\n",
        "\n",
        "for name, x, y, color in specializations:\n",
        "    spec_box = FancyBboxPatch((x, y), 1.8, 1.5, \n",
        "                             boxstyle=\"round,pad=0.1\", \n",
        "                             facecolor=color, \n",
        "                             edgecolor='darkred', linewidth=1.5)\n",
        "    ax.add_patch(spec_box)\n",
        "    ax.text(x + 0.9, y + 0.75, name, ha='center', va='center', \n",
        "            fontsize=10, fontweight='bold')\n",
        "    \n",
        "    # Setas do modelo base para especializados\n",
        "    ax.arrow(3, 7, x + 0.9 - 3, y + 1.5 - 7, \n",
        "             head_width=0.1, head_length=0.1, \n",
        "             fc='gray', ec='gray', alpha=0.7)\n",
        "\n",
        "# Título e labels\n",
        "ax.text(4, 9.5, '🎭 Evolução dos Modelos: Do Geral ao Específico', \n",
        "        ha='center', va='center', fontsize=16, fontweight='bold')\n",
        "\n",
        "ax.text(3, 6, 'Fine-tuning / Especialização', ha='center', va='center', \n",
        "        fontsize=12, style='italic', color='red')\n",
        "\n",
        "ax.set_xlim(0, 8)\n",
        "ax.set_ylim(3, 10)\n",
        "ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"🎯 Liiindo! Viu como funciona a evolução?\")\n",
        "print(\"Base → Especialização = Padeiro → Confeiteiro!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🏛️ Arquiteturas: Decoder-Only, Encoder-Only e Encoder-Decoder\n\nLembra do Módulo 3 sobre Transformers? Agora vamos ver como as diferentes partes são usadas!\n\nPensa no Transformer como uma fábrica com duas seções:\n- **Encoder**: A seção que \"entende\" (lê e processa)\n- **Decoder**: A seção que \"produz\" (gera e cria)\n\n### 🔍 Encoder-Only (Só Entendimento)\n\n**O que faz**: Especialista em entender e analisar texto\n**Como funciona**: Lê todo o texto de uma vez, cria representações ricas\n**Melhor para**: Classificação, análise de sentimento, Q&A\n\n**Exemplo clássico**: BERT\n- Lê \"O filme foi _____ bom\" \n- Entende o contexto todo\n- Preenche a lacuna ou classifica sentimento\n\n### 🎯 Decoder-Only (Só Geração)\n\n**O que faz**: Especialista em gerar texto novo\n**Como funciona**: Lê palavra por palavra, gera a próxima\n**Melhor para**: Geração de texto, conversação, criação\n\n**Exemplo clássico**: GPT família toda\n- Lê \"Era uma vez\"\n- Gera \"uma princesa que vivia em um castelo...\"\n\n### 🔄 Encoder-Decoder (Transformação)\n\n**O que faz**: Especialista em transformar um tipo de texto em outro\n**Como funciona**: Encoder entende, Decoder produz algo diferente\n**Melhor para**: Tradução, sumarização, conversão\n\n**Exemplo clássico**: T5, mT5\n- Encoder lê: \"Hello world\" (inglês)\n- Decoder gera: \"Olá mundo\" (português)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos simular como cada arquitetura processa texto!\n",
        "\n",
        "def simular_encoder_only(texto):\n",
        "    \"\"\"Simula um modelo Encoder-Only (como BERT)\"\"\"\n",
        "    tokens = texto.split()\n",
        "    print(f\"🔍 ENCODER-ONLY processando: '{texto}'\")\n",
        "    print(\"📊 Análise:\")\n",
        "    \n",
        "    # Simula análise de sentimento\n",
        "    palavras_positivas = ['bom', 'ótimo', 'excelente', 'incrível', 'maravilhoso']\n",
        "    palavras_negativas = ['ruim', 'péssimo', 'terrível', 'horrível', 'detesto']\n",
        "    \n",
        "    score = 0\n",
        "    for palavra in tokens:\n",
        "        if palavra.lower() in palavras_positivas:\n",
        "            score += 1\n",
        "        elif palavra.lower() in palavras_negativas:\n",
        "            score -= 1\n",
        "    \n",
        "    if score > 0:\n",
        "        sentimento = \"😊 POSITIVO\"\n",
        "    elif score < 0:\n",
        "        sentimento = \"😞 NEGATIVO\"\n",
        "    else:\n",
        "        sentimento = \"😐 NEUTRO\"\n",
        "    \n",
        "    print(f\"   Sentimento: {sentimento}\")\n",
        "    print(f\"   Confiança: {abs(score * 20)}%\")\n",
        "    return sentimento\n",
        "\n",
        "def simular_decoder_only(prompt):\n",
        "    \"\"\"Simula um modelo Decoder-Only (como GPT)\"\"\"\n",
        "    print(f\"🎯 DECODER-ONLY gerando a partir de: '{prompt}'\")\n",
        "    \n",
        "    # Simula geração baseada no prompt\n",
        "    continuacoes = {\n",
        "        \"era uma vez\": \"uma princesa que vivia em um reino distante...\",\n",
        "        \"python é\": \"uma linguagem de programação poderosa e versátil...\",\n",
        "        \"inteligência artificial\": \"é uma área da computação que busca criar sistemas inteligentes...\",\n",
        "        \"o futuro da tecnologia\": \"será moldado pela integração entre IA, IoT e computação quântica...\"\n",
        "    }\n",
        "    \n",
        "    prompt_lower = prompt.lower()\n",
        "    continuacao = \"será algo incrível e transformador para a humanidade!\"\n",
        "    \n",
        "    for key, value in continuacoes.items():\n",
        "        if key in prompt_lower:\n",
        "            continuacao = value\n",
        "            break\n",
        "    \n",
        "    print(f\"✨ Geração: {continuacao}\")\n",
        "    return continuacao\n",
        "\n",
        "def simular_encoder_decoder(texto_origem, tarefa):\n",
        "    \"\"\"Simula um modelo Encoder-Decoder (como T5)\"\"\"\n",
        "    print(f\"🔄 ENCODER-DECODER - Tarefa: {tarefa}\")\n",
        "    print(f\"📥 Entrada: '{texto_origem}'\")\n",
        "    \n",
        "    if tarefa == \"tradução\":\n",
        "        traducoes = {\n",
        "            \"hello world\": \"olá mundo\",\n",
        "            \"good morning\": \"bom dia\",\n",
        "            \"artificial intelligence\": \"inteligência artificial\",\n",
        "            \"machine learning\": \"aprendizado de máquina\"\n",
        "        }\n",
        "        resultado = traducoes.get(texto_origem.lower(), \"tradução não encontrada\")\n",
        "    \n",
        "    elif tarefa == \"sumarização\":\n",
        "        if len(texto_origem.split()) > 5:\n",
        "            resultado = \" \".join(texto_origem.split()[:5]) + \"...\"\n",
        "        else:\n",
        "            resultado = \"Texto já está resumido\"\n",
        "    \n",
        "    else:\n",
        "        resultado = \"Tarefa não reconhecida\"\n",
        "    \n",
        "    print(f\"📤 Resultado: '{resultado}'\")\n",
        "    return resultado\n",
        "\n",
        "# Testando as três arquiteturas!\n",
        "print(\"🧪 LABORATÓRIO DAS ARQUITETURAS\\n\")\n",
        "\n",
        "# Teste 1: Encoder-Only\n",
        "simular_encoder_only(\"Este filme foi realmente incrível\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Teste 2: Decoder-Only\n",
        "simular_decoder_only(\"Python é\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Teste 3: Encoder-Decoder\n",
        "simular_encoder_decoder(\"hello world\", \"tradução\")\n",
        "print(\"\\n🎭 Cada arquitetura tem sua especialidade!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\n",
        "graph TD\n",
        "    A[Texto de Entrada] --> B{Que tipo de tarefa?}\n",
        "    \n",
        "    B -->|Entender/Classificar| C[Encoder-Only]\n",
        "    B -->|Gerar/Criar| D[Decoder-Only] \n",
        "    B -->|Transformar/Traduzir| E[Encoder-Decoder]\n",
        "    \n",
        "    C --> F[\"🔍 Análise<br/>Sentimento<br/>Classificação<br/>Q&A\"]\n",
        "    D --> G[\"🎯 Geração<br/>Chat<br/>Continuação<br/>Criação\"]\n",
        "    E --> H[\"🔄 Transformação<br/>Tradução<br/>Sumarização<br/>Conversão\"]\n",
        "    \n",
        "    F --> I[BERT, RoBERTa]\n",
        "    G --> J[GPT, LLaMA]\n",
        "    H --> K[T5, BART]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma comparação visual das arquiteturas!\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Cores para cada arquitetura\n",
        "cores = ['lightblue', 'lightgreen', 'lightcoral']\n",
        "arquiteturas = ['Encoder-Only', 'Decoder-Only', 'Encoder-Decoder']\n",
        "exemplos = ['BERT\\nRoBERTa\\nDeBERTa', 'GPT-3/4\\nLLaMA\\nPaLM', 'T5\\nBART\\nmT5']\n",
        "tarefas = ['• Classificação\\n• Análise Sentimento\\n• Q&A\\n• NER', \n",
        "          '• Geração de Texto\\n• Chat\\n• Completar\\n• Criação',\n",
        "          '• Tradução\\n• Sumarização\\n• Paráfrase\\n• Conversão']\n",
        "\n",
        "for i, (arq, ex, tar, cor) in enumerate(zip(arquiteturas, exemplos, tarefas, cores)):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Fundo colorido\n",
        "    ax.add_patch(Rectangle((0, 0), 1, 1, facecolor=cor, alpha=0.3))\n",
        "    \n",
        "    # Título da arquitetura\n",
        "    ax.text(0.5, 0.9, arq, ha='center', va='center', \n",
        "            fontsize=14, fontweight='bold', transform=ax.transAxes)\n",
        "    \n",
        "    # Exemplos de modelos\n",
        "    ax.text(0.5, 0.7, ex, ha='center', va='center', \n",
        "            fontsize=12, fontweight='bold', color='navy',\n",
        "            transform=ax.transAxes)\n",
        "    \n",
        "    # Tarefas principais\n",
        "    ax.text(0.5, 0.4, tar, ha='center', va='center', \n",
        "            fontsize=10, transform=ax.transAxes)\n",
        "    \n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Borda\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(True)\n",
        "        spine.set_linewidth(2)\n",
        "        spine.set_color('navy')\n",
        "\n",
        "plt.suptitle('🏛️ As Três Grandes Arquiteturas dos LLMs', \n",
        "             fontsize=16, fontweight='bold', y=0.95)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"🎯 Dica do Pedro: Cada arquitetura é como uma ferramenta específica!\")\n",
        "print(\"🔍 Encoder-Only = Lupa (para analisar)\")\n",
        "print(\"🎯 Decoder-Only = Caneta (para criar)\")\n",
        "print(\"🔄 Encoder-Decoder = Tradutor (para transformar)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎨 Modelos Generativos vs. Discriminativos\n\nTá, agora vamos falar de uma diferença fundamental! É como a diferença entre um artista e um crítico de arte:\n\n### 🎨 Modelos Generativos\n\n**O que fazem**: Criam conteúdo novo do zero\n**Como pensam**: \"Vou criar algo baseado no que aprendi\"\n**Analogia**: Escritor criando um livro\n\n**Características**:\n- Aprendem a distribuição dos dados\n- Conseguem gerar amostras novas\n- Podem ser criativos e surpreendentes\n- Mais complexos computacionalmente\n\n**Exemplos**: GPT-4, DALL-E, Stable Diffusion\n\n### 🔍 Modelos Discriminativos\n\n**O que fazem**: Classificam e categorizam conteúdo existente\n**Como pensam**: \"Vou analisar e decidir em qual categoria isso se encaixa\"\n**Analogia**: Crítico avaliando uma obra\n\n**Características**:\n- Aprendem fronteiras entre classes\n- Focam em distinção e classificação\n- Mais eficientes para tarefas específicas\n- Geralmente mais rápidos\n\n**Exemplos**: BERT para classificação, modelos de sentiment analysis\n\n**🎯 Dica do Pedro**: Generativo = \"Vou criar!\", Discriminativo = \"Vou julgar!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos simular a diferença entre modelos generativos e discriminativos\n",
        "import random\n",
        "\n",
        "class ModeloGenerativo:\n",
        "    \"\"\"Simula um modelo generativo simples\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # \"Aprendeu\" padrões de diferentes tipos de texto\n",
        "        self.padroes = {\n",
        "            'poesia': ['verso', 'rima', 'alma', 'coração', 'lua', 'estrela'],\n",
        "            'tecnologia': ['algoritmo', 'dados', 'rede', 'sistema', 'código', 'digital'],\n",
        "            'culinaria': ['receita', 'ingrediente', 'sabor', 'tempero', 'cozinha', 'prato']\n",
        "        }\n",
        "    \n",
        "    def gerar_texto(self, categoria, tamanho=5):\n",
        "        \"\"\"Gera texto novo baseado nos padrões aprendidos\"\"\"\n",
        "        palavras = self.padroes.get(categoria, ['palavra', 'genérica'])\n",
        "        texto_gerado = []\n",
        "        \n",
        "        for _ in range(tamanho):\n",
        "            palavra = random.choice(palavras)\n",
        "            texto_gerado.append(palavra)\n",
        "        \n",
        "        return ' '.join(texto_gerado)\n",
        "\n",
        "class ModeloDiscriminativo:\n",
        "    \"\"\"Simula um modelo discriminativo simples\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # \"Aprendeu\" a distinguir entre categorias\n",
        "        self.palavras_chave = {\n",
        "            'poesia': ['verso', 'rima', 'alma', 'coração', 'lua', 'estrela'],\n",
        "            'tecnologia': ['algoritmo', 'dados', 'rede', 'sistema', 'código', 'digital'],\n",
        "            'culinaria': ['receita', 'ingrediente', 'sabor', 'tempero', 'cozinha', 'prato']\n",
        "        }\n",
        "    \n",
        "    def classificar_texto(self, texto):\n",
        "        \"\"\"Classifica texto em uma das categorias\"\"\"\n",
        "        palavras = texto.lower().split()\n",
        "        pontuacoes = {}\n",
        "        \n",
        "        for categoria, chaves in self.palavras_chave.items():\n",
        "            pontuacao = sum(1 for palavra in palavras if palavra in chaves)\n",
        "            pontuacoes[categoria] = pontuacao\n",
        "        \n",
        "        categoria_predita = max(pontuacoes, key=pontuacoes.get)\n",
        "        confianca = pontuacoes[categoria_predita] / len(palavras) * 100\n",
        "        \n",
        "        return categoria_predita, confianca\n",
        "\n",
        "# Testando os dois tipos!\n",
        "print(\"🧪 LABORATÓRIO: GENERATIVO vs DISCRIMINATIVO\\n\")\n",
        "\n",
        "# Criando os modelos\n",
        "gerador = ModeloGenerativo()\n",
        "classificador = ModeloDiscriminativo()\n",
        "\n",
        "print(\"🎨 MODELO GENERATIVO em ação:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "for categoria in ['poesia', 'tecnologia', 'culinaria']:\n",
        "    texto_gerado = gerador.gerar_texto(categoria)\n",
        "    print(f\"📝 Gerando {categoria}: '{texto_gerado}'\")\n",
        "\n",
        "print(\"\\n🔍 MODELO DISCRIMINATIVO em ação:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "textos_teste = [\n",
        "    \"o algoritmo processa dados na rede\",\n",
        "    \"a lua brilha no verso da alma\",\n",
        "    \"a receita leva tempero e sabor\"\n",
        "]\n",
        "\n",
        "for texto in textos_teste:\n",
        "    categoria, confianca = classificador.classificar_texto(texto)\n",
        "    print(f\"📊 Texto: '{texto}'\")\n",
        "    print(f\"    Categoria: {categoria} (confiança: {confianca:.1f}%)\\n\")\n",
        "\n",
        "print(\"✨ Resumo:\")\n",
        "print(\"🎨 Generativo: CRIA conteúdo novo\")\n",
        "print(\"🔍 Discriminativo: ANALISA conteúdo existente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Modelos por Tamanho e Capacidade\n\nAgora vamos falar de tamanho! E não, não é só \"quanto maior, melhor\". É mais como escolher o carro certo para cada situação:\n\n### 🚗 Modelos Pequenos (< 1B parâmetros)\n**Analogia**: Carro compacto - econômico e eficiente para o dia a dia\n- **Vantagens**: Rápidos, baratos, rodam localmente\n- **Desvantagens**: Menos \"inteligentes\", conhecimento limitado\n- **Exemplos**: DistilBERT, TinyBERT, modelos mobile\n\n### 🚙 Modelos Médios (1B - 10B parâmetros)\n**Analogia**: SUV - bom equilíbrio entre performance e praticidade\n- **Vantagens**: Boa performance, custo razoável\n- **Desvantagens**: Ainda limitados para tarefas complexas\n- **Exemplos**: BERT-base, GPT-2, alguns modelos LLaMA\n\n### 🚛 Modelos Grandes (10B - 100B parâmetros)\n**Analogia**: Caminhão - potente para trabalhos pesados\n- **Vantagens**: Muito capaz, conhecimento amplo\n- **Desvantagens**: Caro, lento, precisa de infraestrutura\n- **Exemplos**: GPT-3, PaLM, alguns LLaMA\n\n### 🚀 Modelos Gigantes (> 100B parâmetros)\n**Analogia**: Foguete - para missões impossíveis\n- **Vantagens**: Estado da arte, capacidades emergentes\n- **Desvantagens**: Extremamente caro e complexo\n- **Exemplos**: GPT-4, PaLM-2, Gemini Ultra\n\n**🎯 Dica do Pedro**: O segredo é escolher o tamanho certo para sua tarefa! Não precisa de foguete para ir ao mercado!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma visualização dos tamanhos e custos!\n",
        "import numpy as np\n",
        "\n",
        "# Dados dos modelos (aproximados)\n",
        "modelos = {\n",
        "    'DistilBERT': {'params': 0.066, 'performance': 75, 'custo': 1, 'velocidade': 95},\n",
        "    'BERT-base': {'params': 0.11, 'performance': 80, 'custo': 2, 'velocidade': 85},\n",
        "    'GPT-2': {'params': 1.5, 'performance': 82, 'custo': 5, 'velocidade': 75},\n",
        "    'LLaMA-7B': {'params': 7, 'performance': 85, 'custo': 15, 'velocidade': 60},\n",
        "    'GPT-3': {'params': 175, 'performance': 90, 'custo': 100, 'velocidade': 30},\n",
        "    'GPT-4': {'params': 1000, 'performance': 95, 'custo': 500, 'velocidade': 15}\n",
        "}\n",
        "\n",
        "# Extraindo dados para visualização\n",
        "nomes = list(modelos.keys())\n",
        "params = [modelos[nome]['params'] for nome in nomes]\n",
        "performance = [modelos[nome]['performance'] for nome in nomes]\n",
        "custo = [modelos[nome]['custo'] for nome in nomes]\n",
        "velocidade = [modelos[nome]['velocidade'] for nome in nomes]\n",
        "\n",
        "# Criando visualização com múltiplos subplots\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Parâmetros vs Performance\n",
        "scatter = ax1.scatter(params, performance, s=[c*5 for c in custo], \n",
        "                    c=velocidade, cmap='RdYlGn', alpha=0.7)\n",
        "ax1.set_xscale('log')\n",
        "ax1.set_xlabel('Parâmetros (B)')\n",
        "ax1.set_ylabel('Performance (%)')\n",
        "ax1.set_title('🎯 Parâmetros vs Performance')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Adicionando labels\n",
        "for i, nome in enumerate(nomes):\n",
        "    ax1.annotate(nome, (params[i], performance[i]), \n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "# 2. Custo vs Performance\n",
        "ax2.bar(nomes, custo, color=['lightblue' if c < 10 else 'orange' if c < 100 else 'red' \n",
        "                            for c in custo])\n",
        "ax2.set_yscale('log')\n",
        "ax2.set_ylabel('Custo Relativo')\n",
        "ax2.set_title('💰 Custo por Modelo')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. Velocidade vs Tamanho\n",
        "ax3.plot(params, velocidade, 'ro-', linewidth=2, markersize=8)\n",
        "ax3.set_xscale('log')\n",
        "ax3.set_xlabel('Parâmetros (B)')\n",
        "ax3.set_ylabel('Velocidade Relativa')\n",
        "ax3.set_title('⚡ Velocidade vs Tamanho')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Radar chart das características\n",
        "# Escolhendo 3 modelos representativos\n",
        "modelos_radar = ['DistilBERT', 'LLaMA-7B', 'GPT-4']\n",
        "categorias = ['Performance', 'Velocidade', 'Eficiência\\n(inverso do custo)']\n",
        "\n",
        "angles = np.linspace(0, 2 * np.pi, len(categorias), endpoint=False).tolist()\n",
        "angles += angles[:1]  # Fechando o círculo\n",
        "\n",
        "ax4 = plt.subplot(2, 2, 4, projection='polar')\n",
        "\n",
        "for modelo in modelos_radar:\n",
        "    valores = [\n",
        "        modelos[modelo]['performance'],\n",
        "        modelos[modelo]['velocidade'],\n",
        "        100 - modelos[modelo]['custo']  # Inverso do custo = eficiência\n",
        "    ]\n",
        "    valores += valores[:1]  # Fechando o círculo\n",
        "    \n",
        "    ax4.plot(angles, valores, 'o-', linewidth=2, label=modelo)\n",
        "    ax4.fill(angles, valores, alpha=0.1)\n",
        "\n",
        "ax4.set_xticks(angles[:-1])\n",
        "ax4.set_xticklabels(categorias)\n",
        "ax4.set_ylim(0, 100)\n",
        "ax4.set_title('📊 Comparação Multi-dimensional')\n",
        "ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"🎯 Insights importantes:\")\n",
        "print(\"📈 Mais parâmetros = Melhor performance (mas nem sempre vale a pena!)\")\n",
        "print(\"💰 Custo cresce exponencialmente com o tamanho\")\n",
        "print(\"⚡ Velocidade diminui drasticamente com mais parâmetros\")\n",
        "print(\"🎪 Cada modelo tem seu 'sweet spot' para diferentes usos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Fine-tuning e Especialização\n\nAgora vamos falar do processo que transforma um modelo \"genérico\" em um especialista! É como pegar um médico recém-formado e fazer ele se especializar em cardiologia.\n\n### O que é Fine-tuning?\n\nÉ pegar um modelo já treinado (que custou milhões para treinar) e dar um \"cursinho\" específico para ele ficar expert em algo.\n\n**Processo**:\n1. **Modelo Base**: Já sabe linguagem em geral\n2. **Dataset Específico**: Dados da tarefa que queremos\n3. **Treinamento Adicional**: Algumas épocas de ajuste fino\n4. **Modelo Especializado**: Expert na nova tarefa!\n\n### Tipos de Fine-tuning\n\n#### 🎯 **Instruction Tuning**\n- **O que é**: Ensina o modelo a seguir instruções\n- **Dataset**: Pares de (instrução, resposta esperada)\n- **Resultado**: ChatGPT, Claude, Bard\n- **Exemplo**: \"Resuma este texto\" → modelo aprende a resumir\n\n#### 🏆 **RLHF (Reinforcement Learning from Human Feedback)**\n- **O que é**: Usa feedback humano para melhorar respostas\n- **Processo**: Humanos rankeiam respostas, modelo aprende o que é \"melhor\"\n- **Resultado**: Modelos mais alinhados e úteis\n- **Exemplo**: GPT-4 usa muito RLHF\n\n#### 🎨 **Domain-Specific Fine-tuning**\n- **O que é**: Especialização em uma área específica\n- **Exemplos**: \n  - BioBERT (medicina)\n  - FinBERT (finanças)\n  - CodeT5 (programação)\n  - LegalBERT (direito)\n\n**🎯 Dica do Pedro**: Fine-tuning é como personalizar um carro - você pega a base e adapta para suas necessidades específicas!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos simular o processo de fine-tuning!\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ModeloBase:\n",
        "    \"\"\"Simula um modelo base genérico\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.conhecimento_geral = {\n",
        "            'linguagem': 85,\n",
        "            'contexto': 80,\n",
        "            'coerencia': 75\n",
        "        }\n",
        "        self.especializacoes = {}  # Começa sem especializações\n",
        "    \n",
        "    def responder_generico(self, pergunta):\n",
        "        \"\"\"Resposta genérica sem especialização\"\"\"\n",
        "        respostas_genericas = [\n",
        "            \"Essa é uma pergunta interessante sobre {}\",\n",
        "            \"Posso tentar ajudar com informações gerais sobre {}\",\n",
        "            \"Com base no meu conhecimento geral, {} é um tópico complexo\"\n",
        "        ]\n",
        "        return random.choice(respostas_genericas).format(pergunta)\n",
        "\n",
        "class ModeloEspecializado(ModeloBase):\n",
        "    \"\"\"Simula um modelo após fine-tuning\"\"\"\n",
        "    \n",
        "    def __init__(self, especializacao):\n",
        "        super().__init__()\n",
        "        self.especializacao = especializacao\n",
        "        self.fazer_fine_tuning()\n",
        "    \n",
        "    def fazer_fine_tuning(self):\n",
        "        \"\"\"Simula o processo de fine-tuning\"\"\"\n",
        "        print(f\"🔧 Iniciando fine-tuning para {self.especializacao}...\")\n",
        "        \n",
        "        # Define conhecimentos específicos por área\n",
        "        especializacoes_data = {\n",
        "            'medicina': {\n",
        "                'diagnostico': 90,\n",
        "                'sintomas': 88,\n",
        "                'tratamentos': 85,\n",
        "                'respostas': {\n",
        "                    'dor de cabeça': 'Pode ser tensional, enxaqueca ou sinusite. Recomendo avaliação médica.',\n",
        "                    'febre': 'Sinal de infecção. Hidratação e antipirético podem ajudar.',\n",
        "                    'diabetes': 'Doença metabólica que requer controle glicêmico e acompanhamento.'\n",
        "                }\n",
        "            },\n",
        "            'programacao': {\n",
        "                'sintaxe': 95,\n",
        "                'algoritmos': 90,\n",
        "                'debugging': 87,\n",
        "                'respostas': {\n",
        "                    'python': 'Linguagem versátil, ideal para IA, web e automação.',\n",
        "                    'bug': 'Use print statements, debugger ou testes unitários para identificar.',\n",
        "                    'algoritmo': 'Sequência de passos para resolver um problema computacional.'\n",
        "                }\n",
        "            },\n",
        "            'culinaria': {\n",
        "                'receitas': 92,\n",
        "                'ingredientes': 89,\n",
        "                'tecnicas': 86,\n",
        "                'respostas': {\n",
        "                    'bolo': 'Misture ingredientes secos, adicione líquidos, asse a 180°C.',\n",
        "                    'tempero': 'Sal, pimenta, alho e cebola são bases universais.',\n",
        "                    'massa': 'Farinha, ovos e água. Sove bem e deixe descansar.'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        if self.especializacao in especializacoes_data:\n",
        "            self.especializacoes = especializacoes_data[self.especializacao]\n",
        "            print(f\"✅ Fine-tuning concluído! Especialização em {self.especializacao}\")\n",
        "        else:\n",
        "            print(f\"❌ Especialização {self.especializacao} não disponível\")\n",
        "    \n",
        "    def responder_especializado(self, pergunta):\n",
        "        \"\"\"Resposta especializada após fine-tuning\"\"\"\n",
        "        pergunta_lower = pergunta.lower()\n",
        "        \n",
        "        # Procura por palavras-chave especializadas\n",
        "        if 'respostas' in self.especializacoes:\n",
        "            for keyword, resposta in self.especializacoes['respostas'].items():\n",
        "                if keyword in pergunta_lower:\n",
        "                    return f\"[ESPECIALISTA {self.especializacao.upper()}]: {resposta}\"\n",
        "        \n",
        "        # Se não encontrou, usa conhecimento geral melhorado\n",
        "        return f\"[{self.especializacao.upper()}]: Com minha especialização, posso dizer que {pergunta} é um tópico que requer análise detalhada.\"\n",
        "\n",
        "# Demonstração do fine-tuning\n",
        "print(\"🧪 LABORATÓRIO: ANTES vs DEPOIS DO FINE-TUNING\\n\")\n",
        "\n",
        "# Modelo base (genérico)\n",
        "modelo_base = ModeloBase()\n",
        "print(\"🤖 MODELO BASE (sem especialização):\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "perguntas_teste = ['diabetes', 'python', 'bolo']\n",
        "for pergunta in perguntas_teste:\n",
        "    resposta = modelo_base.responder_generico(pergunta)\n",
        "    print(f\"❓ {pergunta}: {resposta}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# Modelos especializados\n",
        "especializacoes = ['medicina', 'programacao', 'culinaria']\n",
        "perguntas_especializada = ['diabetes', 'python', 'bolo']\n",
        "\n",
        "for i, (spec, pergunta) in enumerate(zip(especializacoes, perguntas_especializada)):\n",
        "    print(f\"🎯 MODELO ESPECIALIZADO ({spec.upper()}):\")\n",
        "    modelo_especializado = ModeloEspecializado(spec)\n",
        "    resposta = modelo_especializado.responder_especializado(pergunta)\n",
        "    print(f\"❓ {pergunta}: {resposta}\\n\")\n",
        "\n",
        "print(\"✨ Resumo:\")\n",
        "print(\"🤖 Base: Conhecimento amplo mas superficial\")\n",
        "print(\"🎯 Especializado: Conhecimento focado e profundo\")\n",
        "print(\"🔧 Fine-tuning = Transformar generalista em especialista!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\n",
        "graph LR\n",
        "    A[Modelo Base<br/>GPT-3] --> B[Dataset<br/>Especializado]\n",
        "    B --> C[Fine-tuning<br/>Process]\n",
        "    C --> D[Modelo<br/>Especializado]\n",
        "    \n",
        "    B1[\"📚 Instruction<br/>Dataset\"] --> C\n",
        "    B2[\"👨‍⚕️ Medical<br/>Dataset\"] --> C\n",
        "    B3[\"💻 Code<br/>Dataset\"] --> C\n",
        "    \n",
        "    D --> E1[\"🤖 ChatGPT<br/>(Instructions)\"]\n",
        "    D --> E2[\"⚕️ BioBERT<br/>(Medicine)\"]\n",
        "    D --> E3[\"💻 CodeT5<br/>(Programming)\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos visualizar o impacto do fine-tuning na performance!\n",
        "import numpy as np\n",
        "\n",
        "# Dados simulados de performance antes e depois do fine-tuning\n",
        "tarefas = ['Classificação\\nSentimento', 'Geração\\nCódigo', 'Diagnóstico\\nMédico', \n",
        "          'Tradução', 'Sumarização', 'Q&A Específico']\n",
        "\n",
        "# Performance do modelo base (genérico)\n",
        "perf_base = [65, 45, 40, 70, 60, 50]\n",
        "\n",
        "# Performance após fine-tuning específico\n",
        "perf_finetuned = [88, 85, 82, 90, 85, 87]\n",
        "\n",
        "# Criando visualização comparativa\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Gráfico 1: Comparação lado a lado\n",
        "x = np.arange(len(tarefas))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, perf_base, width, label='Modelo Base', \n",
        "               color='lightcoral', alpha=0.8)\n",
        "bars2 = ax1.bar(x + width/2, perf_finetuned, width, label='Após Fine-tuning', \n",
        "               color='lightgreen', alpha=0.8)\n",
        "\n",
        "ax1.set_xlabel('Tarefas')\n",
        "ax1.set_ylabel('Performance (%)')\n",
        "ax1.set_title('🎯 Impacto do Fine-tuning na Performance')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(tarefas, rotation=45, ha='right')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "             f'{height}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "             f'{height}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Gráfico 2: Ganho percentual\n",
        "ganho = [(f - b) / b * 100 for f, b in zip(perf_finetuned, perf_base)]\n",
        "cores_ganho = ['green' if g > 50 else 'orange' if g > 25 else 'red' for g in ganho]\n",
        "\n",
        "bars3 = ax2.bar(tarefas, ganho, color=cores_ganho, alpha=0.7)\n",
        "ax2.set_xlabel('Tarefas')\n",
        "ax2.set_ylabel('Ganho de Performance (%)')\n",
        "ax2.set_title('📈 Ganho Relativo com Fine-tuning')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Linha de referência\n",
        "ax2.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='50% de ganho')\n",
        "ax2.legend()\n",
        "\n",
        "# Adicionando valores\n",
        "for bar, valor in zip(bars3, ganho):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
        "             f'+{valor:.0f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Análise dos resultados\n",
        "print(\"🎯 ANÁLISE DOS RESULTADOS:\")\n",
        "print(\"=\" * 40)\n",
        "ganho_medio = np.mean(ganho)\n",
        "melhor_ganho = max(ganho)\n",
        "pior_ganho = min(ganho)\n",
        "\n",
        "print(f\"📊 Ganho médio: {ganho_medio:.1f}%\")\n",
        "print(f\"🏆 Melhor ganho: {melhor_ganho:.1f}% ({tarefas[ganho.index(melhor_ganho)]})\")\n",
        "print(f\"🤔 Menor ganho: {pior_ganho:.1f}% ({tarefas[ganho.index(pior_ganho)]})\")\n",
        "print(f\"\\n✨ Conclusão: Fine-tuning pode dobrar a performance em tarefas específicas!\")\n",
        "print(f\"🎯 Dica do Pedro: Vale muito a pena especializar quando você tem uma tarefa específica!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🏆 Comparação Prática: Qual Modelo Usar Quando?\n\nTá, Pedro, mas na prática, qual modelo eu uso para cada situação? Bora criar um guia prático!\n\n### 🎯 Guia de Escolha por Caso de Uso\n\n#### 📝 **Para Análise de Texto**\n- **Classificação de Sentimento**: BERT, RoBERTa (Encoder-Only)\n- **Detecção de Spam**: Modelos pequenos especializados\n- **Análise de Tópicos**: BERT + clustering\n- **Extração de Entidades**: spaCy + BERT\n\n#### 🤖 **Para Geração de Conteúdo**\n- **Chat/Conversação**: GPT-4, Claude, Bard\n- **Criação de Artigos**: GPT-3.5/4 + prompting\n- **Copywriting**: Modelos instruction-tuned\n- **Poesia/Criatividade**: Modelos grandes generativos\n\n#### 🔄 **Para Transformação de Texto**\n- **Tradução**: T5, mT5, modelos específicos (Google Translate)\n- **Sumarização**: BART, T5, Pegasus\n- **Paráfrase**: T5 fine-tuned\n- **Correção Gramatical**: T5 + datasets específicos\n\n#### 💻 **Para Código**\n- **Geração**: GitHub Copilot, CodeT5, StarCoder\n- **Revisão**: CodeBERT especializados\n- **Documentação**: GPT-4 + prompting específico\n- **Debug**: Modelos treinados em bugs/fixes\n\n#### 🏢 **Para Domínios Específicos**\n- **Medicina**: BioBERT, ClinicalBERT, Med-PaLM\n- **Finanças**: FinBERT, BloombergGPT\n- **Direito**: LegalBERT, modelos específicos\n- **Ciência**: SciBERT, Galactica\n\n**🎯 Dica do Pedro**: Sempre comece com o mais simples que resolve seu problema!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um sistema de recomendação de modelos!\n",
        "class RecomendadorModelos:\n",
        "    \"\"\"Sistema inteligente para recomendar o melhor modelo para cada caso\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.modelos_db = {\n",
        "            # Modelos para análise\n",
        "            'analise': {\n",
        "                'BERT-base': {\n",
        "                    'tipo': 'Encoder-Only',\n",
        "                    'tamanho': 'Médio (110M)',\n",
        "                    'velocidade': 'Rápida',\n",
        "                    'custo': 'Baixo',\n",
        "                    'casos': ['classificação', 'sentimento', 'qa'],\n",
        "                    'pros': ['Boa performance', 'Eficiente', 'Bem documentado'],\n",
        "                    'contras': ['Não gera texto', 'Limitado a 512 tokens']\n",
        "                },\n",
        "                'RoBERTa': {\n",
        "                    'tipo': 'Encoder-Only',\n",
        "                    'tamanho': 'Médio (125M)',\n",
        "                    'velocidade': 'Rápida',\n",
        "                    'custo': 'Baixo',\n",
        "                    'casos': ['classificação', 'análise', 'ner'],\n",
        "                    'pros': ['Melhor que BERT', 'Robusto', 'Estado da arte'],\n",
        "                    'contras': ['Maior que BERT', 'Só análise']\n",
        "                }\n",
        "            },\n",
        "            \n",
        "            # Modelos para geração\n",
        "            'geracao': {\n",
        "                'GPT-3.5-turbo': {\n",
        "                    'tipo': 'Decoder-Only',\n",
        "                    'tamanho': 'Grande (~20B)',\n",
        "                    'velocidade': 'Média',\n",
        "                    'custo': 'Médio',\n",
        "                    'casos': ['chat', 'geração', 'criatividade'],\n",
        "                    'pros': ['Versátil', 'Boa qualidade', 'API fácil'],\n",
        "                    'contras': ['Pago', 'Online only', 'Alucinações']\n",
        "                },\n",
        "                'LLaMA-7B': {\n",
        "                    'tipo': 'Decoder-Only',\n",
        "                    'tamanho': 'Médio (7B)',\n",
        "                    'velocidade': 'Média',\n",
        "                    'custo': 'Médio',\n",
        "                    'casos': ['geração', 'chat', 'fine-tuning'],\n",
        "                    'pros': ['Open source', 'Boa qualidade', 'Customizável'],\n",
        "                    'contras': ['Precisa GPU', 'Setup complexo']\n",
        "                }\n",
        "            },\n",
        "            \n",
        "            # Modelos para transformação\n",
        "            'transformacao': {\n",
        "                'T5-base': {\n",
        "                    'tipo': 'Encoder-Decoder',\n",
        "                    'tamanho': 'Médio (220M)',\n",
        "                    'velocidade': 'Média',\n",
        "                    'custo': 'Médio',\n",
        "                    'casos': ['tradução', 'sumarização', 'qa'],\n",
        "                    'pros': ['Versátil', 'Text-to-text', 'Bem treinado'],\n",
        "                    'contras': ['Complexo', 'Precisa fine-tuning']\n",
        "                },\n",
        "                'BART': {\n",
        "                    'tipo': 'Encoder-Decoder',\n",
        "                    'tamanho': 'Médio (140M)',\n",
        "                    'velocidade': 'Rápida',\n",
        "                    'custo': 'Baixo',\n",
        "                    'casos': ['sumarização', 'paráfrase'],\n",
        "                    'pros': ['Ótimo para resumos', 'Eficiente'],\n",
        "                    'contras': ['Limitado em domínio']\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def recomendar(self, tarefa, orcamento='médio', velocidade_necessaria='média'):\n",
        "        \"\"\"Recomenda o melhor modelo baseado nos critérios\"\"\"\n",
        "        print(f\"🎯 RECOMENDAÇÃO PARA: {tarefa.upper()}\")\n",
        "        print(f\"💰 Orçamento: {orcamento} | ⚡ Velocidade: {velocidade_necessaria}\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Mapeia tarefas para categorias\n",
        "        mapa_tarefas = {\n",
        "            'classificação': 'analise',\n",
        "            'sentimento': 'analise',\n",
        "            'ner': 'analise',\n",
        "            'qa': 'analise',\n",
        "            'chat': 'geracao',\n",
        "            'geração': 'geracao',\n",
        "            'criatividade': 'geracao',\n",
        "            'tradução': 'transformacao',\n",
        "            'sumarização': 'transformacao',\n",
        "            'paráfrase': 'transformacao'\n",
        "        }\n",
        "        \n",
        "        categoria = mapa_tarefas.get(tarefa, 'geracao')\n",
        "        modelos_categoria = self.modelos_db[categoria]\n",
        "        \n",
        "        # Filtra modelos baseado nos critérios\n",
        "        recomendacoes = []\n",
        "        \n",
        "        for nome, info in modelos_categoria.items():\n",
        "            if tarefa in info['casos']:\n",
        "                score = self._calcular_score(info, orcamento, velocidade_necessaria)\n",
        "                recomendacoes.append((nome, info, score))\n",
        "        \n",
        "        # Ordena por score\n",
        "        recomendacoes.sort(key=lambda x: x[2], reverse=True)\n",
        "        \n",
        "        # Apresenta as recomendações\n",
        "        for i, (nome, info, score) in enumerate(recomendacoes[:3]):\n",
        "            emoji = \"🥇\" if i == 0 else \"🥈\" if i == 1 else \"🥉\"\n",
        "            print(f\"{emoji} **{nome}** (Score: {score:.1f}/10)\")\n",
        "            print(f\"   📊 {info['tipo']} | {info['tamanho']} | {info['velocidade']} | {info['custo']}\")\n",
        "            print(f\"   ✅ Pros: {', '.join(info['pros'])}\")\n",
        "            print(f\"   ⚠️  Contras: {', '.join(info['contras'])}\")\n",
        "            print()\n",
        "        \n",
        "        return recomendacoes[0][0] if recomendacoes else None\n",
        "    \n",
        "    def _calcular_score(self, info, orcamento, velocidade):\n",
        "        \"\"\"Calcula score baseado nos critérios\"\"\"\n",
        "        score = 5.0  # Base\n",
        "        \n",
        "        # Ajuste por orçamento\n",
        "        if orcamento == 'baixo' and info['custo'] == 'Baixo':\n",
        "            score += 2\n",
        "        elif orcamento == 'alto' and info['custo'] == 'Alto':\n",
        "            score += 1\n",
        "        elif orcamento != info['custo'].lower():\n",
        "            score -= 1\n",
        "        \n",
        "        # Ajuste por velocidade\n",
        "        if velocidade == 'rápida' and info['velocidade'] == 'Rápida':\n",
        "            score += 2\n",
        "        elif velocidade != info['velocidade'].lower():\n",
        "            score -= 0.5\n",
        "        \n",
        "        return min(10, max(0, score))\n",
        "\n",
        "# Testando o sistema de recomendação!\n",
        "recomendador = RecomendadorModelos()\n",
        "\n",
        "print(\"🤖 SISTEMA DE RECOMENDAÇÃO DE MODELOS\\n\")\n",
        "\n",
        "# Casos de teste\n",
        "casos_teste = [\n",
        "    ('sentimento', 'baixo', 'rápida'),\n",
        "    ('chat', 'médio', 'média'),\n",
        "    ('sumarização', 'baixo', 'média')\n",
        "]\n",
        "\n",
        "for tarefa, orcamento, velocidade in casos_teste:\n",
        "    melhor_modelo = recomendador.recomendar(tarefa, orcamento, velocidade)\n",
        "    print(f\"🎯 Melhor escolha: {melhor_modelo}\")\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "print(\"✨ Dica do Pedro: Use este sistema como ponto de partida!\")\n",
        "print(\"🧪 Sempre teste na prática para validar a escolha!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧪 Exercício Prático 1: Classificação de Modelos\n\n**Desafio**: Dado um conjunto de modelos e suas características, classifique-os nas categorias corretas!\n\nVou te dar as características, você me diz:\n1. Arquitetura (Encoder-Only, Decoder-Only, Encoder-Decoder)\n2. Tipo (Generativo ou Discriminativo)\n3. Melhor caso de uso\n\n**🎯 Dica do Pedro**: Lembra das características que aprendemos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÍCIO 1: Sistema de Quiz sobre Tipos de Modelos\n",
        "import random\n",
        "\n",
        "class QuizModelos:\n",
        "    def __init__(self):\n",
        "        self.perguntas = [\n",
        "            {\n",
        "                'modelo': 'DistilBERT',\n",
        "                'características': ['Lê texto completo de uma vez', 'Excelente para classificação', 'Não gera texto novo', 'Menor que BERT original'],\n",
        "                'arquitetura': 'Encoder-Only',\n",
        "                'tipo': 'Discriminativo',\n",
        "                'uso': 'Análise de sentimento'\n",
        "            },\n",
        "            {\n",
        "                'modelo': 'GPT-2',\n",
        "                'características': ['Gera uma palavra por vez', 'Excelente para criar histórias', 'Lê da esquerda para direita', 'Pode continuar qualquer texto'],\n",
        "                'arquitetura': 'Decoder-Only',\n",
        "                'tipo': 'Generativo',\n",
        "                'uso': 'Geração de texto'\n",
        "            },\n",
        "            {\n",
        "                'modelo': 'T5',\n",
        "                'características': ['Transforma um texto em outro', 'Ótimo para tradução', 'Usa formato \"text-to-text\"', 'Tem encoder e decoder'],\n",
        "                'arquitetura': 'Encoder-Decoder',\n",
        "                'tipo': 'Generativo',\n",
        "                'uso': 'Tradução'\n",
        "            },\n",
        "            {\n",
        "                'modelo': 'RoBERTa',\n",
        "                'características': ['Versão melhorada do BERT', 'Apenas analisa, não gera', 'Bidirecional', 'Ótimo para Q&A'],\n",
        "                'arquitetura': 'Encoder-Only',\n",
        "                'tipo': 'Discriminativo',\n",
        "                'uso': 'Question Answering'\n",
        "            }\n",
        "        ]\n",
        "        self.pontuacao = 0\n",
        "        self.total_perguntas = 0\n",
        "    \n",
        "    def fazer_pergunta(self, pergunta_data):\n",
        "        print(f\"🤖 MODELO: {pergunta_data['modelo']}\")\n",
        "        print(\"📋 CARACTERÍSTICAS:\")\n",
        "        for i, carac in enumerate(pergunta_data['características'], 1):\n",
        "            print(f\"   {i}. {carac}\")\n",
        "        print()\n",
        "        \n",
        "        # Pergunta 1: Arquitetura\n",
        "        print(\"❓ Qual a arquitetura deste modelo?\")\n",
        "        print(\"   A) Encoder-Only\")\n",
        "        print(\"   B) Decoder-Only\")\n",
        "        print(\"   C) Encoder-Decoder\")\n",
        "        \n",
        "        resposta_arq = input(\"Sua resposta (A/B/C): \").upper().strip()\n",
        "        arquiteturas = {'A': 'Encoder-Only', 'B': 'Decoder-Only', 'C': 'Encoder-Decoder'}\n",
        "        \n",
        "        if arquiteturas.get(resposta_arq) == pergunta_data['arquitetura']:\n",
        "            print(\"✅ Correto!\")\n",
        "            self.pontuacao += 1\n",
        "        else:\n",
        "            print(f\"❌ Incorreto. Resposta: {pergunta_data['arquitetura']}\")\n",
        "        \n",
        "        # Pergunta 2: Tipo\n",
        "        print(\"\\n❓ Este modelo é:\")\n",
        "        print(\"   A) Generativo (cria conteúdo novo)\")\n",
        "        print(\"   B) Discriminativo (classifica/analisa)\")\n",
        "        \n",
        "        resposta_tipo = input(\"Sua resposta (A/B): \").upper().strip()\n",
        "        tipos = {'A': 'Generativo', 'B': 'Discriminativo'}\n",
        "        \n",
        "        if tipos.get(resposta_tipo) == pergunta_data['tipo']:\n",
        "            print(\"✅ Correto!\")\n",
        "            self.pontuacao += 1\n",
        "        else:\n",
        "            print(f\"❌ Incorreto. Resposta: {pergunta_data['tipo']}\")\n",
        "        \n",
        "        self.total_perguntas += 2\n",
        "        print(f\"\\n📊 Pontuação atual: {self.pontuacao}/{self.total_perguntas}\")\n",
        "        print(\"=\"*50)\n",
        "    \n",
        "    def executar_quiz(self, num_perguntas=2):\n",
        "        print(\"🎯 QUIZ: TIPOS DE MODELOS\")\n",
        "        print(\"Vamos testar seu conhecimento!\\n\")\n",
        "        \n",
        "        perguntas_selecionadas = random.sample(self.perguntas, min(num_perguntas, len(self.perguntas)))\n",
        "        \n",
        "        for pergunta in perguntas_selecionadas:\n",
        "            self.fazer_pergunta(pergunta)\n",
        "            input(\"\\nPressione Enter para continuar...\")\n",
        "            print(\"\\n\")\n",
        "        \n",
        "        # Resultado final\n",
        "        percentual = (self.pontuacao / self.total_perguntas) * 100\n",
        "        print(\"🏆 RESULTADO FINAL\")\n",
        "        print(f\"📊 Pontuação: {self.pontuacao}/{self.total_perguntas} ({percentual:.1f}%)\")\n",
        "        \n",
        "        if percentual >= 80:\n",
        "            print(\"🥇 Excelente! Você domina os tipos de modelos!\")\n",
        "        elif percentual >= 60:\n",
        "            print(\"🥈 Bom trabalho! Você entende bem o assunto!\")\n",
        "        else:\n",
        "            print(\"📚 Continue estudando! Releia o material!\")\n",
        "\n",
        "# Executando o quiz (versão automática para o notebook)\n",
        "print(\"🎯 QUIZ AUTOMÁTICO - TIPOS DE MODELOS\\n\")\n",
        "\n",
        "# Simulando respostas para demonstração\n",
        "quiz = QuizModelos()\n",
        "exemplos = [\n",
        "    {\n",
        "        'modelo': 'BERT',\n",
        "        'características': ['Bidirecional', 'Máscara tokens', 'Classificação', 'Não gera'],\n",
        "        'resposta_arq': 'Encoder-Only',\n",
        "        'resposta_tipo': 'Discriminativo'\n",
        "    },\n",
        "    {\n",
        "        'modelo': 'GPT-3',\n",
        "        'características': ['Autoregressivo', 'Gera texto', 'Uma palavra por vez', 'Criativo'],\n",
        "        'resposta_arq': 'Decoder-Only',\n",
        "        'resposta_tipo': 'Generativo'\n",
        "    }\n",
        "]\n",
        "\n",
        "for exemplo in exemplos:\n",
        "    print(f\"🤖 MODELO: {exemplo['modelo']}\")\n",
        "    print(\"📋 CARACTERÍSTICAS:\")\n",
        "    for i, carac in enumerate(exemplo['características'], 1):\n",
        "        print(f\"   {i}. {carac}\")\n",
        "    print(f\"\\n✅ Arquitetura: {exemplo['resposta_arq']}\")\n",
        "    print(f\"✅ Tipo: {exemplo['resposta_tipo']}\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"🎯 Agora é sua vez! Tente classificar outros modelos que você conhece!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Exercício Prático 2: Cenário de Escolha de Modelo\n\n**Cenário Real**: Você foi contratado por uma startup brasileira de e-commerce para implementar IA em diferentes partes do sistema.\n\n**Requisitos**:\n1. **Análise de Reviews**: Classificar reviews em positivo/negativo\n2. **Chatbot**: Responder dúvidas dos clientes\n3. **Tradução**: Traduzir produtos para outros idiomas\n4. **Geração de Descrições**: Criar descrições automáticas de produtos\n\n**Restrições**:\n- Orçamento limitado (startup!)\n- Precisa de respostas rápidas\n- Dados em português\n- Alguns dados sensíveis (não pode usar APIs externas)\n\n**Sua missão**: Escolha o modelo ideal para cada caso e justifique!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÍCIO 2: Solucionando Cenário Real de E-commerce\n",
        "\n",
        "class ConsultorIA:\n",
        "    \"\"\"Simulação de um consultor especialista em IA\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.solucoes = {\n",
        "            'reviews': {\n",
        "                'problema': 'Classificar reviews em positivo/negativo',\n",
        "                'requisitos': ['Rápido', 'Baixo custo', 'Português', 'Offline'],\n",
        "                'modelo_recomendado': 'BERT-base multilingual fine-tuned',\n",
        "                'justificativa': [\n",
        "                    '✅ Encoder-Only: Perfeito para classificação',\n",
        "                    '✅ Discriminativo: Foco em análise, não geração',\n",
        "                    '✅ Multilingual: Suporta português nativamente',\n",
        "                    '✅ Fine-tuning: Pode especializar em reviews',\n",
        "                    '✅ Offline: Roda localmente, dados seguros',\n",
        "                    '✅ Custo: Relativamente barato de rodar'\n",
        "                ],\n",
        "                'alternativas': ['RoBERTa-PT', 'DistilBERT (mais rápido)'],\n",
        "                'implementacao': 'Transformers + PyTorch, fine-tune com reviews brasileiros'\n",
        "            },\n",
        "            \n",
        "            'chatbot': {\n",
        "                'problema': 'Responder dúvidas dos clientes',\n",
        "                'requisitos': ['Conversacional', 'Português', 'Custo moderado', 'Contextual'],\n",
        "                'modelo_recomendado': 'LLaMA-7B fine-tuned + RAG',\n",
        "                'justificativa': [\n",
        "                    '✅ Decoder-Only: Excelente para conversação',\n",
        "                    '✅ Generativo: Cria respostas naturais',\n",
        "                    '✅ 7B: Bom equilíbrio tamanho/qualidade',\n",
        "                    '✅ Fine-tuning: Especializar em e-commerce',\n",
        "                    '✅ RAG: Conecta com base de conhecimento',\n",
        "                    '✅ Open-source: Sem custos de API'\n",
        "                ],\n",
        "                'alternativas': ['GPT-3.5 (se budget permitir)', 'Alpaca-7B'],\n",
        "                'implementacao': 'LangChain + Chroma DB para RAG, fine-tune com conversas'\n",
        "            },\n",
        "            \n",
        "            'traducao': {\n",
        "                'problema': 'Traduzir produtos para outros idiomas',\n",
        "                'requisitos': ['Transformação texto', 'Múltiplos idiomas', 'Qualidade alta'],\n",
        "                'modelo_recomendado': 'mT5 ou MarianMT específico',\n",
        "                'justificativa': [\n",
        "                    '✅ Encoder-Decoder: Ideal para tradução',\n",
        "                    '✅ Multilingual: Suporta vários idiomas',\n",
        "                    '✅ Especializado: Foco em tradução',\n",
        "                    '✅ Open-source: Sem custos por uso',\n",
        "                    '✅ Fine-tuning: Pode melhorar para e-commerce'\n",
        "                ],\n",
        "                'alternativas': ['Google Translate API (pago)', 'OPUS-MT'],\n",
        "                'implementacao': 'Transformers, modelos específicos por par de idiomas'\n",
        "            },\n",
        "            \n",
        "            'descricoes': {\n",
        "                'problema': 'Gerar descrições automáticas de produtos',\n",
        "                'requisitos': ['Criativo', 'Persuasivo', 'Português', 'Baseado em features'],\n",
        "                'modelo_recomendado': 'GPT-3.5-turbo com prompting ou T5 fine-tuned',\n",
        "                'justificativa': [\n",
        "                    '✅ Generativo: Cria conteúdo original',\n",
        "                    '✅ Criativo: Textos persuasivos de marketing',\n",
        "                    '✅ Estruturado: Pode seguir templates',\n",
        "                    '✅ Prompting: Controle sobre estilo e tom',\n",
        "                    '⚠️ Custo: GPT-3.5 é pago, T5 offline'\n",
        "                ],\n",
        "                'alternativas': ['LLaMA fine-tuned', 'T5 + templates'],\n",
        "                'implementacao': 'API OpenAI ou T5 fine-tuned com descrições existentes'\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def analisar_caso(self, caso):\n",
        "        \"\"\"Analisa um caso específico e apresenta a solução\"\"\"\n",
        "        if caso not in self.solucoes:\n",
        "            print(f\"❌ Caso '{caso}' não encontrado!\")\n",
        "            return\n",
        "        \n",
        "        solucao = self.solucoes[caso]\n",
        "        \n",
        "        print(f\"🎯 CASO: {solucao['problema'].upper()}\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        print(\"📋 REQUISITOS:\")\n",
        "        for req in solucao['requisitos']:\n",
        "            print(f\"   • {req}\")\n",
        "        \n",
        "        print(f\"\\n🏆 MODELO RECOMENDADO: {solucao['modelo_recomendado']}\")\n",
        "        \n",
        "        print(\"\\n💡 JUSTIFICATIVA:\")\n",
        "        for just in solucao['justificativa']:\n",
        "            print(f\"   {just}\")\n",
        "        \n",
        "        print(f\"\\n🔄 ALTERNATIVAS: {', '.join(solucao['alternativas'])}\")\n",
        "        \n",
        "        print(f\"\\n🛠️ IMPLEMENTAÇÃO: {solucao['implementacao']}\")\n",
        "        \n",
        "        return solucao\n",
        "    \n",
        "    def resumo_arquitetura(self):\n",
        "        \"\"\"Apresenta resumo da arquitetura completa\"\"\"\n",
        "        print(\"🏗️ ARQUITETURA COMPLETA DA SOLUÇÃO\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        arquitetura = {\n",
        "            '🔍 Reviews (Análise)': 'BERT multilingual → Classificação Sentimento',\n",
        "            '🤖 Chatbot (Conversação)': 'LLaMA-7B + RAG → Respostas Contextuais',\n",
        "            '🌍 Tradução (Transformação)': 'mT5/MarianMT → Múltiplos Idiomas',\n",
        "            '✍️ Descrições (Geração)': 'GPT-3.5 ou T5 → Conteúdo Criativo'\n",
        "        }\n",
        "        \n",
        "        for funcao, solucao in arquitetura.items():\n",
        "            print(f\"{funcao}: {solucao}\")\n",
        "        \n",
        "        print(\"\\n💰 ESTIMATIVA DE CUSTOS (mensal):\")\n",
        "        custos = {\n",
        "            'Infrastructure (GPU)': 'R$ 2.000',\n",
        "            'GPT-3.5 API': 'R$ 500-1.500',\n",
        "            'Storage & Compute': 'R$ 300',\n",
        "            'Total estimado': 'R$ 2.800-3.800/mês'\n",
        "        }\n",
        "        \n",
        "        for item, valor in custos.items():\n",
        "            print(f\"   {item}: {valor}\")\n",
        "\n",
        "# Executando a análise completa\n",
        "consultor = ConsultorIA()\n",
        "\n",
        "print(\"🚀 CONSULTORIA IA: E-COMMERCE BRASILEIRO\\n\")\n",
        "\n",
        "casos = ['reviews', 'chatbot', 'traducao', 'descricoes']\n",
        "\n",
        "for caso in casos:\n",
        "    consultor.analisar_caso(caso)\n",
        "    print(\"\\n\" + \"*\"*70 + \"\\n\")\n",
        "\n",
        "consultor.resumo_arquitetura()\n",
        "\n",
        "print(\"\\n🎯 DICAS FINAIS:\")\n",
        "print(\"1. 🧪 Sempre faça POC (Proof of Concept) antes\")\n",
        "print(\"2. 📊 Meça performance em dados reais brasileiros\")\n",
        "print(\"3. 🔄 Comece simples, evolua conforme necessidade\")\n",
        "print(\"4. 💰 Monitore custos constantemente\")\n",
        "print(\"5. 🛡️ Considere privacidade e LGPD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎭 Resumo: O Grande Teatro dos LLMs\n\nLiiindo! Chegamos ao final da nossa jornada pelo mundo dos tipos de modelos! 🎪\n\n### 🎯 O que aprendemos:\n\n#### 🏗️ **Divisão Fundamental**\n- **Modelos Base**: O \"padeiro genérico\" que sabe o básico\n- **Modelos Especializados**: O \"confeiteiro expert\" focado em algo específico\n\n#### 🏛️ **Arquiteturas (as ferramentas)**\n- **Encoder-Only** 🔍: A lupa - para analisar e entender\n- **Decoder-Only** 🎯: A caneta - para criar e gerar\n- **Encoder-Decoder** 🔄: O tradutor - para transformar\n\n#### 🎨 **Tipos Fundamentais**\n- **Generativo**: O artista que cria do zero\n- **Discriminativo**: O crítico que analisa e julga\n\n#### 📏 **Tamanhos e Capacidades**\n- **Pequenos**: Carro compacto - econômico para o dia a dia\n- **Médios**: SUV - equilíbrio entre performance e praticidade\n- **Grandes**: Caminhão - potente para trabalhos pesados\n- **Gigantes**: Foguete - para missões impossíveis\n\n#### 🔧 **Fine-tuning: A Especialização**\n- Transformar generalista em especialista\n- Instruction Tuning, RLHF, Domain-specific\n- Como personalizar um carro para suas necessidades\n\n### 🎯 **Dicas de Ouro do Pedro:**\n\n1. **Comece Simples**: Nem sempre precisa do GPT-4!\n2. **Conheça seu Problema**: Análise → Encoder, Geração → Decoder, Transformação → Encoder-Decoder\n3. **Considere Recursos**: Tempo, dinheiro, infraestrutura\n4. **Teste na Prática**: POC antes de produção\n5. **Evolua Gradualmente**: Comece pequeno, cresça conforme necessidade\n\n### 🔮 **Preparando para o Próximo Módulo**\n\nNo **Módulo 7 - Treinamento e Pre-treinamento**, vamos descobrir:\n- Como esses modelos são \"nascidos\" e \"educados\"\n- O processo completo de treinamento\n- Pre-training vs Fine-tuning em detalhes\n- Custos e desafios do treinamento\n\n**🎪 Até a próxima, e lembra: cada modelo tem seu palco onde brilhar!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Célula final - Recapitulação visual e próximos passos\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.patches import FancyBboxPatch\n",
        "\n",
        "# Criando um mapa mental visual do que aprendemos\n",
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "\n",
        "# Centro - Tipos de Modelos\n",
        "centro = FancyBboxPatch((7, 5.5), 2, 1, boxstyle=\"round,pad=0.1\", \n",
        "                       facecolor='gold', edgecolor='darkred', linewidth=3)\n",
        "ax.add_patch(centro)\n",
        "ax.text(8, 6, 'TIPOS DE\\nMODELOS', ha='center', va='center', \n",
        "        fontsize=14, fontweight='bold')\n",
        "\n",
        "# Categorias principais\n",
        "categorias = [\n",
        "    # (nome, x, y, cor, conteúdo)\n",
        "    ('ARQUITETURAS', 3, 9, 'lightblue', 'Encoder-Only\\nDecoder-Only\\nEncoder-Decoder'),\n",
        "    ('TAMANHOS', 13, 9, 'lightgreen', 'Pequenos (<1B)\\nMédios (1-10B)\\nGrandes (10-100B)\\nGigantes (>100B)'),\n",
        "    ('TIPOS', 3, 3, 'lightcoral', 'Generativo\\n(Cria)\\nDiscriminativo\\n(Analisa)'),\n",
        "    ('ESPECIALIZAÇÃO', 13, 3, 'lightyellow', 'Base Models\\nFine-tuned\\nInstruction\\nRLHF'),\n",
        "]\n",
        "\n",
        "for nome, x, y, cor, conteudo in categorias:\n",
        "    # Caixa da categoria\n",
        "    caixa = FancyBboxPatch((x-1, y-1), 2, 2, boxstyle=\"round,pad=0.1\", \n",
        "                          facecolor=cor, edgecolor='navy', linewidth=2)\n",
        "    ax.add_patch(caixa)\n",
        "    \n",
        "    # Título da categoria\n",
        "    ax.text(x, y+0.7, nome, ha='center', va='center', \n",
        "            fontsize=11, fontweight='bold')\n",
        "    \n",
        "    # Conteúdo\n",
        "    ax.text(x, y-0.2, conteudo, ha='center', va='center', \n",
        "            fontsize=9)\n",
        "    \n",
        "    # Seta para o centro\n",
        "    ax.arrow(x, y-1 if y > 6 else y+1, 8-x, (5.5 if y < 6 else 6.5)-y, \n",
        "             head_width=0.1, head_length=0.1, fc='gray', ec='gray', alpha=0.5)\n",
        "\n",
        "# Exemplos práticos nas laterais\n",
        "exemplos_esq = ['BERT\\n(Análise)', 'T5\\n(Tradução)', 'GPT\\n(Chat)']\n",
        "exemplos_dir = ['BioBERT\\n(Medicina)', 'CodeT5\\n(Código)', 'FinBERT\\n(Finanças)']\n",
        "\n",
        "for i, (esq, dir) in enumerate(zip(exemplos_esq, exemplos_dir)):\n",
        "    y_pos = 8 - i * 2\n",
        "    \n",
        "    # Esquerda\n",
        "    ax.text(0.5, y_pos, esq, ha='center', va='center', \n",
        "            fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightpink'))\n",
        "    \n",
        "    # Direita\n",
        "    ax.text(15.5, y_pos, dir, ha='center', va='center', \n",
        "            fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightcyan'))\n",
        "\n",
        "# Título\n",
        "ax.text(8, 11, '🎭 MAPA MENTAL: TIPOS DE MODELOS LLM', \n",
        "        ha='center', va='center', fontsize=18, fontweight='bold')\n",
        "\n",
        "# Rodapé com próximos passos\n",
        "ax.text(8, 0.5, '🔮 PRÓXIMO: Módulo 7 - Treinamento e Pre-treinamento', \n",
        "        ha='center', va='center', fontsize=12, style='italic', \n",
        "        bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lavender'))\n",
        "\n",
        "ax.set_xlim(-1, 17)\n",
        "ax.set_ylim(0, 12)\n",
        "ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"🎯 PARABÉNS! Você completou o Módulo 6!\")\n",
        "print(\"\\n📚 CHECKLIST DO QUE VOCÊ APRENDEU:\")\n",
        "checklist = [\n",
        "    \"✅ Diferença entre modelos base e especializados\",\n",
        "    \"✅ Três arquiteturas principais (Encoder/Decoder/Both)\",\n",
        "    \"✅ Generativo vs Discriminativo\",\n",
        "    \"✅ Impacto do tamanho nos modelos\",\n",
        "    \"✅ Processo de fine-tuning\",\n",
        "    \"✅ Como escolher modelo para cada tarefa\",\n",
        "    \"✅ Casos práticos de uso\"\n",
        "]\n",
        "\n",
        "for item in checklist:\n",
        "    print(f\"   {item}\")\n",
        "\n",
        "print(\"\\n🚀 VOCÊ ESTÁ PRONTO PARA:\")\n",
        "print(\"   • Escolher o modelo certo para cada projeto\")\n",
        "print(\"   • Entender custos e trade-offs\")\n",
        "print(\"   • Planejar arquiteturas de IA\")\n",
        "print(\"   • Aprender sobre treinamento (próximo módulo!)\")\n",
        "\n",
        "print(\"\\n🎪 Até o próximo módulo! Bora descobrir como esses modelos 'nascem'!\")"
      ]
    }
  ]
}