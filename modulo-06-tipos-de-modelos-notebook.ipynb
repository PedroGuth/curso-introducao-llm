{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé≠ O Grande Teatro dos LLMs: Conhecendo os Diferentes Tipos de Modelos\n\n**M√≥dulo 6 - Introdu√ß√£o √† LLMs**\n\n*Por Pedro Nunes Guth*\n\n---\n\nBora! Chegou a hora de conhecer os diferentes \"personagens\" do mundo dos LLMs! üé™\n\nT√°, mas Pedro, por que \"teatro\"? Simples! Cada tipo de modelo tem sua especialidade, seu papel espec√≠fico no palco da IA. √â como uma companhia teatral: tem o protagonista, o coadjuvante, o especialista em com√©dia, o dram√°tico... cada um brilha de um jeito!\n\nNeste m√≥dulo vamos explorar:\n- üèóÔ∏è **Modelos Base vs. Modelos Especializados**\n- üéØ **Decoder-Only, Encoder-Only e Encoder-Decoder**\n- üöÄ **Modelos Generativos vs. Discriminativos**\n- üîß **Fine-tuning e Especializa√ß√£o**\n- üìä **Compara√ß√£o de Performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Preparando nosso laborat√≥rio!\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import display, Markdown, HTML\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√µes visuais\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"üé≠ Ambiente configurado! Bora conhecer os tipos de modelos!\")\n",
        "print(\"üìö Lembra dos m√≥dulos anteriores:\")\n",
        "print(\"   ‚Ä¢ Transformers (M√≥dulo 3)\")\n",
        "print(\"   ‚Ä¢ Tokens (M√≥dulo 4)\")\n",
        "print(\"   ‚Ä¢ Embeddings (M√≥dulo 5)\")\n",
        "print(\"Agora vamos ver como tudo isso se junta em diferentes tipos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Modelos Base vs. Modelos Especializados\n\nT√°, vamos come√ßar com o b√°sico! Imagina que voc√™ tem uma padaria:\n\n- **Modelo Base**: √â como um padeiro que sabe fazer p√£o b√°sico muito bem, mas ainda n√£o se especializou em nada espec√≠fico\n- **Modelo Especializado**: √â o mesmo padeiro depois de fazer curso de confeitaria, agora ele faz bolos incr√≠veis!\n\n### Modelos Base (Foundation Models)\n\nS√£o os modelos \"crus\", treinados em grandes quantidades de texto para aprender padr√µes gerais da linguagem. Eles sabem:\n- Completar frases\n- Entender contexto b√°sico\n- Gerar texto coerente\n\n**Exemplos**: GPT-3 base, LLaMA base, PaLM base\n\n### Modelos Especializados (Fine-tuned Models)\n\nS√£o modelos base que passaram por treinamento adicional para tarefas espec√≠ficas:\n- **Instru√ß√£o**: ChatGPT, Claude (seguem comandos)\n- **Conversa√ß√£o**: Bard, Assistant models\n- **C√≥digo**: CodeT5, GitHub Copilot\n- **Dom√≠nio espec√≠fico**: BioBERT (medicina), FinBERT (finan√ßas)\n\n**üéØ Dica do Pedro**: Pense assim - modelo base √© como saber portugu√™s, modelo especializado √© como ser jornalista, advogado ou poeta!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma visualiza√ß√£o dos tipos de modelos!\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle, FancyBboxPatch\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "# Modelo Base (Foundation)\n",
        "base_box = FancyBboxPatch((1, 7), 4, 2, \n",
        "                         boxstyle=\"round,pad=0.1\", \n",
        "                         facecolor='lightblue', \n",
        "                         edgecolor='navy', linewidth=2)\n",
        "ax.add_patch(base_box)\n",
        "ax.text(3, 8, 'MODELO BASE\\n(Foundation)', ha='center', va='center', \n",
        "        fontsize=12, fontweight='bold')\n",
        "\n",
        "# Modelos Especializados\n",
        "specializations = [\n",
        "    ('Chat/Instru√ß√£o\\n(ChatGPT)', 0.5, 4, 'lightgreen'),\n",
        "    ('C√≥digo\\n(Copilot)', 2.5, 4, 'lightyellow'),\n",
        "    ('Dom√≠nio\\n(BioBERT)', 4.5, 4, 'lightcoral'),\n",
        "    ('Classifica√ß√£o\\n(Sentiment)', 6.5, 4, 'lightpink')\n",
        "]\n",
        "\n",
        "for name, x, y, color in specializations:\n",
        "    spec_box = FancyBboxPatch((x, y), 1.8, 1.5, \n",
        "                             boxstyle=\"round,pad=0.1\", \n",
        "                             facecolor=color, \n",
        "                             edgecolor='darkred', linewidth=1.5)\n",
        "    ax.add_patch(spec_box)\n",
        "    ax.text(x + 0.9, y + 0.75, name, ha='center', va='center', \n",
        "            fontsize=10, fontweight='bold')\n",
        "    \n",
        "    # Setas do modelo base para especializados\n",
        "    ax.arrow(3, 7, x + 0.9 - 3, y + 1.5 - 7, \n",
        "             head_width=0.1, head_length=0.1, \n",
        "             fc='gray', ec='gray', alpha=0.7)\n",
        "\n",
        "# T√≠tulo e labels\n",
        "ax.text(4, 9.5, 'üé≠ Evolu√ß√£o dos Modelos: Do Geral ao Espec√≠fico', \n",
        "        ha='center', va='center', fontsize=16, fontweight='bold')\n",
        "\n",
        "ax.text(3, 6, 'Fine-tuning / Especializa√ß√£o', ha='center', va='center', \n",
        "        fontsize=12, style='italic', color='red')\n",
        "\n",
        "ax.set_xlim(0, 8)\n",
        "ax.set_ylim(3, 10)\n",
        "ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Liiindo! Viu como funciona a evolu√ß√£o?\")\n",
        "print(\"Base ‚Üí Especializa√ß√£o = Padeiro ‚Üí Confeiteiro!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèõÔ∏è Arquiteturas: Decoder-Only, Encoder-Only e Encoder-Decoder\n\nLembra do M√≥dulo 3 sobre Transformers? Agora vamos ver como as diferentes partes s√£o usadas!\n\nPensa no Transformer como uma f√°brica com duas se√ß√µes:\n- **Encoder**: A se√ß√£o que \"entende\" (l√™ e processa)\n- **Decoder**: A se√ß√£o que \"produz\" (gera e cria)\n\n### üîç Encoder-Only (S√≥ Entendimento)\n\n**O que faz**: Especialista em entender e analisar texto\n**Como funciona**: L√™ todo o texto de uma vez, cria representa√ß√µes ricas\n**Melhor para**: Classifica√ß√£o, an√°lise de sentimento, Q&A\n\n**Exemplo cl√°ssico**: BERT\n- L√™ \"O filme foi _____ bom\" \n- Entende o contexto todo\n- Preenche a lacuna ou classifica sentimento\n\n### üéØ Decoder-Only (S√≥ Gera√ß√£o)\n\n**O que faz**: Especialista em gerar texto novo\n**Como funciona**: L√™ palavra por palavra, gera a pr√≥xima\n**Melhor para**: Gera√ß√£o de texto, conversa√ß√£o, cria√ß√£o\n\n**Exemplo cl√°ssico**: GPT fam√≠lia toda\n- L√™ \"Era uma vez\"\n- Gera \"uma princesa que vivia em um castelo...\"\n\n### üîÑ Encoder-Decoder (Transforma√ß√£o)\n\n**O que faz**: Especialista em transformar um tipo de texto em outro\n**Como funciona**: Encoder entende, Decoder produz algo diferente\n**Melhor para**: Tradu√ß√£o, sumariza√ß√£o, convers√£o\n\n**Exemplo cl√°ssico**: T5, mT5\n- Encoder l√™: \"Hello world\" (ingl√™s)\n- Decoder gera: \"Ol√° mundo\" (portugu√™s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos simular como cada arquitetura processa texto!\n",
        "\n",
        "def simular_encoder_only(texto):\n",
        "    \"\"\"Simula um modelo Encoder-Only (como BERT)\"\"\"\n",
        "    tokens = texto.split()\n",
        "    print(f\"üîç ENCODER-ONLY processando: '{texto}'\")\n",
        "    print(\"üìä An√°lise:\")\n",
        "    \n",
        "    # Simula an√°lise de sentimento\n",
        "    palavras_positivas = ['bom', '√≥timo', 'excelente', 'incr√≠vel', 'maravilhoso']\n",
        "    palavras_negativas = ['ruim', 'p√©ssimo', 'terr√≠vel', 'horr√≠vel', 'detesto']\n",
        "    \n",
        "    score = 0\n",
        "    for palavra in tokens:\n",
        "        if palavra.lower() in palavras_positivas:\n",
        "            score += 1\n",
        "        elif palavra.lower() in palavras_negativas:\n",
        "            score -= 1\n",
        "    \n",
        "    if score > 0:\n",
        "        sentimento = \"üòä POSITIVO\"\n",
        "    elif score < 0:\n",
        "        sentimento = \"üòû NEGATIVO\"\n",
        "    else:\n",
        "        sentimento = \"üòê NEUTRO\"\n",
        "    \n",
        "    print(f\"   Sentimento: {sentimento}\")\n",
        "    print(f\"   Confian√ßa: {abs(score * 20)}%\")\n",
        "    return sentimento\n",
        "\n",
        "def simular_decoder_only(prompt):\n",
        "    \"\"\"Simula um modelo Decoder-Only (como GPT)\"\"\"\n",
        "    print(f\"üéØ DECODER-ONLY gerando a partir de: '{prompt}'\")\n",
        "    \n",
        "    # Simula gera√ß√£o baseada no prompt\n",
        "    continuacoes = {\n",
        "        \"era uma vez\": \"uma princesa que vivia em um reino distante...\",\n",
        "        \"python √©\": \"uma linguagem de programa√ß√£o poderosa e vers√°til...\",\n",
        "        \"intelig√™ncia artificial\": \"√© uma √°rea da computa√ß√£o que busca criar sistemas inteligentes...\",\n",
        "        \"o futuro da tecnologia\": \"ser√° moldado pela integra√ß√£o entre IA, IoT e computa√ß√£o qu√¢ntica...\"\n",
        "    }\n",
        "    \n",
        "    prompt_lower = prompt.lower()\n",
        "    continuacao = \"ser√° algo incr√≠vel e transformador para a humanidade!\"\n",
        "    \n",
        "    for key, value in continuacoes.items():\n",
        "        if key in prompt_lower:\n",
        "            continuacao = value\n",
        "            break\n",
        "    \n",
        "    print(f\"‚ú® Gera√ß√£o: {continuacao}\")\n",
        "    return continuacao\n",
        "\n",
        "def simular_encoder_decoder(texto_origem, tarefa):\n",
        "    \"\"\"Simula um modelo Encoder-Decoder (como T5)\"\"\"\n",
        "    print(f\"üîÑ ENCODER-DECODER - Tarefa: {tarefa}\")\n",
        "    print(f\"üì• Entrada: '{texto_origem}'\")\n",
        "    \n",
        "    if tarefa == \"tradu√ß√£o\":\n",
        "        traducoes = {\n",
        "            \"hello world\": \"ol√° mundo\",\n",
        "            \"good morning\": \"bom dia\",\n",
        "            \"artificial intelligence\": \"intelig√™ncia artificial\",\n",
        "            \"machine learning\": \"aprendizado de m√°quina\"\n",
        "        }\n",
        "        resultado = traducoes.get(texto_origem.lower(), \"tradu√ß√£o n√£o encontrada\")\n",
        "    \n",
        "    elif tarefa == \"sumariza√ß√£o\":\n",
        "        if len(texto_origem.split()) > 5:\n",
        "            resultado = \" \".join(texto_origem.split()[:5]) + \"...\"\n",
        "        else:\n",
        "            resultado = \"Texto j√° est√° resumido\"\n",
        "    \n",
        "    else:\n",
        "        resultado = \"Tarefa n√£o reconhecida\"\n",
        "    \n",
        "    print(f\"üì§ Resultado: '{resultado}'\")\n",
        "    return resultado\n",
        "\n",
        "# Testando as tr√™s arquiteturas!\n",
        "print(\"üß™ LABORAT√ìRIO DAS ARQUITETURAS\\n\")\n",
        "\n",
        "# Teste 1: Encoder-Only\n",
        "simular_encoder_only(\"Este filme foi realmente incr√≠vel\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Teste 2: Decoder-Only\n",
        "simular_decoder_only(\"Python √©\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Teste 3: Encoder-Decoder\n",
        "simular_encoder_decoder(\"hello world\", \"tradu√ß√£o\")\n",
        "print(\"\\nüé≠ Cada arquitetura tem sua especialidade!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\n",
        "graph TD\n",
        "    A[Texto de Entrada] --> B{Que tipo de tarefa?}\n",
        "    \n",
        "    B -->|Entender/Classificar| C[Encoder-Only]\n",
        "    B -->|Gerar/Criar| D[Decoder-Only] \n",
        "    B -->|Transformar/Traduzir| E[Encoder-Decoder]\n",
        "    \n",
        "    C --> F[\"üîç An√°lise<br/>Sentimento<br/>Classifica√ß√£o<br/>Q&A\"]\n",
        "    D --> G[\"üéØ Gera√ß√£o<br/>Chat<br/>Continua√ß√£o<br/>Cria√ß√£o\"]\n",
        "    E --> H[\"üîÑ Transforma√ß√£o<br/>Tradu√ß√£o<br/>Sumariza√ß√£o<br/>Convers√£o\"]\n",
        "    \n",
        "    F --> I[BERT, RoBERTa]\n",
        "    G --> J[GPT, LLaMA]\n",
        "    H --> K[T5, BART]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma compara√ß√£o visual das arquiteturas!\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Cores para cada arquitetura\n",
        "cores = ['lightblue', 'lightgreen', 'lightcoral']\n",
        "arquiteturas = ['Encoder-Only', 'Decoder-Only', 'Encoder-Decoder']\n",
        "exemplos = ['BERT\\nRoBERTa\\nDeBERTa', 'GPT-3/4\\nLLaMA\\nPaLM', 'T5\\nBART\\nmT5']\n",
        "tarefas = ['‚Ä¢ Classifica√ß√£o\\n‚Ä¢ An√°lise Sentimento\\n‚Ä¢ Q&A\\n‚Ä¢ NER', \n",
        "          '‚Ä¢ Gera√ß√£o de Texto\\n‚Ä¢ Chat\\n‚Ä¢ Completar\\n‚Ä¢ Cria√ß√£o',\n",
        "          '‚Ä¢ Tradu√ß√£o\\n‚Ä¢ Sumariza√ß√£o\\n‚Ä¢ Par√°frase\\n‚Ä¢ Convers√£o']\n",
        "\n",
        "for i, (arq, ex, tar, cor) in enumerate(zip(arquiteturas, exemplos, tarefas, cores)):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Fundo colorido\n",
        "    ax.add_patch(Rectangle((0, 0), 1, 1, facecolor=cor, alpha=0.3))\n",
        "    \n",
        "    # T√≠tulo da arquitetura\n",
        "    ax.text(0.5, 0.9, arq, ha='center', va='center', \n",
        "            fontsize=14, fontweight='bold', transform=ax.transAxes)\n",
        "    \n",
        "    # Exemplos de modelos\n",
        "    ax.text(0.5, 0.7, ex, ha='center', va='center', \n",
        "            fontsize=12, fontweight='bold', color='navy',\n",
        "            transform=ax.transAxes)\n",
        "    \n",
        "    # Tarefas principais\n",
        "    ax.text(0.5, 0.4, tar, ha='center', va='center', \n",
        "            fontsize=10, transform=ax.transAxes)\n",
        "    \n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Borda\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(True)\n",
        "        spine.set_linewidth(2)\n",
        "        spine.set_color('navy')\n",
        "\n",
        "plt.suptitle('üèõÔ∏è As Tr√™s Grandes Arquiteturas dos LLMs', \n",
        "             fontsize=16, fontweight='bold', y=0.95)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Dica do Pedro: Cada arquitetura √© como uma ferramenta espec√≠fica!\")\n",
        "print(\"üîç Encoder-Only = Lupa (para analisar)\")\n",
        "print(\"üéØ Decoder-Only = Caneta (para criar)\")\n",
        "print(\"üîÑ Encoder-Decoder = Tradutor (para transformar)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Modelos Generativos vs. Discriminativos\n\nT√°, agora vamos falar de uma diferen√ßa fundamental! √â como a diferen√ßa entre um artista e um cr√≠tico de arte:\n\n### üé® Modelos Generativos\n\n**O que fazem**: Criam conte√∫do novo do zero\n**Como pensam**: \"Vou criar algo baseado no que aprendi\"\n**Analogia**: Escritor criando um livro\n\n**Caracter√≠sticas**:\n- Aprendem a distribui√ß√£o dos dados\n- Conseguem gerar amostras novas\n- Podem ser criativos e surpreendentes\n- Mais complexos computacionalmente\n\n**Exemplos**: GPT-4, DALL-E, Stable Diffusion\n\n### üîç Modelos Discriminativos\n\n**O que fazem**: Classificam e categorizam conte√∫do existente\n**Como pensam**: \"Vou analisar e decidir em qual categoria isso se encaixa\"\n**Analogia**: Cr√≠tico avaliando uma obra\n\n**Caracter√≠sticas**:\n- Aprendem fronteiras entre classes\n- Focam em distin√ß√£o e classifica√ß√£o\n- Mais eficientes para tarefas espec√≠ficas\n- Geralmente mais r√°pidos\n\n**Exemplos**: BERT para classifica√ß√£o, modelos de sentiment analysis\n\n**üéØ Dica do Pedro**: Generativo = \"Vou criar!\", Discriminativo = \"Vou julgar!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos simular a diferen√ßa entre modelos generativos e discriminativos\n",
        "import random\n",
        "\n",
        "class ModeloGenerativo:\n",
        "    \"\"\"Simula um modelo generativo simples\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # \"Aprendeu\" padr√µes de diferentes tipos de texto\n",
        "        self.padroes = {\n",
        "            'poesia': ['verso', 'rima', 'alma', 'cora√ß√£o', 'lua', 'estrela'],\n",
        "            'tecnologia': ['algoritmo', 'dados', 'rede', 'sistema', 'c√≥digo', 'digital'],\n",
        "            'culinaria': ['receita', 'ingrediente', 'sabor', 'tempero', 'cozinha', 'prato']\n",
        "        }\n",
        "    \n",
        "    def gerar_texto(self, categoria, tamanho=5):\n",
        "        \"\"\"Gera texto novo baseado nos padr√µes aprendidos\"\"\"\n",
        "        palavras = self.padroes.get(categoria, ['palavra', 'gen√©rica'])\n",
        "        texto_gerado = []\n",
        "        \n",
        "        for _ in range(tamanho):\n",
        "            palavra = random.choice(palavras)\n",
        "            texto_gerado.append(palavra)\n",
        "        \n",
        "        return ' '.join(texto_gerado)\n",
        "\n",
        "class ModeloDiscriminativo:\n",
        "    \"\"\"Simula um modelo discriminativo simples\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # \"Aprendeu\" a distinguir entre categorias\n",
        "        self.palavras_chave = {\n",
        "            'poesia': ['verso', 'rima', 'alma', 'cora√ß√£o', 'lua', 'estrela'],\n",
        "            'tecnologia': ['algoritmo', 'dados', 'rede', 'sistema', 'c√≥digo', 'digital'],\n",
        "            'culinaria': ['receita', 'ingrediente', 'sabor', 'tempero', 'cozinha', 'prato']\n",
        "        }\n",
        "    \n",
        "    def classificar_texto(self, texto):\n",
        "        \"\"\"Classifica texto em uma das categorias\"\"\"\n",
        "        palavras = texto.lower().split()\n",
        "        pontuacoes = {}\n",
        "        \n",
        "        for categoria, chaves in self.palavras_chave.items():\n",
        "            pontuacao = sum(1 for palavra in palavras if palavra in chaves)\n",
        "            pontuacoes[categoria] = pontuacao\n",
        "        \n",
        "        categoria_predita = max(pontuacoes, key=pontuacoes.get)\n",
        "        confianca = pontuacoes[categoria_predita] / len(palavras) * 100\n",
        "        \n",
        "        return categoria_predita, confianca\n",
        "\n",
        "# Testando os dois tipos!\n",
        "print(\"üß™ LABORAT√ìRIO: GENERATIVO vs DISCRIMINATIVO\\n\")\n",
        "\n",
        "# Criando os modelos\n",
        "gerador = ModeloGenerativo()\n",
        "classificador = ModeloDiscriminativo()\n",
        "\n",
        "print(\"üé® MODELO GENERATIVO em a√ß√£o:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "for categoria in ['poesia', 'tecnologia', 'culinaria']:\n",
        "    texto_gerado = gerador.gerar_texto(categoria)\n",
        "    print(f\"üìù Gerando {categoria}: '{texto_gerado}'\")\n",
        "\n",
        "print(\"\\nüîç MODELO DISCRIMINATIVO em a√ß√£o:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "textos_teste = [\n",
        "    \"o algoritmo processa dados na rede\",\n",
        "    \"a lua brilha no verso da alma\",\n",
        "    \"a receita leva tempero e sabor\"\n",
        "]\n",
        "\n",
        "for texto in textos_teste:\n",
        "    categoria, confianca = classificador.classificar_texto(texto)\n",
        "    print(f\"üìä Texto: '{texto}'\")\n",
        "    print(f\"    Categoria: {categoria} (confian√ßa: {confianca:.1f}%)\\n\")\n",
        "\n",
        "print(\"‚ú® Resumo:\")\n",
        "print(\"üé® Generativo: CRIA conte√∫do novo\")\n",
        "print(\"üîç Discriminativo: ANALISA conte√∫do existente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Modelos por Tamanho e Capacidade\n\nAgora vamos falar de tamanho! E n√£o, n√£o √© s√≥ \"quanto maior, melhor\". √â mais como escolher o carro certo para cada situa√ß√£o:\n\n### üöó Modelos Pequenos (< 1B par√¢metros)\n**Analogia**: Carro compacto - econ√¥mico e eficiente para o dia a dia\n- **Vantagens**: R√°pidos, baratos, rodam localmente\n- **Desvantagens**: Menos \"inteligentes\", conhecimento limitado\n- **Exemplos**: DistilBERT, TinyBERT, modelos mobile\n\n### üöô Modelos M√©dios (1B - 10B par√¢metros)\n**Analogia**: SUV - bom equil√≠brio entre performance e praticidade\n- **Vantagens**: Boa performance, custo razo√°vel\n- **Desvantagens**: Ainda limitados para tarefas complexas\n- **Exemplos**: BERT-base, GPT-2, alguns modelos LLaMA\n\n### üöõ Modelos Grandes (10B - 100B par√¢metros)\n**Analogia**: Caminh√£o - potente para trabalhos pesados\n- **Vantagens**: Muito capaz, conhecimento amplo\n- **Desvantagens**: Caro, lento, precisa de infraestrutura\n- **Exemplos**: GPT-3, PaLM, alguns LLaMA\n\n### üöÄ Modelos Gigantes (> 100B par√¢metros)\n**Analogia**: Foguete - para miss√µes imposs√≠veis\n- **Vantagens**: Estado da arte, capacidades emergentes\n- **Desvantagens**: Extremamente caro e complexo\n- **Exemplos**: GPT-4, PaLM-2, Gemini Ultra\n\n**üéØ Dica do Pedro**: O segredo √© escolher o tamanho certo para sua tarefa! N√£o precisa de foguete para ir ao mercado!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma visualiza√ß√£o dos tamanhos e custos!\n",
        "import numpy as np\n",
        "\n",
        "# Dados dos modelos (aproximados)\n",
        "modelos = {\n",
        "    'DistilBERT': {'params': 0.066, 'performance': 75, 'custo': 1, 'velocidade': 95},\n",
        "    'BERT-base': {'params': 0.11, 'performance': 80, 'custo': 2, 'velocidade': 85},\n",
        "    'GPT-2': {'params': 1.5, 'performance': 82, 'custo': 5, 'velocidade': 75},\n",
        "    'LLaMA-7B': {'params': 7, 'performance': 85, 'custo': 15, 'velocidade': 60},\n",
        "    'GPT-3': {'params': 175, 'performance': 90, 'custo': 100, 'velocidade': 30},\n",
        "    'GPT-4': {'params': 1000, 'performance': 95, 'custo': 500, 'velocidade': 15}\n",
        "}\n",
        "\n",
        "# Extraindo dados para visualiza√ß√£o\n",
        "nomes = list(modelos.keys())\n",
        "params = [modelos[nome]['params'] for nome in nomes]\n",
        "performance = [modelos[nome]['performance'] for nome in nomes]\n",
        "custo = [modelos[nome]['custo'] for nome in nomes]\n",
        "velocidade = [modelos[nome]['velocidade'] for nome in nomes]\n",
        "\n",
        "# Criando visualiza√ß√£o com m√∫ltiplos subplots\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Par√¢metros vs Performance\n",
        "scatter = ax1.scatter(params, performance, s=[c*5 for c in custo], \n",
        "                    c=velocidade, cmap='RdYlGn', alpha=0.7)\n",
        "ax1.set_xscale('log')\n",
        "ax1.set_xlabel('Par√¢metros (B)')\n",
        "ax1.set_ylabel('Performance (%)')\n",
        "ax1.set_title('üéØ Par√¢metros vs Performance')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Adicionando labels\n",
        "for i, nome in enumerate(nomes):\n",
        "    ax1.annotate(nome, (params[i], performance[i]), \n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "# 2. Custo vs Performance\n",
        "ax2.bar(nomes, custo, color=['lightblue' if c < 10 else 'orange' if c < 100 else 'red' \n",
        "                            for c in custo])\n",
        "ax2.set_yscale('log')\n",
        "ax2.set_ylabel('Custo Relativo')\n",
        "ax2.set_title('üí∞ Custo por Modelo')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. Velocidade vs Tamanho\n",
        "ax3.plot(params, velocidade, 'ro-', linewidth=2, markersize=8)\n",
        "ax3.set_xscale('log')\n",
        "ax3.set_xlabel('Par√¢metros (B)')\n",
        "ax3.set_ylabel('Velocidade Relativa')\n",
        "ax3.set_title('‚ö° Velocidade vs Tamanho')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Radar chart das caracter√≠sticas\n",
        "# Escolhendo 3 modelos representativos\n",
        "modelos_radar = ['DistilBERT', 'LLaMA-7B', 'GPT-4']\n",
        "categorias = ['Performance', 'Velocidade', 'Efici√™ncia\\n(inverso do custo)']\n",
        "\n",
        "angles = np.linspace(0, 2 * np.pi, len(categorias), endpoint=False).tolist()\n",
        "angles += angles[:1]  # Fechando o c√≠rculo\n",
        "\n",
        "ax4 = plt.subplot(2, 2, 4, projection='polar')\n",
        "\n",
        "for modelo in modelos_radar:\n",
        "    valores = [\n",
        "        modelos[modelo]['performance'],\n",
        "        modelos[modelo]['velocidade'],\n",
        "        100 - modelos[modelo]['custo']  # Inverso do custo = efici√™ncia\n",
        "    ]\n",
        "    valores += valores[:1]  # Fechando o c√≠rculo\n",
        "    \n",
        "    ax4.plot(angles, valores, 'o-', linewidth=2, label=modelo)\n",
        "    ax4.fill(angles, valores, alpha=0.1)\n",
        "\n",
        "ax4.set_xticks(angles[:-1])\n",
        "ax4.set_xticklabels(categorias)\n",
        "ax4.set_ylim(0, 100)\n",
        "ax4.set_title('üìä Compara√ß√£o Multi-dimensional')\n",
        "ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Insights importantes:\")\n",
        "print(\"üìà Mais par√¢metros = Melhor performance (mas nem sempre vale a pena!)\")\n",
        "print(\"üí∞ Custo cresce exponencialmente com o tamanho\")\n",
        "print(\"‚ö° Velocidade diminui drasticamente com mais par√¢metros\")\n",
        "print(\"üé™ Cada modelo tem seu 'sweet spot' para diferentes usos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Fine-tuning e Especializa√ß√£o\n\nAgora vamos falar do processo que transforma um modelo \"gen√©rico\" em um especialista! √â como pegar um m√©dico rec√©m-formado e fazer ele se especializar em cardiologia.\n\n### O que √© Fine-tuning?\n\n√â pegar um modelo j√° treinado (que custou milh√µes para treinar) e dar um \"cursinho\" espec√≠fico para ele ficar expert em algo.\n\n**Processo**:\n1. **Modelo Base**: J√° sabe linguagem em geral\n2. **Dataset Espec√≠fico**: Dados da tarefa que queremos\n3. **Treinamento Adicional**: Algumas √©pocas de ajuste fino\n4. **Modelo Especializado**: Expert na nova tarefa!\n\n### Tipos de Fine-tuning\n\n#### üéØ **Instruction Tuning**\n- **O que √©**: Ensina o modelo a seguir instru√ß√µes\n- **Dataset**: Pares de (instru√ß√£o, resposta esperada)\n- **Resultado**: ChatGPT, Claude, Bard\n- **Exemplo**: \"Resuma este texto\" ‚Üí modelo aprende a resumir\n\n#### üèÜ **RLHF (Reinforcement Learning from Human Feedback)**\n- **O que √©**: Usa feedback humano para melhorar respostas\n- **Processo**: Humanos rankeiam respostas, modelo aprende o que √© \"melhor\"\n- **Resultado**: Modelos mais alinhados e √∫teis\n- **Exemplo**: GPT-4 usa muito RLHF\n\n#### üé® **Domain-Specific Fine-tuning**\n- **O que √©**: Especializa√ß√£o em uma √°rea espec√≠fica\n- **Exemplos**: \n  - BioBERT (medicina)\n  - FinBERT (finan√ßas)\n  - CodeT5 (programa√ß√£o)\n  - LegalBERT (direito)\n\n**üéØ Dica do Pedro**: Fine-tuning √© como personalizar um carro - voc√™ pega a base e adapta para suas necessidades espec√≠ficas!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos simular o processo de fine-tuning!\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ModeloBase:\n",
        "    \"\"\"Simula um modelo base gen√©rico\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.conhecimento_geral = {\n",
        "            'linguagem': 85,\n",
        "            'contexto': 80,\n",
        "            'coerencia': 75\n",
        "        }\n",
        "        self.especializacoes = {}  # Come√ßa sem especializa√ß√µes\n",
        "    \n",
        "    def responder_generico(self, pergunta):\n",
        "        \"\"\"Resposta gen√©rica sem especializa√ß√£o\"\"\"\n",
        "        respostas_genericas = [\n",
        "            \"Essa √© uma pergunta interessante sobre {}\",\n",
        "            \"Posso tentar ajudar com informa√ß√µes gerais sobre {}\",\n",
        "            \"Com base no meu conhecimento geral, {} √© um t√≥pico complexo\"\n",
        "        ]\n",
        "        return random.choice(respostas_genericas).format(pergunta)\n",
        "\n",
        "class ModeloEspecializado(ModeloBase):\n",
        "    \"\"\"Simula um modelo ap√≥s fine-tuning\"\"\"\n",
        "    \n",
        "    def __init__(self, especializacao):\n",
        "        super().__init__()\n",
        "        self.especializacao = especializacao\n",
        "        self.fazer_fine_tuning()\n",
        "    \n",
        "    def fazer_fine_tuning(self):\n",
        "        \"\"\"Simula o processo de fine-tuning\"\"\"\n",
        "        print(f\"üîß Iniciando fine-tuning para {self.especializacao}...\")\n",
        "        \n",
        "        # Define conhecimentos espec√≠ficos por √°rea\n",
        "        especializacoes_data = {\n",
        "            'medicina': {\n",
        "                'diagnostico': 90,\n",
        "                'sintomas': 88,\n",
        "                'tratamentos': 85,\n",
        "                'respostas': {\n",
        "                    'dor de cabe√ßa': 'Pode ser tensional, enxaqueca ou sinusite. Recomendo avalia√ß√£o m√©dica.',\n",
        "                    'febre': 'Sinal de infec√ß√£o. Hidrata√ß√£o e antipir√©tico podem ajudar.',\n",
        "                    'diabetes': 'Doen√ßa metab√≥lica que requer controle glic√™mico e acompanhamento.'\n",
        "                }\n",
        "            },\n",
        "            'programacao': {\n",
        "                'sintaxe': 95,\n",
        "                'algoritmos': 90,\n",
        "                'debugging': 87,\n",
        "                'respostas': {\n",
        "                    'python': 'Linguagem vers√°til, ideal para IA, web e automa√ß√£o.',\n",
        "                    'bug': 'Use print statements, debugger ou testes unit√°rios para identificar.',\n",
        "                    'algoritmo': 'Sequ√™ncia de passos para resolver um problema computacional.'\n",
        "                }\n",
        "            },\n",
        "            'culinaria': {\n",
        "                'receitas': 92,\n",
        "                'ingredientes': 89,\n",
        "                'tecnicas': 86,\n",
        "                'respostas': {\n",
        "                    'bolo': 'Misture ingredientes secos, adicione l√≠quidos, asse a 180¬∞C.',\n",
        "                    'tempero': 'Sal, pimenta, alho e cebola s√£o bases universais.',\n",
        "                    'massa': 'Farinha, ovos e √°gua. Sove bem e deixe descansar.'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        if self.especializacao in especializacoes_data:\n",
        "            self.especializacoes = especializacoes_data[self.especializacao]\n",
        "            print(f\"‚úÖ Fine-tuning conclu√≠do! Especializa√ß√£o em {self.especializacao}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Especializa√ß√£o {self.especializacao} n√£o dispon√≠vel\")\n",
        "    \n",
        "    def responder_especializado(self, pergunta):\n",
        "        \"\"\"Resposta especializada ap√≥s fine-tuning\"\"\"\n",
        "        pergunta_lower = pergunta.lower()\n",
        "        \n",
        "        # Procura por palavras-chave especializadas\n",
        "        if 'respostas' in self.especializacoes:\n",
        "            for keyword, resposta in self.especializacoes['respostas'].items():\n",
        "                if keyword in pergunta_lower:\n",
        "                    return f\"[ESPECIALISTA {self.especializacao.upper()}]: {resposta}\"\n",
        "        \n",
        "        # Se n√£o encontrou, usa conhecimento geral melhorado\n",
        "        return f\"[{self.especializacao.upper()}]: Com minha especializa√ß√£o, posso dizer que {pergunta} √© um t√≥pico que requer an√°lise detalhada.\"\n",
        "\n",
        "# Demonstra√ß√£o do fine-tuning\n",
        "print(\"üß™ LABORAT√ìRIO: ANTES vs DEPOIS DO FINE-TUNING\\n\")\n",
        "\n",
        "# Modelo base (gen√©rico)\n",
        "modelo_base = ModeloBase()\n",
        "print(\"ü§ñ MODELO BASE (sem especializa√ß√£o):\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "perguntas_teste = ['diabetes', 'python', 'bolo']\n",
        "for pergunta in perguntas_teste:\n",
        "    resposta = modelo_base.responder_generico(pergunta)\n",
        "    print(f\"‚ùì {pergunta}: {resposta}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# Modelos especializados\n",
        "especializacoes = ['medicina', 'programacao', 'culinaria']\n",
        "perguntas_especializada = ['diabetes', 'python', 'bolo']\n",
        "\n",
        "for i, (spec, pergunta) in enumerate(zip(especializacoes, perguntas_especializada)):\n",
        "    print(f\"üéØ MODELO ESPECIALIZADO ({spec.upper()}):\")\n",
        "    modelo_especializado = ModeloEspecializado(spec)\n",
        "    resposta = modelo_especializado.responder_especializado(pergunta)\n",
        "    print(f\"‚ùì {pergunta}: {resposta}\\n\")\n",
        "\n",
        "print(\"‚ú® Resumo:\")\n",
        "print(\"ü§ñ Base: Conhecimento amplo mas superficial\")\n",
        "print(\"üéØ Especializado: Conhecimento focado e profundo\")\n",
        "print(\"üîß Fine-tuning = Transformar generalista em especialista!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\n",
        "graph LR\n",
        "    A[Modelo Base<br/>GPT-3] --> B[Dataset<br/>Especializado]\n",
        "    B --> C[Fine-tuning<br/>Process]\n",
        "    C --> D[Modelo<br/>Especializado]\n",
        "    \n",
        "    B1[\"üìö Instruction<br/>Dataset\"] --> C\n",
        "    B2[\"üë®‚Äç‚öïÔ∏è Medical<br/>Dataset\"] --> C\n",
        "    B3[\"üíª Code<br/>Dataset\"] --> C\n",
        "    \n",
        "    D --> E1[\"ü§ñ ChatGPT<br/>(Instructions)\"]\n",
        "    D --> E2[\"‚öïÔ∏è BioBERT<br/>(Medicine)\"]\n",
        "    D --> E3[\"üíª CodeT5<br/>(Programming)\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos visualizar o impacto do fine-tuning na performance!\n",
        "import numpy as np\n",
        "\n",
        "# Dados simulados de performance antes e depois do fine-tuning\n",
        "tarefas = ['Classifica√ß√£o\\nSentimento', 'Gera√ß√£o\\nC√≥digo', 'Diagn√≥stico\\nM√©dico', \n",
        "          'Tradu√ß√£o', 'Sumariza√ß√£o', 'Q&A Espec√≠fico']\n",
        "\n",
        "# Performance do modelo base (gen√©rico)\n",
        "perf_base = [65, 45, 40, 70, 60, 50]\n",
        "\n",
        "# Performance ap√≥s fine-tuning espec√≠fico\n",
        "perf_finetuned = [88, 85, 82, 90, 85, 87]\n",
        "\n",
        "# Criando visualiza√ß√£o comparativa\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Gr√°fico 1: Compara√ß√£o lado a lado\n",
        "x = np.arange(len(tarefas))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, perf_base, width, label='Modelo Base', \n",
        "               color='lightcoral', alpha=0.8)\n",
        "bars2 = ax1.bar(x + width/2, perf_finetuned, width, label='Ap√≥s Fine-tuning', \n",
        "               color='lightgreen', alpha=0.8)\n",
        "\n",
        "ax1.set_xlabel('Tarefas')\n",
        "ax1.set_ylabel('Performance (%)')\n",
        "ax1.set_title('üéØ Impacto do Fine-tuning na Performance')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(tarefas, rotation=45, ha='right')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "             f'{height}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "             f'{height}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Gr√°fico 2: Ganho percentual\n",
        "ganho = [(f - b) / b * 100 for f, b in zip(perf_finetuned, perf_base)]\n",
        "cores_ganho = ['green' if g > 50 else 'orange' if g > 25 else 'red' for g in ganho]\n",
        "\n",
        "bars3 = ax2.bar(tarefas, ganho, color=cores_ganho, alpha=0.7)\n",
        "ax2.set_xlabel('Tarefas')\n",
        "ax2.set_ylabel('Ganho de Performance (%)')\n",
        "ax2.set_title('üìà Ganho Relativo com Fine-tuning')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Linha de refer√™ncia\n",
        "ax2.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='50% de ganho')\n",
        "ax2.legend()\n",
        "\n",
        "# Adicionando valores\n",
        "for bar, valor in zip(bars3, ganho):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
        "             f'+{valor:.0f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# An√°lise dos resultados\n",
        "print(\"üéØ AN√ÅLISE DOS RESULTADOS:\")\n",
        "print(\"=\" * 40)\n",
        "ganho_medio = np.mean(ganho)\n",
        "melhor_ganho = max(ganho)\n",
        "pior_ganho = min(ganho)\n",
        "\n",
        "print(f\"üìä Ganho m√©dio: {ganho_medio:.1f}%\")\n",
        "print(f\"üèÜ Melhor ganho: {melhor_ganho:.1f}% ({tarefas[ganho.index(melhor_ganho)]})\")\n",
        "print(f\"ü§î Menor ganho: {pior_ganho:.1f}% ({tarefas[ganho.index(pior_ganho)]})\")\n",
        "print(f\"\\n‚ú® Conclus√£o: Fine-tuning pode dobrar a performance em tarefas espec√≠ficas!\")\n",
        "print(f\"üéØ Dica do Pedro: Vale muito a pena especializar quando voc√™ tem uma tarefa espec√≠fica!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèÜ Compara√ß√£o Pr√°tica: Qual Modelo Usar Quando?\n\nT√°, Pedro, mas na pr√°tica, qual modelo eu uso para cada situa√ß√£o? Bora criar um guia pr√°tico!\n\n### üéØ Guia de Escolha por Caso de Uso\n\n#### üìù **Para An√°lise de Texto**\n- **Classifica√ß√£o de Sentimento**: BERT, RoBERTa (Encoder-Only)\n- **Detec√ß√£o de Spam**: Modelos pequenos especializados\n- **An√°lise de T√≥picos**: BERT + clustering\n- **Extra√ß√£o de Entidades**: spaCy + BERT\n\n#### ü§ñ **Para Gera√ß√£o de Conte√∫do**\n- **Chat/Conversa√ß√£o**: GPT-4, Claude, Bard\n- **Cria√ß√£o de Artigos**: GPT-3.5/4 + prompting\n- **Copywriting**: Modelos instruction-tuned\n- **Poesia/Criatividade**: Modelos grandes generativos\n\n#### üîÑ **Para Transforma√ß√£o de Texto**\n- **Tradu√ß√£o**: T5, mT5, modelos espec√≠ficos (Google Translate)\n- **Sumariza√ß√£o**: BART, T5, Pegasus\n- **Par√°frase**: T5 fine-tuned\n- **Corre√ß√£o Gramatical**: T5 + datasets espec√≠ficos\n\n#### üíª **Para C√≥digo**\n- **Gera√ß√£o**: GitHub Copilot, CodeT5, StarCoder\n- **Revis√£o**: CodeBERT especializados\n- **Documenta√ß√£o**: GPT-4 + prompting espec√≠fico\n- **Debug**: Modelos treinados em bugs/fixes\n\n#### üè¢ **Para Dom√≠nios Espec√≠ficos**\n- **Medicina**: BioBERT, ClinicalBERT, Med-PaLM\n- **Finan√ßas**: FinBERT, BloombergGPT\n- **Direito**: LegalBERT, modelos espec√≠ficos\n- **Ci√™ncia**: SciBERT, Galactica\n\n**üéØ Dica do Pedro**: Sempre comece com o mais simples que resolve seu problema!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um sistema de recomenda√ß√£o de modelos!\n",
        "class RecomendadorModelos:\n",
        "    \"\"\"Sistema inteligente para recomendar o melhor modelo para cada caso\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.modelos_db = {\n",
        "            # Modelos para an√°lise\n",
        "            'analise': {\n",
        "                'BERT-base': {\n",
        "                    'tipo': 'Encoder-Only',\n",
        "                    'tamanho': 'M√©dio (110M)',\n",
        "                    'velocidade': 'R√°pida',\n",
        "                    'custo': 'Baixo',\n",
        "                    'casos': ['classifica√ß√£o', 'sentimento', 'qa'],\n",
        "                    'pros': ['Boa performance', 'Eficiente', 'Bem documentado'],\n",
        "                    'contras': ['N√£o gera texto', 'Limitado a 512 tokens']\n",
        "                },\n",
        "                'RoBERTa': {\n",
        "                    'tipo': 'Encoder-Only',\n",
        "                    'tamanho': 'M√©dio (125M)',\n",
        "                    'velocidade': 'R√°pida',\n",
        "                    'custo': 'Baixo',\n",
        "                    'casos': ['classifica√ß√£o', 'an√°lise', 'ner'],\n",
        "                    'pros': ['Melhor que BERT', 'Robusto', 'Estado da arte'],\n",
        "                    'contras': ['Maior que BERT', 'S√≥ an√°lise']\n",
        "                }\n",
        "            },\n",
        "            \n",
        "            # Modelos para gera√ß√£o\n",
        "            'geracao': {\n",
        "                'GPT-3.5-turbo': {\n",
        "                    'tipo': 'Decoder-Only',\n",
        "                    'tamanho': 'Grande (~20B)',\n",
        "                    'velocidade': 'M√©dia',\n",
        "                    'custo': 'M√©dio',\n",
        "                    'casos': ['chat', 'gera√ß√£o', 'criatividade'],\n",
        "                    'pros': ['Vers√°til', 'Boa qualidade', 'API f√°cil'],\n",
        "                    'contras': ['Pago', 'Online only', 'Alucina√ß√µes']\n",
        "                },\n",
        "                'LLaMA-7B': {\n",
        "                    'tipo': 'Decoder-Only',\n",
        "                    'tamanho': 'M√©dio (7B)',\n",
        "                    'velocidade': 'M√©dia',\n",
        "                    'custo': 'M√©dio',\n",
        "                    'casos': ['gera√ß√£o', 'chat', 'fine-tuning'],\n",
        "                    'pros': ['Open source', 'Boa qualidade', 'Customiz√°vel'],\n",
        "                    'contras': ['Precisa GPU', 'Setup complexo']\n",
        "                }\n",
        "            },\n",
        "            \n",
        "            # Modelos para transforma√ß√£o\n",
        "            'transformacao': {\n",
        "                'T5-base': {\n",
        "                    'tipo': 'Encoder-Decoder',\n",
        "                    'tamanho': 'M√©dio (220M)',\n",
        "                    'velocidade': 'M√©dia',\n",
        "                    'custo': 'M√©dio',\n",
        "                    'casos': ['tradu√ß√£o', 'sumariza√ß√£o', 'qa'],\n",
        "                    'pros': ['Vers√°til', 'Text-to-text', 'Bem treinado'],\n",
        "                    'contras': ['Complexo', 'Precisa fine-tuning']\n",
        "                },\n",
        "                'BART': {\n",
        "                    'tipo': 'Encoder-Decoder',\n",
        "                    'tamanho': 'M√©dio (140M)',\n",
        "                    'velocidade': 'R√°pida',\n",
        "                    'custo': 'Baixo',\n",
        "                    'casos': ['sumariza√ß√£o', 'par√°frase'],\n",
        "                    'pros': ['√ìtimo para resumos', 'Eficiente'],\n",
        "                    'contras': ['Limitado em dom√≠nio']\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def recomendar(self, tarefa, orcamento='m√©dio', velocidade_necessaria='m√©dia'):\n",
        "        \"\"\"Recomenda o melhor modelo baseado nos crit√©rios\"\"\"\n",
        "        print(f\"üéØ RECOMENDA√á√ÉO PARA: {tarefa.upper()}\")\n",
        "        print(f\"üí∞ Or√ßamento: {orcamento} | ‚ö° Velocidade: {velocidade_necessaria}\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Mapeia tarefas para categorias\n",
        "        mapa_tarefas = {\n",
        "            'classifica√ß√£o': 'analise',\n",
        "            'sentimento': 'analise',\n",
        "            'ner': 'analise',\n",
        "            'qa': 'analise',\n",
        "            'chat': 'geracao',\n",
        "            'gera√ß√£o': 'geracao',\n",
        "            'criatividade': 'geracao',\n",
        "            'tradu√ß√£o': 'transformacao',\n",
        "            'sumariza√ß√£o': 'transformacao',\n",
        "            'par√°frase': 'transformacao'\n",
        "        }\n",
        "        \n",
        "        categoria = mapa_tarefas.get(tarefa, 'geracao')\n",
        "        modelos_categoria = self.modelos_db[categoria]\n",
        "        \n",
        "        # Filtra modelos baseado nos crit√©rios\n",
        "        recomendacoes = []\n",
        "        \n",
        "        for nome, info in modelos_categoria.items():\n",
        "            if tarefa in info['casos']:\n",
        "                score = self._calcular_score(info, orcamento, velocidade_necessaria)\n",
        "                recomendacoes.append((nome, info, score))\n",
        "        \n",
        "        # Ordena por score\n",
        "        recomendacoes.sort(key=lambda x: x[2], reverse=True)\n",
        "        \n",
        "        # Apresenta as recomenda√ß√µes\n",
        "        for i, (nome, info, score) in enumerate(recomendacoes[:3]):\n",
        "            emoji = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\"\n",
        "            print(f\"{emoji} **{nome}** (Score: {score:.1f}/10)\")\n",
        "            print(f\"   üìä {info['tipo']} | {info['tamanho']} | {info['velocidade']} | {info['custo']}\")\n",
        "            print(f\"   ‚úÖ Pros: {', '.join(info['pros'])}\")\n",
        "            print(f\"   ‚ö†Ô∏è  Contras: {', '.join(info['contras'])}\")\n",
        "            print()\n",
        "        \n",
        "        return recomendacoes[0][0] if recomendacoes else None\n",
        "    \n",
        "    def _calcular_score(self, info, orcamento, velocidade):\n",
        "        \"\"\"Calcula score baseado nos crit√©rios\"\"\"\n",
        "        score = 5.0  # Base\n",
        "        \n",
        "        # Ajuste por or√ßamento\n",
        "        if orcamento == 'baixo' and info['custo'] == 'Baixo':\n",
        "            score += 2\n",
        "        elif orcamento == 'alto' and info['custo'] == 'Alto':\n",
        "            score += 1\n",
        "        elif orcamento != info['custo'].lower():\n",
        "            score -= 1\n",
        "        \n",
        "        # Ajuste por velocidade\n",
        "        if velocidade == 'r√°pida' and info['velocidade'] == 'R√°pida':\n",
        "            score += 2\n",
        "        elif velocidade != info['velocidade'].lower():\n",
        "            score -= 0.5\n",
        "        \n",
        "        return min(10, max(0, score))\n",
        "\n",
        "# Testando o sistema de recomenda√ß√£o!\n",
        "recomendador = RecomendadorModelos()\n",
        "\n",
        "print(\"ü§ñ SISTEMA DE RECOMENDA√á√ÉO DE MODELOS\\n\")\n",
        "\n",
        "# Casos de teste\n",
        "casos_teste = [\n",
        "    ('sentimento', 'baixo', 'r√°pida'),\n",
        "    ('chat', 'm√©dio', 'm√©dia'),\n",
        "    ('sumariza√ß√£o', 'baixo', 'm√©dia')\n",
        "]\n",
        "\n",
        "for tarefa, orcamento, velocidade in casos_teste:\n",
        "    melhor_modelo = recomendador.recomendar(tarefa, orcamento, velocidade)\n",
        "    print(f\"üéØ Melhor escolha: {melhor_modelo}\")\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "print(\"‚ú® Dica do Pedro: Use este sistema como ponto de partida!\")\n",
        "print(\"üß™ Sempre teste na pr√°tica para validar a escolha!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Exerc√≠cio Pr√°tico 1: Classifica√ß√£o de Modelos\n\n**Desafio**: Dado um conjunto de modelos e suas caracter√≠sticas, classifique-os nas categorias corretas!\n\nVou te dar as caracter√≠sticas, voc√™ me diz:\n1. Arquitetura (Encoder-Only, Decoder-Only, Encoder-Decoder)\n2. Tipo (Generativo ou Discriminativo)\n3. Melhor caso de uso\n\n**üéØ Dica do Pedro**: Lembra das caracter√≠sticas que aprendemos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 1: Sistema de Quiz sobre Tipos de Modelos\n",
        "import random\n",
        "\n",
        "class QuizModelos:\n",
        "    def __init__(self):\n",
        "        self.perguntas = [\n",
        "            {\n",
        "                'modelo': 'DistilBERT',\n",
        "                'caracter√≠sticas': ['L√™ texto completo de uma vez', 'Excelente para classifica√ß√£o', 'N√£o gera texto novo', 'Menor que BERT original'],\n",
        "                'arquitetura': 'Encoder-Only',\n",
        "                'tipo': 'Discriminativo',\n",
        "                'uso': 'An√°lise de sentimento'\n",
        "            },\n",
        "            {\n",
        "                'modelo': 'GPT-2',\n",
        "                'caracter√≠sticas': ['Gera uma palavra por vez', 'Excelente para criar hist√≥rias', 'L√™ da esquerda para direita', 'Pode continuar qualquer texto'],\n",
        "                'arquitetura': 'Decoder-Only',\n",
        "                'tipo': 'Generativo',\n",
        "                'uso': 'Gera√ß√£o de texto'\n",
        "            },\n",
        "            {\n",
        "                'modelo': 'T5',\n",
        "                'caracter√≠sticas': ['Transforma um texto em outro', '√ìtimo para tradu√ß√£o', 'Usa formato \"text-to-text\"', 'Tem encoder e decoder'],\n",
        "                'arquitetura': 'Encoder-Decoder',\n",
        "                'tipo': 'Generativo',\n",
        "                'uso': 'Tradu√ß√£o'\n",
        "            },\n",
        "            {\n",
        "                'modelo': 'RoBERTa',\n",
        "                'caracter√≠sticas': ['Vers√£o melhorada do BERT', 'Apenas analisa, n√£o gera', 'Bidirecional', '√ìtimo para Q&A'],\n",
        "                'arquitetura': 'Encoder-Only',\n",
        "                'tipo': 'Discriminativo',\n",
        "                'uso': 'Question Answering'\n",
        "            }\n",
        "        ]\n",
        "        self.pontuacao = 0\n",
        "        self.total_perguntas = 0\n",
        "    \n",
        "    def fazer_pergunta(self, pergunta_data):\n",
        "        print(f\"ü§ñ MODELO: {pergunta_data['modelo']}\")\n",
        "        print(\"üìã CARACTER√çSTICAS:\")\n",
        "        for i, carac in enumerate(pergunta_data['caracter√≠sticas'], 1):\n",
        "            print(f\"   {i}. {carac}\")\n",
        "        print()\n",
        "        \n",
        "        # Pergunta 1: Arquitetura\n",
        "        print(\"‚ùì Qual a arquitetura deste modelo?\")\n",
        "        print(\"   A) Encoder-Only\")\n",
        "        print(\"   B) Decoder-Only\")\n",
        "        print(\"   C) Encoder-Decoder\")\n",
        "        \n",
        "        resposta_arq = input(\"Sua resposta (A/B/C): \").upper().strip()\n",
        "        arquiteturas = {'A': 'Encoder-Only', 'B': 'Decoder-Only', 'C': 'Encoder-Decoder'}\n",
        "        \n",
        "        if arquiteturas.get(resposta_arq) == pergunta_data['arquitetura']:\n",
        "            print(\"‚úÖ Correto!\")\n",
        "            self.pontuacao += 1\n",
        "        else:\n",
        "            print(f\"‚ùå Incorreto. Resposta: {pergunta_data['arquitetura']}\")\n",
        "        \n",
        "        # Pergunta 2: Tipo\n",
        "        print(\"\\n‚ùì Este modelo √©:\")\n",
        "        print(\"   A) Generativo (cria conte√∫do novo)\")\n",
        "        print(\"   B) Discriminativo (classifica/analisa)\")\n",
        "        \n",
        "        resposta_tipo = input(\"Sua resposta (A/B): \").upper().strip()\n",
        "        tipos = {'A': 'Generativo', 'B': 'Discriminativo'}\n",
        "        \n",
        "        if tipos.get(resposta_tipo) == pergunta_data['tipo']:\n",
        "            print(\"‚úÖ Correto!\")\n",
        "            self.pontuacao += 1\n",
        "        else:\n",
        "            print(f\"‚ùå Incorreto. Resposta: {pergunta_data['tipo']}\")\n",
        "        \n",
        "        self.total_perguntas += 2\n",
        "        print(f\"\\nüìä Pontua√ß√£o atual: {self.pontuacao}/{self.total_perguntas}\")\n",
        "        print(\"=\"*50)\n",
        "    \n",
        "    def executar_quiz(self, num_perguntas=2):\n",
        "        print(\"üéØ QUIZ: TIPOS DE MODELOS\")\n",
        "        print(\"Vamos testar seu conhecimento!\\n\")\n",
        "        \n",
        "        perguntas_selecionadas = random.sample(self.perguntas, min(num_perguntas, len(self.perguntas)))\n",
        "        \n",
        "        for pergunta in perguntas_selecionadas:\n",
        "            self.fazer_pergunta(pergunta)\n",
        "            input(\"\\nPressione Enter para continuar...\")\n",
        "            print(\"\\n\")\n",
        "        \n",
        "        # Resultado final\n",
        "        percentual = (self.pontuacao / self.total_perguntas) * 100\n",
        "        print(\"üèÜ RESULTADO FINAL\")\n",
        "        print(f\"üìä Pontua√ß√£o: {self.pontuacao}/{self.total_perguntas} ({percentual:.1f}%)\")\n",
        "        \n",
        "        if percentual >= 80:\n",
        "            print(\"ü•á Excelente! Voc√™ domina os tipos de modelos!\")\n",
        "        elif percentual >= 60:\n",
        "            print(\"ü•à Bom trabalho! Voc√™ entende bem o assunto!\")\n",
        "        else:\n",
        "            print(\"üìö Continue estudando! Releia o material!\")\n",
        "\n",
        "# Executando o quiz (vers√£o autom√°tica para o notebook)\n",
        "print(\"üéØ QUIZ AUTOM√ÅTICO - TIPOS DE MODELOS\\n\")\n",
        "\n",
        "# Simulando respostas para demonstra√ß√£o\n",
        "quiz = QuizModelos()\n",
        "exemplos = [\n",
        "    {\n",
        "        'modelo': 'BERT',\n",
        "        'caracter√≠sticas': ['Bidirecional', 'M√°scara tokens', 'Classifica√ß√£o', 'N√£o gera'],\n",
        "        'resposta_arq': 'Encoder-Only',\n",
        "        'resposta_tipo': 'Discriminativo'\n",
        "    },\n",
        "    {\n",
        "        'modelo': 'GPT-3',\n",
        "        'caracter√≠sticas': ['Autoregressivo', 'Gera texto', 'Uma palavra por vez', 'Criativo'],\n",
        "        'resposta_arq': 'Decoder-Only',\n",
        "        'resposta_tipo': 'Generativo'\n",
        "    }\n",
        "]\n",
        "\n",
        "for exemplo in exemplos:\n",
        "    print(f\"ü§ñ MODELO: {exemplo['modelo']}\")\n",
        "    print(\"üìã CARACTER√çSTICAS:\")\n",
        "    for i, carac in enumerate(exemplo['caracter√≠sticas'], 1):\n",
        "        print(f\"   {i}. {carac}\")\n",
        "    print(f\"\\n‚úÖ Arquitetura: {exemplo['resposta_arq']}\")\n",
        "    print(f\"‚úÖ Tipo: {exemplo['resposta_tipo']}\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"üéØ Agora √© sua vez! Tente classificar outros modelos que voc√™ conhece!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Exerc√≠cio Pr√°tico 2: Cen√°rio de Escolha de Modelo\n\n**Cen√°rio Real**: Voc√™ foi contratado por uma startup brasileira de e-commerce para implementar IA em diferentes partes do sistema.\n\n**Requisitos**:\n1. **An√°lise de Reviews**: Classificar reviews em positivo/negativo\n2. **Chatbot**: Responder d√∫vidas dos clientes\n3. **Tradu√ß√£o**: Traduzir produtos para outros idiomas\n4. **Gera√ß√£o de Descri√ß√µes**: Criar descri√ß√µes autom√°ticas de produtos\n\n**Restri√ß√µes**:\n- Or√ßamento limitado (startup!)\n- Precisa de respostas r√°pidas\n- Dados em portugu√™s\n- Alguns dados sens√≠veis (n√£o pode usar APIs externas)\n\n**Sua miss√£o**: Escolha o modelo ideal para cada caso e justifique!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 2: Solucionando Cen√°rio Real de E-commerce\n",
        "\n",
        "class ConsultorIA:\n",
        "    \"\"\"Simula√ß√£o de um consultor especialista em IA\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.solucoes = {\n",
        "            'reviews': {\n",
        "                'problema': 'Classificar reviews em positivo/negativo',\n",
        "                'requisitos': ['R√°pido', 'Baixo custo', 'Portugu√™s', 'Offline'],\n",
        "                'modelo_recomendado': 'BERT-base multilingual fine-tuned',\n",
        "                'justificativa': [\n",
        "                    '‚úÖ Encoder-Only: Perfeito para classifica√ß√£o',\n",
        "                    '‚úÖ Discriminativo: Foco em an√°lise, n√£o gera√ß√£o',\n",
        "                    '‚úÖ Multilingual: Suporta portugu√™s nativamente',\n",
        "                    '‚úÖ Fine-tuning: Pode especializar em reviews',\n",
        "                    '‚úÖ Offline: Roda localmente, dados seguros',\n",
        "                    '‚úÖ Custo: Relativamente barato de rodar'\n",
        "                ],\n",
        "                'alternativas': ['RoBERTa-PT', 'DistilBERT (mais r√°pido)'],\n",
        "                'implementacao': 'Transformers + PyTorch, fine-tune com reviews brasileiros'\n",
        "            },\n",
        "            \n",
        "            'chatbot': {\n",
        "                'problema': 'Responder d√∫vidas dos clientes',\n",
        "                'requisitos': ['Conversacional', 'Portugu√™s', 'Custo moderado', 'Contextual'],\n",
        "                'modelo_recomendado': 'LLaMA-7B fine-tuned + RAG',\n",
        "                'justificativa': [\n",
        "                    '‚úÖ Decoder-Only: Excelente para conversa√ß√£o',\n",
        "                    '‚úÖ Generativo: Cria respostas naturais',\n",
        "                    '‚úÖ 7B: Bom equil√≠brio tamanho/qualidade',\n",
        "                    '‚úÖ Fine-tuning: Especializar em e-commerce',\n",
        "                    '‚úÖ RAG: Conecta com base de conhecimento',\n",
        "                    '‚úÖ Open-source: Sem custos de API'\n",
        "                ],\n",
        "                'alternativas': ['GPT-3.5 (se budget permitir)', 'Alpaca-7B'],\n",
        "                'implementacao': 'LangChain + Chroma DB para RAG, fine-tune com conversas'\n",
        "            },\n",
        "            \n",
        "            'traducao': {\n",
        "                'problema': 'Traduzir produtos para outros idiomas',\n",
        "                'requisitos': ['Transforma√ß√£o texto', 'M√∫ltiplos idiomas', 'Qualidade alta'],\n",
        "                'modelo_recomendado': 'mT5 ou MarianMT espec√≠fico',\n",
        "                'justificativa': [\n",
        "                    '‚úÖ Encoder-Decoder: Ideal para tradu√ß√£o',\n",
        "                    '‚úÖ Multilingual: Suporta v√°rios idiomas',\n",
        "                    '‚úÖ Especializado: Foco em tradu√ß√£o',\n",
        "                    '‚úÖ Open-source: Sem custos por uso',\n",
        "                    '‚úÖ Fine-tuning: Pode melhorar para e-commerce'\n",
        "                ],\n",
        "                'alternativas': ['Google Translate API (pago)', 'OPUS-MT'],\n",
        "                'implementacao': 'Transformers, modelos espec√≠ficos por par de idiomas'\n",
        "            },\n",
        "            \n",
        "            'descricoes': {\n",
        "                'problema': 'Gerar descri√ß√µes autom√°ticas de produtos',\n",
        "                'requisitos': ['Criativo', 'Persuasivo', 'Portugu√™s', 'Baseado em features'],\n",
        "                'modelo_recomendado': 'GPT-3.5-turbo com prompting ou T5 fine-tuned',\n",
        "                'justificativa': [\n",
        "                    '‚úÖ Generativo: Cria conte√∫do original',\n",
        "                    '‚úÖ Criativo: Textos persuasivos de marketing',\n",
        "                    '‚úÖ Estruturado: Pode seguir templates',\n",
        "                    '‚úÖ Prompting: Controle sobre estilo e tom',\n",
        "                    '‚ö†Ô∏è Custo: GPT-3.5 √© pago, T5 offline'\n",
        "                ],\n",
        "                'alternativas': ['LLaMA fine-tuned', 'T5 + templates'],\n",
        "                'implementacao': 'API OpenAI ou T5 fine-tuned com descri√ß√µes existentes'\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def analisar_caso(self, caso):\n",
        "        \"\"\"Analisa um caso espec√≠fico e apresenta a solu√ß√£o\"\"\"\n",
        "        if caso not in self.solucoes:\n",
        "            print(f\"‚ùå Caso '{caso}' n√£o encontrado!\")\n",
        "            return\n",
        "        \n",
        "        solucao = self.solucoes[caso]\n",
        "        \n",
        "        print(f\"üéØ CASO: {solucao['problema'].upper()}\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        print(\"üìã REQUISITOS:\")\n",
        "        for req in solucao['requisitos']:\n",
        "            print(f\"   ‚Ä¢ {req}\")\n",
        "        \n",
        "        print(f\"\\nüèÜ MODELO RECOMENDADO: {solucao['modelo_recomendado']}\")\n",
        "        \n",
        "        print(\"\\nüí° JUSTIFICATIVA:\")\n",
        "        for just in solucao['justificativa']:\n",
        "            print(f\"   {just}\")\n",
        "        \n",
        "        print(f\"\\nüîÑ ALTERNATIVAS: {', '.join(solucao['alternativas'])}\")\n",
        "        \n",
        "        print(f\"\\nüõ†Ô∏è IMPLEMENTA√á√ÉO: {solucao['implementacao']}\")\n",
        "        \n",
        "        return solucao\n",
        "    \n",
        "    def resumo_arquitetura(self):\n",
        "        \"\"\"Apresenta resumo da arquitetura completa\"\"\"\n",
        "        print(\"üèóÔ∏è ARQUITETURA COMPLETA DA SOLU√á√ÉO\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        arquitetura = {\n",
        "            'üîç Reviews (An√°lise)': 'BERT multilingual ‚Üí Classifica√ß√£o Sentimento',\n",
        "            'ü§ñ Chatbot (Conversa√ß√£o)': 'LLaMA-7B + RAG ‚Üí Respostas Contextuais',\n",
        "            'üåç Tradu√ß√£o (Transforma√ß√£o)': 'mT5/MarianMT ‚Üí M√∫ltiplos Idiomas',\n",
        "            '‚úçÔ∏è Descri√ß√µes (Gera√ß√£o)': 'GPT-3.5 ou T5 ‚Üí Conte√∫do Criativo'\n",
        "        }\n",
        "        \n",
        "        for funcao, solucao in arquitetura.items():\n",
        "            print(f\"{funcao}: {solucao}\")\n",
        "        \n",
        "        print(\"\\nüí∞ ESTIMATIVA DE CUSTOS (mensal):\")\n",
        "        custos = {\n",
        "            'Infrastructure (GPU)': 'R$ 2.000',\n",
        "            'GPT-3.5 API': 'R$ 500-1.500',\n",
        "            'Storage & Compute': 'R$ 300',\n",
        "            'Total estimado': 'R$ 2.800-3.800/m√™s'\n",
        "        }\n",
        "        \n",
        "        for item, valor in custos.items():\n",
        "            print(f\"   {item}: {valor}\")\n",
        "\n",
        "# Executando a an√°lise completa\n",
        "consultor = ConsultorIA()\n",
        "\n",
        "print(\"üöÄ CONSULTORIA IA: E-COMMERCE BRASILEIRO\\n\")\n",
        "\n",
        "casos = ['reviews', 'chatbot', 'traducao', 'descricoes']\n",
        "\n",
        "for caso in casos:\n",
        "    consultor.analisar_caso(caso)\n",
        "    print(\"\\n\" + \"*\"*70 + \"\\n\")\n",
        "\n",
        "consultor.resumo_arquitetura()\n",
        "\n",
        "print(\"\\nüéØ DICAS FINAIS:\")\n",
        "print(\"1. üß™ Sempre fa√ßa POC (Proof of Concept) antes\")\n",
        "print(\"2. üìä Me√ßa performance em dados reais brasileiros\")\n",
        "print(\"3. üîÑ Comece simples, evolua conforme necessidade\")\n",
        "print(\"4. üí∞ Monitore custos constantemente\")\n",
        "print(\"5. üõ°Ô∏è Considere privacidade e LGPD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé≠ Resumo: O Grande Teatro dos LLMs\n\nLiiindo! Chegamos ao final da nossa jornada pelo mundo dos tipos de modelos! üé™\n\n### üéØ O que aprendemos:\n\n#### üèóÔ∏è **Divis√£o Fundamental**\n- **Modelos Base**: O \"padeiro gen√©rico\" que sabe o b√°sico\n- **Modelos Especializados**: O \"confeiteiro expert\" focado em algo espec√≠fico\n\n#### üèõÔ∏è **Arquiteturas (as ferramentas)**\n- **Encoder-Only** üîç: A lupa - para analisar e entender\n- **Decoder-Only** üéØ: A caneta - para criar e gerar\n- **Encoder-Decoder** üîÑ: O tradutor - para transformar\n\n#### üé® **Tipos Fundamentais**\n- **Generativo**: O artista que cria do zero\n- **Discriminativo**: O cr√≠tico que analisa e julga\n\n#### üìè **Tamanhos e Capacidades**\n- **Pequenos**: Carro compacto - econ√¥mico para o dia a dia\n- **M√©dios**: SUV - equil√≠brio entre performance e praticidade\n- **Grandes**: Caminh√£o - potente para trabalhos pesados\n- **Gigantes**: Foguete - para miss√µes imposs√≠veis\n\n#### üîß **Fine-tuning: A Especializa√ß√£o**\n- Transformar generalista em especialista\n- Instruction Tuning, RLHF, Domain-specific\n- Como personalizar um carro para suas necessidades\n\n### üéØ **Dicas de Ouro do Pedro:**\n\n1. **Comece Simples**: Nem sempre precisa do GPT-4!\n2. **Conhe√ßa seu Problema**: An√°lise ‚Üí Encoder, Gera√ß√£o ‚Üí Decoder, Transforma√ß√£o ‚Üí Encoder-Decoder\n3. **Considere Recursos**: Tempo, dinheiro, infraestrutura\n4. **Teste na Pr√°tica**: POC antes de produ√ß√£o\n5. **Evolua Gradualmente**: Comece pequeno, cres√ßa conforme necessidade\n\n### üîÆ **Preparando para o Pr√≥ximo M√≥dulo**\n\nNo **M√≥dulo 7 - Treinamento e Pre-treinamento**, vamos descobrir:\n- Como esses modelos s√£o \"nascidos\" e \"educados\"\n- O processo completo de treinamento\n- Pre-training vs Fine-tuning em detalhes\n- Custos e desafios do treinamento\n\n**üé™ At√© a pr√≥xima, e lembra: cada modelo tem seu palco onde brilhar!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√©lula final - Recapitula√ß√£o visual e pr√≥ximos passos\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.patches import FancyBboxPatch\n",
        "\n",
        "# Criando um mapa mental visual do que aprendemos\n",
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "\n",
        "# Centro - Tipos de Modelos\n",
        "centro = FancyBboxPatch((7, 5.5), 2, 1, boxstyle=\"round,pad=0.1\", \n",
        "                       facecolor='gold', edgecolor='darkred', linewidth=3)\n",
        "ax.add_patch(centro)\n",
        "ax.text(8, 6, 'TIPOS DE\\nMODELOS', ha='center', va='center', \n",
        "        fontsize=14, fontweight='bold')\n",
        "\n",
        "# Categorias principais\n",
        "categorias = [\n",
        "    # (nome, x, y, cor, conte√∫do)\n",
        "    ('ARQUITETURAS', 3, 9, 'lightblue', 'Encoder-Only\\nDecoder-Only\\nEncoder-Decoder'),\n",
        "    ('TAMANHOS', 13, 9, 'lightgreen', 'Pequenos (<1B)\\nM√©dios (1-10B)\\nGrandes (10-100B)\\nGigantes (>100B)'),\n",
        "    ('TIPOS', 3, 3, 'lightcoral', 'Generativo\\n(Cria)\\nDiscriminativo\\n(Analisa)'),\n",
        "    ('ESPECIALIZA√á√ÉO', 13, 3, 'lightyellow', 'Base Models\\nFine-tuned\\nInstruction\\nRLHF'),\n",
        "]\n",
        "\n",
        "for nome, x, y, cor, conteudo in categorias:\n",
        "    # Caixa da categoria\n",
        "    caixa = FancyBboxPatch((x-1, y-1), 2, 2, boxstyle=\"round,pad=0.1\", \n",
        "                          facecolor=cor, edgecolor='navy', linewidth=2)\n",
        "    ax.add_patch(caixa)\n",
        "    \n",
        "    # T√≠tulo da categoria\n",
        "    ax.text(x, y+0.7, nome, ha='center', va='center', \n",
        "            fontsize=11, fontweight='bold')\n",
        "    \n",
        "    # Conte√∫do\n",
        "    ax.text(x, y-0.2, conteudo, ha='center', va='center', \n",
        "            fontsize=9)\n",
        "    \n",
        "    # Seta para o centro\n",
        "    ax.arrow(x, y-1 if y > 6 else y+1, 8-x, (5.5 if y < 6 else 6.5)-y, \n",
        "             head_width=0.1, head_length=0.1, fc='gray', ec='gray', alpha=0.5)\n",
        "\n",
        "# Exemplos pr√°ticos nas laterais\n",
        "exemplos_esq = ['BERT\\n(An√°lise)', 'T5\\n(Tradu√ß√£o)', 'GPT\\n(Chat)']\n",
        "exemplos_dir = ['BioBERT\\n(Medicina)', 'CodeT5\\n(C√≥digo)', 'FinBERT\\n(Finan√ßas)']\n",
        "\n",
        "for i, (esq, dir) in enumerate(zip(exemplos_esq, exemplos_dir)):\n",
        "    y_pos = 8 - i * 2\n",
        "    \n",
        "    # Esquerda\n",
        "    ax.text(0.5, y_pos, esq, ha='center', va='center', \n",
        "            fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightpink'))\n",
        "    \n",
        "    # Direita\n",
        "    ax.text(15.5, y_pos, dir, ha='center', va='center', \n",
        "            fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightcyan'))\n",
        "\n",
        "# T√≠tulo\n",
        "ax.text(8, 11, 'üé≠ MAPA MENTAL: TIPOS DE MODELOS LLM', \n",
        "        ha='center', va='center', fontsize=18, fontweight='bold')\n",
        "\n",
        "# Rodap√© com pr√≥ximos passos\n",
        "ax.text(8, 0.5, 'üîÆ PR√ìXIMO: M√≥dulo 7 - Treinamento e Pre-treinamento', \n",
        "        ha='center', va='center', fontsize=12, style='italic', \n",
        "        bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lavender'))\n",
        "\n",
        "ax.set_xlim(-1, 17)\n",
        "ax.set_ylim(0, 12)\n",
        "ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ PARAB√âNS! Voc√™ completou o M√≥dulo 6!\")\n",
        "print(\"\\nüìö CHECKLIST DO QUE VOC√ä APRENDEU:\")\n",
        "checklist = [\n",
        "    \"‚úÖ Diferen√ßa entre modelos base e especializados\",\n",
        "    \"‚úÖ Tr√™s arquiteturas principais (Encoder/Decoder/Both)\",\n",
        "    \"‚úÖ Generativo vs Discriminativo\",\n",
        "    \"‚úÖ Impacto do tamanho nos modelos\",\n",
        "    \"‚úÖ Processo de fine-tuning\",\n",
        "    \"‚úÖ Como escolher modelo para cada tarefa\",\n",
        "    \"‚úÖ Casos pr√°ticos de uso\"\n",
        "]\n",
        "\n",
        "for item in checklist:\n",
        "    print(f\"   {item}\")\n",
        "\n",
        "print(\"\\nüöÄ VOC√ä EST√Å PRONTO PARA:\")\n",
        "print(\"   ‚Ä¢ Escolher o modelo certo para cada projeto\")\n",
        "print(\"   ‚Ä¢ Entender custos e trade-offs\")\n",
        "print(\"   ‚Ä¢ Planejar arquiteturas de IA\")\n",
        "print(\"   ‚Ä¢ Aprender sobre treinamento (pr√≥ximo m√≥dulo!)\")\n",
        "\n",
        "print(\"\\nüé™ At√© o pr√≥ximo m√≥dulo! Bora descobrir como esses modelos 'nascem'!\")"
      ]
    }
  ]
}