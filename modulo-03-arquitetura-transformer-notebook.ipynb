{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Transformers: A Revolu√ß√£o que Mudou a IA Para Sempre!\n\n",
        "## M√≥dulo 3: Desvendando a Arquitetura por Tr√°s dos LLMs\n\n",
        "**Pedro Nunes Guth - Expert em IA & AWS**\n\n",
        "---\n\n",
        "Ea√≠ pessoal! üéØ\n\n",
        "Lembram do m√≥dulo passado quando falamos sobre LLMs? Pois √©, agora chegou a hora de entender **COMO** essas m√°quinas incr√≠veis funcionam por dentro!\n\n",
        "√â como se no m√≥dulo anterior voc√™ conheceu um carro incr√≠vel (os LLMs), e agora vamos abrir o cap√¥ para ver o motor V8 turbinado que faz tudo funcionar: **a Arquitetura Transformer**!\n\n",
        "Bora que hoje vai ser **LIIINDO**! üî•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Preparando nosso laborat√≥rio!\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import MultiheadAttention\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√µes para gr√°ficos mais bonitos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"üöÄ Ambiente configurado! Prontos para explorar os Transformers!\")\n",
        "print(f\"üì± PyTorch vers√£o: {torch.__version__}\")\n",
        "print(\"üéØ Bora come√ßar a divers√£o!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé≠ O Que S√£o Transformers Afinal?\n\n",
        "T√°, mas o que diabos √© um Transformer?\n\n",
        "Imagina que voc√™ t√° numa festa e precisa prestar aten√ß√£o em v√°rias conversas ao mesmo tempo. Seu c√©rebro faz isso naturalmente - voc√™ consegue:\n",
        "- Focar na conversa principal\n",
        "- Captar informa√ß√µes importantes de outras conversas \n",
        "- Entender o contexto geral do ambiente\n",
        "- Conectar informa√ß√µes de diferentes momentos\n\n",
        "**√â exatamente isso que os Transformers fazem!** üß†\n\n",
        "### Por que Revolutionaram a IA?\n\n",
        "Antes dos Transformers (2017), os modelos de linguagem eram como aquele amigo que s√≥ consegue prestar aten√ß√£o numa coisa de cada vez. Eles processavam texto **sequencialmente** - palavra por palavra, da esquerda para direita.\n\n",
        "Os Transformers chegaram e disseram: \"√î gente, por que n√£o olhamos para TODAS as palavras ao mesmo tempo e deixamos elas conversarem entre si?\"\n\n",
        "**BOOM!** üí• Nasceram o GPT, BERT, ChatGPT e toda essa revolu√ß√£o que vivemos hoje!\n\n",
        "### üí° Dica do Pedro\n",
        "O nome \"Transformer\" vem do paper \"Attention Is All You Need\" (2017). √â como se os pesquisadores dissessem: \"Esque√ßam tudo! A aten√ß√£o √© tudo que precisamos!\" E cara... eles estavam certos!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Vis√£o Geral da Arquitetura\n\n",
        "Vamos visualizar como um Transformer funciona de forma geral:\n\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[\"üìù Texto de Entrada\"] --> B[\"üî¢ Tokeniza√ß√£o\"]\n",
        "    B --> C[\"üìä Embeddings\"]\n",
        "    C --> D[\"üìç Positional Encoding\"]\n",
        "    D --> E[\"üéØ Multi-Head Attention\"]\n",
        "    E --> F[\"üßÆ Feed Forward\"]\n",
        "    F --> G[\"üîÑ Camadas Transformer\"]\n",
        "    G --> H[\"üì§ Sa√≠da Final\"]\n",
        "    \n",
        "    style A fill:#ff9999\n",
        "    style E fill:#99ccff\n",
        "    style H fill:#99ff99\n",
        "```\n\n",
        "Pensa no Transformer como uma **f√°brica super inteligente**:\n\n",
        "1. **Entrada**: Texto cru (\"Hoje est√° um belo dia\")\n",
        "2. **Tokeniza√ß√£o**: Quebra em peda√ßos menores\n",
        "3. **Embeddings**: Transforma palavras em n√∫meros\n",
        "4. **Aten√ß√£o**: As palavras \"conversam\" entre si\n",
        "5. **Processamento**: V√°rias camadas refinam o entendimento\n",
        "6. **Sa√≠da**: Resultado final (pr√≥xima palavra, tradu√ß√£o, etc.)\n\n",
        "Liiindo n√©? Agora vamos mergulhar em cada parte! üèä‚Äç‚ôÇÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma visualiza√ß√£o da arquitetura Transformer\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Definindo as camadas do Transformer\n",
        "layers = [\n",
        "    'Texto de Entrada',\n",
        "    'Token Embeddings',\n",
        "    'Positional Encoding',\n",
        "    'Multi-Head Attention',\n",
        "    'Add & Norm',\n",
        "    'Feed Forward',\n",
        "    'Add & Norm',\n",
        "    'Sa√≠da'\n",
        "]\n",
        "\n",
        "colors = ['#ff9999', '#ffcc99', '#99ff99', '#99ccff', \n",
        "          '#cc99ff', '#ffff99', '#ff99cc', '#99ffcc']\n",
        "\n",
        "y_positions = range(len(layers))\n",
        "\n",
        "# Criando as barras horizontais\n",
        "bars = ax.barh(y_positions, [1]*len(layers), color=colors, alpha=0.7, height=0.6)\n",
        "\n",
        "# Adicionando os nomes das camadas\n",
        "for i, layer in enumerate(layers):\n",
        "    ax.text(0.5, i, layer, ha='center', va='center', fontweight='bold', fontsize=11)\n",
        "\n",
        "ax.set_ylim(-0.5, len(layers) - 0.5)\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_yticks([])\n",
        "ax.set_xticks([])\n",
        "ax.set_title('üèóÔ∏è Arquitetura Transformer - Fluxo de Dados', fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "# Adicionando setas\n",
        "for i in range(len(layers)-1):\n",
        "    ax.annotate('', xy=(0.5, i+1), xytext=(0.5, i),\n",
        "                arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Essa √© a jornada que cada palavra faz dentro de um Transformer!\")\n",
        "print(\"üìö Nos pr√≥ximos m√≥dulos veremos Tokens e Embeddings em detalhes!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ O Cora√ß√£o dos Transformers: Attention Mechanism\n\n",
        "Chegamos na parte mais **IMPORTANTE** de todo o Transformer: o **Mecanismo de Aten√ß√£o**!\n\n",
        "### A Analogia da Festa (Vers√£o Melhorada) üéâ\n\n",
        "Imagina que voc√™ t√° numa festa e algu√©m grita: \"O Pedro chegou!\"\n\n",
        "Seu c√©rebro automaticamente:\n",
        "1. **Processa a frase completa** (n√£o palavra por palavra)\n",
        "2. **Identifica que \"Pedro\" √© o foco** principal\n",
        "3. **Conecta \"chegou\" com \"Pedro\"** (n√£o com \"O\")\n",
        "4. **Entende o contexto** da situa√ß√£o\n\n",
        "√â isso que a **Aten√ß√£o** faz! Ela permite que cada palavra \"olhe\" para todas as outras e decida o quanto deve prestar aten√ß√£o em cada uma.\n\n",
        "### Como Funciona Matematicamente?\n\n",
        "A f√≥rmula da aten√ß√£o √© surpreendentemente elegante:\n\n",
        "**Attention(Q, K, V) = softmax(QK^T / ‚àöd_k)V**\n\n",
        "Onde:\n",
        "- **Q (Query)**: \"O que estou procurando?\"\n",
        "- **K (Key)**: \"O que tenho dispon√≠vel?\"\n",
        "- **V (Value)**: \"Qual informa√ß√£o vou usar?\"\n\n",
        "### üí° Dica do Pedro\n",
        "Pensa assim: Q √© sua pergunta, K s√£o as op√ß√µes de resposta, e V √© o conte√∫do real das respostas. A aten√ß√£o calcula o quanto cada resposta √© relevante para sua pergunta!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando um mecanismo de aten√ß√£o simples\n",
        "def simple_attention(query, key, value, mask=None):\n",
        "    \"\"\"\n",
        "    Implementa√ß√£o simples do mecanismo de aten√ß√£o\n",
        "    \n",
        "    Args:\n",
        "        query: Tensor [batch_size, seq_len, d_model]\n",
        "        key: Tensor [batch_size, seq_len, d_model] \n",
        "        value: Tensor [batch_size, seq_len, d_model]\n",
        "        mask: Tensor opcional para mascarar posi√ß√µes\n",
        "    \n",
        "    Returns:\n",
        "        output: Tensor com aten√ß√£o aplicada\n",
        "        attention_weights: Pesos de aten√ß√£o\n",
        "    \"\"\"\n",
        "    d_k = query.size(-1)\n",
        "    \n",
        "    # Passo 1: Calcular scores de aten√ß√£o (Q * K^T)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    \n",
        "    # Passo 2: Aplicar m√°scara se fornecida\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    # Passo 3: Aplicar softmax para obter probabilidades\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    # Passo 4: Aplicar aten√ß√£o aos valores\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "    \n",
        "    return output, attention_weights\n\n",
        "# Teste com dados sint√©ticos\n",
        "batch_size, seq_len, d_model = 1, 4, 8\n",
        "\n",
        "# Criando tensores de exemplo\n",
        "Q = torch.randn(batch_size, seq_len, d_model)\n",
        "K = torch.randn(batch_size, seq_len, d_model)\n",
        "V = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "print(\"üßÆ Testando nossa implementa√ß√£o de aten√ß√£o:\")\n",
        "print(f\"üìê Dimens√µes: batch={batch_size}, seq_len={seq_len}, d_model={d_model}\")\n",
        "\n",
        "output, attention_weights = simple_attention(Q, K, V)\n",
        "\n",
        "print(f\"‚úÖ Sa√≠da da aten√ß√£o: {output.shape}\")\n",
        "print(f\"üéØ Pesos de aten√ß√£o: {attention_weights.shape}\")\n",
        "print(\"\\nüî• Implementa√ß√£o funcionando perfeitamente!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos visualizar os pesos de aten√ß√£o!\n",
        "# Simulando uma frase: \"O gato subiu no telhado\"\n",
        "words = [\"O\", \"gato\", \"subiu\", \"no\", \"telhado\"]\n",
        "seq_len = len(words)\n",
        "\n",
        "# Criando uma matriz de aten√ß√£o real√≠stica\n",
        "# Onde palavras importantes prestam mais aten√ß√£o umas nas outras\n",
        "attention_matrix = np.array([\n",
        "    [0.1, 0.8, 0.05, 0.03, 0.02],  # \"O\" presta aten√ß√£o principalmente em \"gato\"\n",
        "    [0.15, 0.4, 0.3, 0.1, 0.05],   # \"gato\" se conecta com \"subiu\"\n",
        "    [0.05, 0.3, 0.4, 0.15, 0.1],   # \"subiu\" conecta \"gato\" e \"telhado\"\n",
        "    [0.02, 0.1, 0.15, 0.3, 0.43],  # \"no\" presta aten√ß√£o em \"telhado\"\n",
        "    [0.03, 0.2, 0.2, 0.27, 0.3]    # \"telhado\" se conecta com v√°rias palavras\n",
        "])\n",
        "\n",
        "# Criando o heatmap\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "sns.heatmap(attention_matrix, \n",
        "            xticklabels=words, \n",
        "            yticklabels=words,\n",
        "            annot=True, \n",
        "            fmt='.2f', \n",
        "            cmap='Blues',\n",
        "            cbar_kws={'label': 'Peso de Aten√ß√£o'},\n",
        "            ax=ax)\n",
        "\n",
        "ax.set_title('üéØ Matriz de Aten√ß√£o: \"O gato subiu no telhado\"', \n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "ax.set_xlabel('Palavras (Keys)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Palavras (Queries)', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üîç Analisando a matriz de aten√ß√£o:\")\n",
        "print(\"üìà Valores mais altos = mais aten√ß√£o entre as palavras\")\n",
        "print(\"üéØ Note como 'gato' e 'subiu' prestam aten√ß√£o um no outro!\")\n",
        "print(\"üè† 'Telhado' recebe aten√ß√£o de v√°rias palavras (√© o destino da a√ß√£o!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Multi-Head Attention: M√∫ltiplas Perspectivas\n\n",
        "T√°, mas aqui vem a parte **GENIAL** dos Transformers!\n\n",
        "Imagina que voc√™ tem **8 c√©rebros diferentes** analisando a mesma frase, cada um prestando aten√ß√£o em aspectos diferentes:\n\n",
        "- **C√©rebro 1**: Foca em rela√ß√µes sujeito-verbo\n",
        "- **C√©rebro 2**: Analisa objetos e complementos  \n",
        "- **C√©rebro 3**: Identifica emo√ß√µes e sentimentos\n",
        "- **C√©rebro 4**: Procura por nega√ß√µes\n",
        "- E assim por diante...\n\n",
        "No final, voc√™ **combina** todas essas perspectivas para ter uma compreens√£o **MUITO** mais rica do texto!\n\n",
        "### Como Funciona?\n\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[\"üß† Entrada\"] --> B1[\"Head 1\"]\n",
        "    A --> B2[\"Head 2\"]\n",
        "    A --> B3[\"Head 3\"]\n",
        "    A --> B4[\"Head ...\"]\n",
        "    A --> B5[\"Head 8\"]\n",
        "    \n",
        "    B1 --> C[\"üîó Concatenar\"]\n",
        "    B2 --> C\n",
        "    B3 --> C\n",
        "    B4 --> C\n",
        "    B5 --> C\n",
        "    \n",
        "    C --> D[\"üìä Proje√ß√£o Linear\"]\n",
        "    D --> E[\"‚ú® Sa√≠da Final\"]\n",
        "    \n",
        "    style A fill:#ff9999\n",
        "    style E fill:#99ff99\n",
        "```\n\n",
        "### üí° Dica do Pedro\n",
        "√â como ter uma mesa redonda com 8 especialistas diferentes analisando o mesmo problema. Cada um traz uma perspectiva √∫nica, e a decis√£o final considera todas as opini√µes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        # Proje√ß√µes lineares para Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"Implementa a aten√ß√£o scaled dot-product\"\"\"\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "            \n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        \n",
        "        # 1. Aplicar proje√ß√µes lineares\n",
        "        Q = self.W_q(query)\n",
        "        K = self.W_k(key) \n",
        "        V = self.W_v(value)\n",
        "        \n",
        "        # 2. Dividir em m√∫ltiplas heads\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # 3. Aplicar aten√ß√£o\n",
        "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        \n",
        "        # 4. Concatenar heads\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, -1, self.d_model)\n",
        "        \n",
        "        # 5. Proje√ß√£o final\n",
        "        output = self.W_o(attention_output)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# Testando nossa implementa√ß√£o\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "seq_len = 10\n",
        "batch_size = 2\n",
        "\n",
        "mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# Dados de teste\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "print(\"üß† Testando Multi-Head Attention:\")\n",
        "print(f\"üìä Entrada: {x.shape}\")\n",
        "print(f\"üéØ N√∫mero de heads: {num_heads}\")\n",
        "print(f\"üìê Dimens√£o por head: {d_model // num_heads}\")\n",
        "\n",
        "output, attention_weights = mha(x, x, x)\n",
        "\n",
        "print(f\"‚úÖ Sa√≠da: {output.shape}\")\n",
        "print(f\"‚öñÔ∏è Pesos de aten√ß√£o: {attention_weights.shape}\")\n",
        "print(\"\\nüî• Multi-Head Attention implementado com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando como diferentes heads focam em aspectos diferentes\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Simulando padr√µes de aten√ß√£o de 8 heads diferentes\n",
        "words = [\"Eu\", \"amo\", \"programar\", \"IA\", \"com\", \"Python\"]\n",
        "seq_len = len(words)\n",
        "\n",
        "# Cada head tem um padr√£o de aten√ß√£o diferente\n",
        "attention_patterns = [\n",
        "    # Head 1: Foca em pronomes e verbos\n",
        "    np.array([[0.7, 0.2, 0.05, 0.02, 0.02, 0.01],\n",
        "              [0.3, 0.4, 0.2, 0.05, 0.03, 0.02],\n",
        "              [0.1, 0.3, 0.4, 0.1, 0.05, 0.05],\n",
        "              [0.05, 0.1, 0.3, 0.4, 0.1, 0.05],\n",
        "              [0.02, 0.05, 0.2, 0.2, 0.4, 0.13],\n",
        "              [0.01, 0.02, 0.1, 0.15, 0.2, 0.52]]),\n",
        "    \n",
        "    # Head 2: Foca em objetos e complementos\n",
        "    np.array([[0.1, 0.1, 0.3, 0.4, 0.05, 0.05],\n",
        "              [0.05, 0.2, 0.25, 0.35, 0.1, 0.05],\n",
        "              [0.02, 0.08, 0.4, 0.3, 0.15, 0.05],\n",
        "              [0.03, 0.07, 0.3, 0.4, 0.15, 0.05],\n",
        "              [0.02, 0.03, 0.2, 0.25, 0.3, 0.2],\n",
        "              [0.01, 0.02, 0.15, 0.2, 0.22, 0.4]]),\n",
        "    \n",
        "    # Head 3: Padr√£o local (palavras adjacentes)\n",
        "    np.array([[0.5, 0.4, 0.05, 0.02, 0.02, 0.01],\n",
        "              [0.4, 0.3, 0.25, 0.03, 0.01, 0.01],\n",
        "              [0.05, 0.25, 0.4, 0.25, 0.03, 0.02],\n",
        "              [0.02, 0.03, 0.25, 0.4, 0.25, 0.05],\n",
        "              [0.01, 0.01, 0.03, 0.25, 0.4, 0.3],\n",
        "              [0.01, 0.01, 0.02, 0.05, 0.31, 0.6]]),\n",
        "    \n",
        "    # Mais 5 heads com padr√µes variados...\n",
        "    np.random.dirichlet(np.ones(seq_len), seq_len),\n",
        "    np.random.dirichlet(np.ones(seq_len), seq_len),\n",
        "    np.random.dirichlet(np.ones(seq_len), seq_len),\n",
        "    np.random.dirichlet(np.ones(seq_len), seq_len),\n",
        "    np.random.dirichlet(np.ones(seq_len), seq_len)\n",
        "]\n",
        "\n",
        "head_names = [\n",
        "    \"Head 1: Sujeito-Verbo\",\n",
        "    \"Head 2: Objetos\", \n",
        "    \"Head 3: Adjac√™ncia\",\n",
        "    \"Head 4: Padr√£o A\",\n",
        "    \"Head 5: Padr√£o B\",\n",
        "    \"Head 6: Padr√£o C\", \n",
        "    \"Head 7: Padr√£o D\",\n",
        "    \"Head 8: Padr√£o E\"\n",
        "]\n",
        "\n",
        "for i in range(8):\n",
        "    sns.heatmap(attention_patterns[i],\n",
        "                xticklabels=words,\n",
        "                yticklabels=words,\n",
        "                cmap='Blues',\n",
        "                ax=axes[i],\n",
        "                cbar=False,\n",
        "                square=True)\n",
        "    axes[i].set_title(head_names[i], fontsize=10, fontweight='bold')\n",
        "    axes[i].tick_params(labelsize=8)\n",
        "\n",
        "plt.suptitle('üß† Multi-Head Attention: 8 Perspectivas Diferentes da Mesma Frase', \n",
        "             fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Cada head aprende a focar em aspectos diferentes!\")\n",
        "print(\"üîç Head 1: Conecta sujeitos com verbos\")\n",
        "print(\"üé™ Head 2: Identifica objetos e complementos\") \n",
        "print(\"üîó Head 3: Analisa palavras pr√≥ximas\")\n",
        "print(\"‚ú® Os outros heads descobrem padr√µes √∫nicos durante o treinamento!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Feed Forward Network: O Processamento Individual\n\n",
        "Depois que cada palavra \"conversou\" com todas as outras atrav√©s da aten√ß√£o, chegou a hora do **processamento individual**!\n\n",
        "√â como se ap√≥s a reuni√£o em grupo, cada pessoa fosse para sua sala pensar individualmente sobre o que discutiu.\n\n",
        "### O que faz a Feed Forward Network?\n\n",
        "```\n",
        "FFN(x) = max(0, xW‚ÇÅ + b‚ÇÅ)W‚ÇÇ + b‚ÇÇ\n",
        "```\n\n",
        "Traduzindo:\n",
        "1. **Primeira camada**: Expande a representa√ß√£o (geralmente 4x maior)\n",
        "2. **ReLU**: Adiciona n√£o-linearidade \n",
        "3. **Segunda camada**: Comprime de volta ao tamanho original\n",
        "\n",
        "### Analogia do Processamento de Texto üìù\n\n",
        "Imagina que voc√™ recebeu v√°rias informa√ß√µes sobre uma pessoa:\n",
        "- Nome: Pedro\n",
        "- Profiss√£o: Instrutor\n",
        "- Expertise: IA e AWS\n",
        "- Local: Brasil\n",
        "\n",
        "A Feed Forward pega essas informa√ß√µes e:\n",
        "1. **Expande**: Gera v√°rias hip√≥teses e conex√µes\n",
        "2. **Processa**: Analisa padr√µes e rela√ß√µes\n",
        "3. **Condensa**: Retorna uma representa√ß√£o refinada\n\n",
        "### üí° Dica do Pedro\n",
        "A FFN √© onde o \"pensamento profundo\" acontece! Enquanto a aten√ß√£o conecta informa√ß√µes, a FFN processa e refina cada representa√ß√£o individualmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando a Feed Forward Network\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Primeira transforma√ß√£o linear + ReLU\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Segunda transforma√ß√£o linear\n",
        "        x = self.linear2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Testando a FFN\n",
        "d_model = 512\n",
        "d_ff = 2048  # Geralmente 4x maior que d_model\n",
        "batch_size = 1\n",
        "seq_len = 8\n",
        "\n",
        "ffn = FeedForwardNetwork(d_model, d_ff)\n",
        "\n",
        "# Dados de teste\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "print(\"‚ö° Testando Feed Forward Network:\")\n",
        "print(f\"üìä Entrada: {x.shape} (batch_size, seq_len, d_model)\")\n",
        "print(f\"üîç d_model: {d_model}\")\n",
        "print(f\"üìà d_ff (dimens√£o interna): {d_ff}\")\n",
        "\n",
        "output = ffn(x)\n",
        "\n",
        "print(f\"‚úÖ Sa√≠da: {output.shape}\")\n",
        "print(f\"üéØ Mesma dimens√£o da entrada: {output.shape == x.shape}\")\n",
        "\n",
        "# Analisando os par√¢metros\n",
        "total_params = sum(p.numel() for p in ffn.parameters())\n",
        "print(f\"\\nüìä Par√¢metros da FFN: {total_params:,}\")\n",
        "print(\"üî• FFN funcionando perfeitamente!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando o que acontece dentro da FFN\n",
        "# Vamos analisar como as dimens√µes mudam ao longo da rede\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Entrada\n",
        "    x = torch.randn(1, 8, 512)\n",
        "    \n",
        "    # Primeira camada linear\n",
        "    x1 = ffn.linear1(x)  # [1, 8, 512] -> [1, 8, 2048]\n",
        "    \n",
        "    # Ap√≥s ReLU\n",
        "    x2 = ffn.relu(x1)    # Aplica ReLU (zera valores negativos)\n",
        "    \n",
        "    # Segunda camada linear\n",
        "    x3 = ffn.linear2(x2) # [1, 8, 2048] -> [1, 8, 512]\n",
        "\n",
        "# Visualizando o fluxo de dimens√µes\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Heatmap da entrada\n",
        "im1 = axes[0,0].imshow(x[0].numpy(), cmap='RdBu', aspect='auto')\n",
        "axes[0,0].set_title('üì• Entrada (8 x 512)', fontweight='bold')\n",
        "axes[0,0].set_ylabel('Sequ√™ncia')\n",
        "axes[0,0].set_xlabel('Dimens√µes')\n",
        "plt.colorbar(im1, ax=axes[0,0])\n",
        "\n",
        "# Heatmap ap√≥s primeira linear (s√≥ primeiras 512 dims para visualizar)\n",
        "im2 = axes[0,1].imshow(x1[0, :, :512].numpy(), cmap='RdBu', aspect='auto')\n",
        "axes[0,1].set_title('üîÑ Ap√≥s 1¬™ Linear (8 x 2048‚Üí512)', fontweight='bold')\n",
        "axes[0,1].set_ylabel('Sequ√™ncia')\n",
        "axes[0,1].set_xlabel('Dimens√µes (amostra)')\n",
        "plt.colorbar(im2, ax=axes[0,1])\n",
        "\n",
        "# Heatmap ap√≥s ReLU\n",
        "im3 = axes[1,0].imshow(x2[0, :, :512].numpy(), cmap='RdBu', aspect='auto')\n",
        "axes[1,0].set_title('‚ö° Ap√≥s ReLU (8 x 2048‚Üí512)', fontweight='bold')\n",
        "axes[1,0].set_ylabel('Sequ√™ncia')\n",
        "axes[1,0].set_xlabel('Dimens√µes (amostra)')\n",
        "plt.colorbar(im3, ax=axes[1,0])\n",
        "\n",
        "# Heatmap da sa√≠da\n",
        "im4 = axes[1,1].imshow(x3[0].numpy(), cmap='RdBu', aspect='auto')\n",
        "axes[1,1].set_title('üì§ Sa√≠da Final (8 x 512)', fontweight='bold')\n",
        "axes[1,1].set_ylabel('Sequ√™ncia')\n",
        "axes[1,1].set_xlabel('Dimens√µes')\n",
        "plt.colorbar(im4, ax=axes[1,1])\n",
        "\n",
        "plt.suptitle('‚ö° Fluxo de Dados na Feed Forward Network', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üîç An√°lise do fluxo:\")\n",
        "print(f\"üì• Entrada: {x.shape}\")\n",
        "print(f\"üìà Expans√£o: {x1.shape} (4x maior!)\")\n",
        "print(f\"‚ö° Ap√≥s ReLU: {x2.shape} (zeros valores negativos)\")\n",
        "print(f\"üìâ Compress√£o: {x3.shape} (volta ao tamanho original)\")\n",
        "print(\"\\nüí° A FFN expande para processar melhor, depois comprime!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó Residual Connections e Layer Normalization\n\n",
        "Agora vamos falar sobre duas t√©cnicas **FUNDAMENTAIS** que fazem os Transformers funcionarem t√£o bem!\n\n",
        "### Residual Connections (Conex√µes Residuais) üåâ\n\n",
        "Imagina que voc√™ t√° fazendo uma receita complexa, mas quer garantir que n√£o vai perder o sabor original dos ingredientes.\n\n",
        "As conex√µes residuais fazem exatamente isso! Elas **preservam a informa√ß√£o original** enquanto permitem que as camadas adicionem informa√ß√µes novas.\n\n",
        "**F√≥rmula**: `sa√≠da = camada(entrada) + entrada`\n\n",
        "### Layer Normalization üìè\n\n",
        "√â como ter um \"controlador de qualidade\" que garante que os valores n√£o fiquem nem muito grandes nem muito pequenos.\n\n",
        "Imagina uma banda onde:\n",
        "- O **baixista** t√° tocando muito baixo\n",
        "- O **guitarrista** t√° muito alto\n",
        "- A **bateria** t√° no volume certo\n\n",
        "A Layer Norm √© como o **t√©cnico de som** que equilibra tudo para soar harmonioso!\n\n",
        "### Por que s√£o importantes?\n\n",
        "1. **Residual**: Evita o \"vanishing gradient\" (gradientes que somem)\n",
        "2. **Layer Norm**: Estabiliza o treinamento\n",
        "3. **Juntas**: Permitem redes muito profundas (at√© 96 camadas no GPT-3!)\n\n",
        "### üí° Dica do Pedro\n",
        "Sem essas duas t√©cnicas, seria imposs√≠vel treinar modelos gigantes como GPT-4. Elas s√£o os \"superpoderes\" que tornaram os LLMs modernos poss√≠veis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando um bloco Transformer completo com Residual + LayerNorm\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        \n",
        "        # Componentes principais\n",
        "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout)\n",
        "        \n",
        "        # Layer Normalizations\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        # Primeiro sub-bloco: Multi-Head Attention + Residual + LayerNorm\n",
        "        attention_output, _ = self.multi_head_attention(x, x, x, mask)\n",
        "        attention_output = self.dropout(attention_output)\n",
        "        x = self.ln1(x + attention_output)  # Residual connection + LayerNorm\n",
        "        \n",
        "        # Segundo sub-bloco: Feed Forward + Residual + LayerNorm  \n",
        "        ff_output = self.feed_forward(x)\n",
        "        ff_output = self.dropout(ff_output)\n",
        "        x = self.ln2(x + ff_output)  # Residual connection + LayerNorm\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Testando um bloco Transformer completo\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "seq_len = 16\n",
        "batch_size = 2\n",
        "\n",
        "transformer_block = TransformerBlock(d_model, num_heads, d_ff)\n",
        "\n",
        "# Dados de teste\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "print(\"üß± Testando Bloco Transformer Completo:\")\n",
        "print(f\"üìä Entrada: {x.shape}\")\n",
        "print(f\"üß† Heads: {num_heads}\")\n",
        "print(f\"‚ö° FFN dim: {d_ff}\")\n",
        "\n",
        "# Forward pass\n",
        "output = transformer_block(x)\n",
        "\n",
        "print(f\"‚úÖ Sa√≠da: {output.shape}\")\n",
        "print(f\"üéØ Formato preservado: {output.shape == x.shape}\")\n",
        "\n",
        "# Contando par√¢metros\n",
        "total_params = sum(p.numel() for p in transformer_block.parameters())\n",
        "print(f\"\\nüìä Par√¢metros do bloco: {total_params:,}\")\n",
        "print(\"üî• Bloco Transformer completo funcionando!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos visualizar o efeito da Layer Normalization\n",
        "import torch.nn as nn\n",
        "\n",
        "# Criando dados com distribui√ß√µes diferentes (simulando instabilidade)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Dados antes da normaliza√ß√£o (inst√°veis)\n",
        "data_unstable = torch.tensor([\n",
        "    [100.0, 0.1, 50.0, 0.01],  # Primeira sequ√™ncia: valores muito variados\n",
        "    [0.001, 200.0, 0.5, 75.0], # Segunda sequ√™ncia: tamb√©m inst√°vel\n",
        "    [1000.0, 0.2, 10.0, 0.1],  # Terceira sequ√™ncia: extremamente variado\n",
        "], dtype=torch.float32)\n",
        "\n",
        "# Aplicando Layer Normalization\n",
        "layer_norm = nn.LayerNorm(4)  # 4 √© o n√∫mero de features\n",
        "data_normalized = layer_norm(data_unstable)\n",
        "\n",
        "# Visualizando a diferen√ßa\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Dados antes da normaliza√ß√£o\n",
        "im1 = ax1.imshow(data_unstable.numpy(), cmap='RdYlBu', aspect='auto')\n",
        "ax1.set_title('üìà Antes da Layer Normalization\\n(Valores Inst√°veis)', fontweight='bold', fontsize=12)\n",
        "ax1.set_xlabel('Features')\n",
        "ax1.set_ylabel('Sequ√™ncias')\n",
        "ax1.set_xticks(range(4))\n",
        "ax1.set_yticks(range(3))\n",
        "plt.colorbar(im1, ax=ax1, label='Valores')\n",
        "\n",
        "# Adicionando valores nas c√©lulas\n",
        "for i in range(3):\n",
        "    for j in range(4):\n",
        "        ax1.text(j, i, f'{data_unstable[i,j]:.3f}', \n",
        "                ha='center', va='center', fontweight='bold')\n",
        "\n",
        "# Dados ap√≥s normaliza√ß√£o\n",
        "im2 = ax2.imshow(data_normalized.detach().numpy(), cmap='RdYlBu', aspect='auto')\n",
        "ax2.set_title('üìè Ap√≥s Layer Normalization\\n(Valores Estabilizados)', fontweight='bold', fontsize=12)\n",
        "ax2.set_xlabel('Features')\n",
        "ax2.set_ylabel('Sequ√™ncias')\n",
        "ax2.set_xticks(range(4))\n",
        "ax2.set_yticks(range(3))\n",
        "plt.colorbar(im2, ax=ax2, label='Valores')\n",
        "\n",
        "# Adicionando valores nas c√©lulas\n",
        "for i in range(3):\n",
        "    for j in range(4):\n",
        "        ax2.text(j, i, f'{data_normalized[i,j]:.3f}', \n",
        "                ha='center', va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Estat√≠sticas\n",
        "print(\"üìä Estat√≠sticas ANTES da Layer Norm:\")\n",
        "print(f\"üî¢ M√©dia por linha: {data_unstable.mean(dim=1)}\")\n",
        "print(f\"üìà Desvio padr√£o por linha: {data_unstable.std(dim=1)}\")\n",
        "print(f\"üìè Range de valores: {data_unstable.min():.3f} a {data_unstable.max():.3f}\")\n",
        "\n",
        "print(\"\\nüìä Estat√≠sticas AP√ìS a Layer Norm:\")\n",
        "print(f\"üî¢ M√©dia por linha: {data_normalized.mean(dim=1)}\")\n",
        "print(f\"üìà Desvio padr√£o por linha: {data_normalized.std(dim=1)}\")\n",
        "print(f\"üìè Range de valores: {data_normalized.min():.3f} a {data_normalized.max():.3f}\")\n",
        "\n",
        "print(\"\\nüí° Viu como a Layer Norm estabilizou os valores?\")\n",
        "print(\"üéØ M√©dia ‚âà 0 e desvio padr√£o ‚âà 1 para cada linha!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìç Positional Encoding: Ensinando Posi√ß√£o aos Transformers\n\n",
        "Aqui temos um **PROBLEMA INTERESSANTE**: os Transformers processam todas as palavras ao mesmo tempo, mas como eles sabem a **ordem** das palavras?\n\n",
        "Pensa na diferen√ßa entre:\n",
        "- \"O gato subiu no telhado\" üê±‚¨ÜÔ∏è\n",
        "- \"O telhado subiu no gato\" üè†‚¨ÜÔ∏èüò±\n\n",
        "A ordem das palavras **IMPORTA MUITO**!\n\n",
        "### A Solu√ß√£o: Positional Encoding üó∫Ô∏è\n\n",
        "√â como dar um **GPS** para cada palavra! Cada posi√ß√£o recebe um \"c√≥digo √∫nico\" que √© somado ao embedding da palavra.\n\n",
        "### Como Funciona?\n\n",
        "```\n",
        "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
        "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "```\n\n",
        "Parece complicado? Na verdade √© **GENIAL**! ü§Ø\n\n",
        "### Analogia da Impress√£o Digital üëÜ\n\n",
        "Imagina que cada posi√ß√£o na frase tem uma \"impress√£o digital\" √∫nica feita de ondas seno e cosseno:\n",
        "- **Posi√ß√£o 0**: Um padr√£o √∫nico de ondas\n",
        "- **Posi√ß√£o 1**: Outro padr√£o levemente diferente  \n",
        "- **Posi√ß√£o 2**: Mais um padr√£o √∫nico\n",
        "- E assim por diante...\n\n",
        "### üí° Dica do Pedro\n",
        "O uso de seno e cosseno √© brilhante! Permite ao modelo aprender facilmente dist√¢ncias relativas entre posi√ß√µes. √â matem√°tica pura a servi√ßo da IA!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \n",
        "        # Criando matriz de positional encodings\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        \n",
        "        # Calculando os divisores para as frequ√™ncias\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
        "                           (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        # Aplicando seno nas posi√ß√µes pares e cosseno nas √≠mpares\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        # Adicionando dimens√£o de batch\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        \n",
        "        # Registrando como buffer (n√£o √© par√¢metro trein√°vel)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, d_model]\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:seq_len, :].transpose(0, 1)\n",
        "\n",
        "# Testando Positional Encoding\n",
        "d_model = 512\n",
        "max_seq_length = 100\n",
        "batch_size = 1\n",
        "seq_len = 20\n",
        "\n",
        "pos_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "# Dados de teste (simulando embeddings)\n",
        "embeddings = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "print(\"üìç Testando Positional Encoding:\")\n",
        "print(f\"üìä Embeddings originais: {embeddings.shape}\")\n",
        "print(f\"üó∫Ô∏è Comprimento m√°ximo suportado: {max_seq_length}\")\n",
        "\n",
        "# Aplicando positional encoding\n",
        "embeddings_with_pos = pos_encoding(embeddings)\n",
        "\n",
        "print(f\"‚úÖ Embeddings + Posi√ß√£o: {embeddings_with_pos.shape}\")\n",
        "print(f\"üéØ Formato preservado: {embeddings_with_pos.shape == embeddings.shape}\")\n",
        "\n",
        "# Verificando que o positional encoding foi adicionado\n",
        "difference = embeddings_with_pos - embeddings\n",
        "print(f\"\\nüìè Diferen√ßa (positional encoding): min={difference.min():.4f}, max={difference.max():.4f}\")\n",
        "print(\"üî• Positional Encoding implementado com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando os padr√µes de Positional Encoding\n",
        "d_model = 128  # Usando dimens√£o menor para visualizar melhor\n",
        "seq_len = 50\n",
        "\n",
        "# Criando positional encoding\n",
        "pe_viz = PositionalEncoding(d_model, seq_len)\n",
        "\n",
        "# Extraindo apenas o positional encoding (sem somar aos embeddings)\n",
        "pos_encodings = pe_viz.pe[:seq_len, 0, :].numpy()  # [seq_len, d_model]\n",
        "\n",
        "# Criando visualiza√ß√µes\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Heatmap completo do positional encoding\n",
        "im1 = ax1.imshow(pos_encodings.T, cmap='RdBu', aspect='auto')\n",
        "ax1.set_title('üó∫Ô∏è Positional Encodings Completos\\n(Dimens√µes vs Posi√ß√µes)', fontweight='bold')\n",
        "ax1.set_xlabel('Posi√ß√£o na Sequ√™ncia')\n",
        "ax1.set_ylabel('Dimens√µes do Modelo')\n",
        "plt.colorbar(im1, ax=ax1)\n",
        "\n",
        "# 2. Padr√µes de algumas dimens√µes espec√≠ficas\n",
        "positions = np.arange(seq_len)\n",
        "dimensions_to_show = [0, 1, 10, 11, 30, 31]\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
        "\n",
        "for i, dim in enumerate(dimensions_to_show):\n",
        "    ax2.plot(positions, pos_encodings[:, dim], \n",
        "             label=f'Dim {dim}', color=colors[i], linewidth=2)\n",
        "\n",
        "ax2.set_title('üìä Padr√µes de Ondas por Dimens√£o', fontweight='bold')\n",
        "ax2.set_xlabel('Posi√ß√£o')\n",
        "ax2.set_ylabel('Valor do Encoding')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Comparando frequ√™ncias baixas vs altas\n",
        "ax3.plot(positions, pos_encodings[:, 0], 'r-', linewidth=3, label='Dim 0 (freq. baixa)')\n",
        "ax3.plot(positions, pos_encodings[:, 1], 'r--', linewidth=3, label='Dim 1 (freq. baixa)')\n",
        "ax3.plot(positions, pos_encodings[:, 60], 'b-', linewidth=3, label='Dim 60 (freq. alta)')\n",
        "ax3.plot(positions, pos_encodings[:, 61], 'b--', linewidth=3, label='Dim 61 (freq. alta)')\n",
        "\n",
        "ax3.set_title('üåä Frequ√™ncias Baixas vs Altas', fontweight='bold')\n",
        "ax3.set_xlabel('Posi√ß√£o')\n",
        "ax3.set_ylabel('Valor do Encoding')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Similaridade entre posi√ß√µes (produto escalar)\n",
        "# Calculando similaridade entre posi√ß√£o 10 e todas as outras\n",
        "reference_pos = 10\n",
        "similarities = []\n",
        "\n",
        "for pos in range(seq_len):\n",
        "    similarity = np.dot(pos_encodings[reference_pos], pos_encodings[pos])\n",
        "    similarities.append(similarity)\n",
        "\n",
        "ax4.plot(positions, similarities, 'g-', linewidth=3)\n",
        "ax4.axvline(x=reference_pos, color='red', linestyle='--', linewidth=2, \n",
        "           label=f'Posi√ß√£o de refer√™ncia ({reference_pos})')\n",
        "ax4.set_title(f'üéØ Similaridade com Posi√ß√£o {reference_pos}', fontweight='bold')\n",
        "ax4.set_xlabel('Posi√ß√£o')\n",
        "ax4.set_ylabel('Similaridade (produto escalar)')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üîç An√°lise dos Positional Encodings:\")\n",
        "print(\"üåä Frequ√™ncias baixas: Mudam lentamente, capturam posi√ß√µes distantes\")\n",
        "print(\"‚ö° Frequ√™ncias altas: Mudam rapidamente, distinguem posi√ß√µes pr√≥ximas\")\n",
        "print(\"üéØ Cada posi√ß√£o tem uma 'assinatura' √∫nica!\")\n",
        "print(\"üìè A similaridade mostra como posi√ß√µes pr√≥ximas s√£o mais similares\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Construindo um Transformer Completo\n\n",
        "Agora chegou a hora do **GRAND FINALE**! Vamos juntar todas as pe√ßas e construir um Transformer completo! üéØ\n\n",
        "√â como montar um quebra-cabe√ßa gigante onde cada pe√ßa que estudamos se encaixa perfeitamente:\n\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[\"üìù Input Tokens\"] --> B[\"üìä Token Embeddings\"]\n",
        "    B --> C[\"üìç + Positional Encoding\"]\n",
        "    C --> D[\"üß± Transformer Block 1\"]\n",
        "    D --> E[\"üß± Transformer Block 2\"]\n",
        "    E --> F[\"üß± Transformer Block N\"]\n",
        "    F --> G[\"üì§ Output Layer\"]\n",
        "    \n",
        "    subgraph \"üß± Transformer Block\"\n",
        "        H[\"üéØ Multi-Head Attention\"]\n",
        "        I[\"‚ûï Add & Norm\"]\n",
        "        J[\"‚ö° Feed Forward\"]\n",
        "        K[\"‚ûï Add & Norm\"]\n",
        "        H --> I --> J --> K\n",
        "    end\n",
        "    \n",
        "    style A fill:#ff9999\n",
        "    style G fill:#99ff99\n",
        "```\n\n",
        "### Arquitetura do Nosso Transformer üèóÔ∏è\n\n",
        "Vamos criar um **GPT simplificado** (decoder-only) que pode:\n",
        "- Receber uma sequ√™ncia de tokens\n",
        "- Processar atrav√©s de m√∫ltiplas camadas\n",
        "- Gerar probabilidades para o pr√≥ximo token\n\n",
        "### üí° Dica do Pedro\n",
        "Esse √© o mesmo princ√≠pio por tr√°s do ChatGPT! Claro, com algumas diferen√ßas de escala (eles t√™m 96+ camadas e trilh√µes de par√¢metros), mas a arquitetura base √© essa!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construindo um Transformer completo (GPT-style)\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout=0.1):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # Embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "        \n",
        "        # Transformer Blocks\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        # Final layer normalization\n",
        "        self.ln_final = nn.LayerNorm(d_model)\n",
        "        \n",
        "        # Output projection to vocabulary\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        \"\"\"Inicializa√ß√£o dos pesos seguindo as melhores pr√°ticas\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    \n",
        "    def create_causal_mask(self, seq_len):\n",
        "        \"\"\"Cria m√°scara causal para impedir que o modelo veja tokens futuros\"\"\"\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "        return mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]\n",
        "    \n",
        "    def forward(self, input_ids, targets=None):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        \n",
        "        # 1. Token embeddings\n",
        "        token_embeddings = self.token_embedding(input_ids)  # [batch, seq_len, d_model]\n",
        "        \n",
        "        # 2. Add positional encoding\n",
        "        x = self.positional_encoding(token_embeddings)\n",
        "        \n",
        "        # 3. Create causal mask (para GPT-style autoregressive generation)\n",
        "        causal_mask = self.create_causal_mask(seq_len)\n",
        "        \n",
        "        # 4. Pass through transformer blocks\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, causal_mask)\n",
        "        \n",
        "        # 5. Final layer normalization\n",
        "        x = self.ln_final(x)\n",
        "        \n",
        "        # 6. Project to vocabulary size\n",
        "        logits = self.output_projection(x)  # [batch, seq_len, vocab_size]\n",
        "        \n",
        "        # 7. Calculate loss if targets provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                targets.view(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "        \n",
        "        return logits, loss\n",
        "\n",
        "# Configura√ß√µes do modelo\n",
        "config = {\n",
        "    'vocab_size': 1000,      # Vocabul√°rio pequeno para teste\n",
        "    'd_model': 256,          # Dimens√£o menor para ser mais r√°pido\n",
        "    'num_heads': 8,          # 8 cabe√ßas de aten√ß√£o\n",
        "    'num_layers': 6,         # 6 camadas (GPT-2 small tem 12)\n",
        "    'd_ff': 1024,           # FFN dimension (4x d_model)\n",
        "    'max_seq_length': 128,   # Sequ√™ncia m√°xima\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "# Criando nosso Transformer!\n",
        "model = SimpleTransformer(**config)\n",
        "\n",
        "# Calculando n√∫mero de par√¢metros\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"üöÄ TRANSFORMER COMPLETO CRIADO!\")\n",
        "print(f\"üìä Par√¢metros totais: {total_params:,}\")\n",
        "print(f\"üéØ Par√¢metros trein√°veis: {trainable_params:,}\")\n",
        "print(f\"üß† N√∫mero de camadas: {config['num_layers']}\")\n",
        "print(f\"üé≠ N√∫mero de heads: {config['num_heads']}\")\n",
        "print(f\"üìö Tamanho do vocabul√°rio: {config['vocab_size']}\")\n",
        "print(\"\\nüî• Pronto para processar texto como um GPT!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando nosso Transformer com dados sint√©ticos\n",
        "batch_size = 2\n",
        "seq_len = 20\n",
        "\n",
        "# Criando sequ√™ncias de tokens aleat√≥rios\n",
        "input_ids = torch.randint(0, config['vocab_size'], (batch_size, seq_len))\n",
        "targets = torch.randint(0, config['vocab_size'], (batch_size, seq_len))\n",
        "\n",
        "print(\"üß™ Testando nosso Transformer:\")\n",
        "print(f\"üì• Input shape: {input_ids.shape}\")\n",
        "print(f\"üéØ Targets shape: {targets.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    logits, loss = model(input_ids, targets)\n",
        "\n",
        "print(f\"\\nüì§ Logits shape: {logits.shape}\")\n",
        "print(f\"üí∏ Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Calculando probabilidades para o √∫ltimo token\n",
        "last_token_logits = logits[0, -1, :]  # Primeiro batch, √∫ltimo token\n",
        "probabilities = F.softmax(last_token_logits, dim=-1)\n",
        "\n",
        "# Encontrando os top-5 tokens mais prov√°veis\n",
        "top5_probs, top5_indices = torch.topk(probabilities, 5)\n",
        "\n",
        "print(\"\\nüèÜ Top 5 pr√≥ximos tokens mais prov√°veis:\")\n",
        "for i, (token_id, prob) in enumerate(zip(top5_indices, top5_probs)):\n",
        "    print(f\"{i+1}. Token {token_id.item()}: {prob.item():.4f} ({prob.item()*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n‚úÖ Transformer funcionando perfeitamente!\")\n",
        "print(\"üéâ Parab√©ns! Voc√™ acabou de criar e testar um modelo Transformer completo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Exerc√≠cio Pr√°tico: Construa Seu Pr√≥prio Mini-Transformer!\n\n",
        "Agora √© a **SUA VEZ** de brilhar! üåü\n\n",
        "### Desafio 1: Implementa√ß√£o B√°sica\n",
        "\n",
        "Complete a implementa√ß√£o de um **Mini-Transformer** com as seguintes especifica√ß√µes:\n",
        "- 2 camadas\n",
        "- 4 heads de aten√ß√£o  \n",
        "- Vocabul√°rio de 500 tokens\n",
        "- Dimens√£o de 128\n",
        "\n",
        "### Dicas para o Sucesso:\n",
        "1. Use os componentes que j√° implementamos\n",
        "2. Teste com sequ√™ncias pequenas primeiro\n",
        "3. Verifique se as dimens√µes est√£o corretas\n",
        "4. Monitore se o loss diminui (sinal de que est√° aprendendo)\n\n",
        "### üí° Dica do Pedro\n",
        "Comece pequeno e v√° crescendo! √â melhor ter um modelo simples funcionando do que um complexo quebrado. Lembre-se: mesmo o GPT-4 come√ßou com conceitos b√°sicos como esses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 1: Complete a implementa√ß√£o do Mini-Transformer\n",
        "\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MiniTransformer, self).__init__()\n",
        "        \n",
        "        # TODO: Defina os par√¢metros do modelo\n",
        "        self.vocab_size = 500\n",
        "        self.d_model = 128\n",
        "        self.num_heads = 4\n",
        "        self.num_layers = 2\n",
        "        self.d_ff = 512  # 4x d_model\n",
        "        self.max_seq_length = 64\n",
        "        \n",
        "        # TODO: Implementar os componentes\n",
        "        # Dica: Use as classes que j√° criamos acima!\n",
        "        \n",
        "        # 1. Token embeddings\n",
        "        self.token_embedding = None  # COMPLETE AQUI\n",
        "        \n",
        "        # 2. Positional encoding\n",
        "        self.pos_encoding = None  # COMPLETE AQUI\n",
        "        \n",
        "        # 3. Transformer blocks\n",
        "        self.blocks = None  # COMPLETE AQUI\n",
        "        \n",
        "        # 4. Output layer\n",
        "        self.ln_final = None  # COMPLETE AQUI\n",
        "        self.output_proj = None  # COMPLETE AQUI\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO: Implementar o forward pass\n",
        "        # 1. Token embeddings\n",
        "        # 2. Positional encoding\n",
        "        # 3. Passar pelas camadas transformer\n",
        "        # 4. Layer norm final\n",
        "        # 5. Proje√ß√£o para vocabul√°rio\n",
        "        \n",
        "        pass  # SUBSTITUA POR SUA IMPLEMENTA√á√ÉO\n",
        "\n",
        "# Teste seu modelo aqui!\n",
        "print(\"üéØ EXERC√çCIO 1: Implemente o MiniTransformer acima!\")\n",
        "print(\"üí° Use as classes que j√° criamos como base\")\n",
        "print(\"üöÄ Quando terminar, descomente o c√≥digo abaixo para testar:\")\n",
        "\n",
        "# Descomente quando completar a implementa√ß√£o:\n",
        "# mini_model = MiniTransformer()\n",
        "# test_input = torch.randint(0, 500, (1, 10))\n",
        "# output = mini_model(test_input)\n",
        "# print(f\"‚úÖ Sa√≠da: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Exerc√≠cio Avan√ßado: An√°lise de Aten√ß√£o\n\n",
        "### Desafio 2: Visualiza√ß√£o Inteligente\n\n",
        "Agora vamos fazer algo **MUITO LEGAL**: analisar como nosso Transformer presta aten√ß√£o em diferentes partes de uma frase!\n\n",
        "Crie uma fun√ß√£o que:\n",
        "1. Receba uma frase tokenizada\n",
        "2. Passe pelo modelo\n",
        "3. Extraia os padr√µes de aten√ß√£o\n",
        "4. Visualize quais palavras prestam aten√ß√£o em quais outras\n\n",
        "### O que voc√™ vai descobrir:\n",
        "- Como o modelo conecta sujeitos com verbos\n",
        "- Quais heads focam em que tipos de rela√ß√µes\n",
        "- Padr√µes interessantes de linguagem\n\n",
        "### üí° Dica do Pedro\n",
        "Esse tipo de an√°lise √© usado pelos pesquisadores do Google, OpenAI e Meta para entender como os modelos \"pensam\"! Voc√™ vai estar fazendo ci√™ncia de ponta!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 2: An√°lise de Padr√µes de Aten√ß√£o\n",
        "\n",
        "def analyze_attention_patterns(model, input_ids, layer_idx=0, head_idx=0):\n",
        "    \"\"\"\n",
        "    Extrai e visualiza padr√µes de aten√ß√£o de uma camada espec√≠fica\n",
        "    \n",
        "    Args:\n",
        "        model: Modelo Transformer\n",
        "        input_ids: Tokens de entrada\n",
        "        layer_idx: Qual camada analisar\n",
        "        head_idx: Qual head analisar\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO: Implementar extra√ß√£o de aten√ß√£o\n",
        "    # Dica: Voc√™ precisa modificar o forward pass para retornar \n",
        "    # os pesos de aten√ß√£o al√©m dos logits\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # TODO: Forward pass e extra√ß√£o dos pesos de aten√ß√£o\n",
        "        pass\n",
        "    \n",
        "    # TODO: Criar visualiza√ß√£o dos padr√µes\n",
        "    # Dica: Use seaborn.heatmap como fizemos antes\n",
        "    \n",
        "    pass\n",
        "\n",
        "def find_interesting_patterns(attention_weights):\n",
        "    \"\"\"\n",
        "    Encontra padr√µes interessantes nos pesos de aten√ß√£o\n",
        "    \n",
        "    Args:\n",
        "        attention_weights: Matriz de aten√ß√£o [seq_len, seq_len]\n",
        "        \n",
        "    Returns:\n",
        "        dict: Padr√µes encontrados\n",
        "    \"\"\"\n",
        "    \n",
        "    patterns = {}\n",
        "    \n",
        "    # TODO: Implementar detec√ß√£o de padr√µes\n",
        "    # Exemplos de padr√µes para detectar:\n",
        "    # 1. Aten√ß√£o predominantemente local (diagonal)\n",
        "    # 2. Aten√ß√£o em posi√ß√µes espec√≠ficas\n",
        "    # 3. Padr√µes de long-range dependencies\n",
        "    \n",
        "    return patterns\n",
        "\n",
        "print(\"üîç EXERC√çCIO 2: An√°lise de Aten√ß√£o\")\n",
        "print(\"üí≠ Implementar as fun√ß√µes acima para analisar como o modelo 'pensa'\")\n",
        "print(\"üé® Criar visualiza√ß√µes que mostrem padr√µes de aten√ß√£o\")\n",
        "print(\"üß† Descobrir insights sobre o comportamento do modelo!\")\n",
        "\n",
        "# Exemplo de uso (quando implementado):\n",
        "# sentence_tokens = torch.tensor([[1, 45, 123, 67, 89, 2]])  # Exemplo\n",
        "# analyze_attention_patterns(model, sentence_tokens, layer_idx=0, head_idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéä Resumo: A Jornada pelos Transformers\n\n",
        "**PARAB√âNS!** üéâ Voc√™ acabou de fazer uma jornada **INCR√çVEL** pelo cora√ß√£o dos LLMs!\n\n",
        "### üß† O que Aprendemos Hoje:\n\n",
        "#### 1. **Fundamentos dos Transformers**\n",
        "- ‚úÖ Como funcionam diferente dos modelos sequenciais\n",
        "- ‚úÖ Por que revolucionaram a IA\n",
        "- ‚úÖ Arquitetura geral e fluxo de dados\n\n",
        "#### 2. **Mecanismo de Aten√ß√£o** üéØ\n",
        "- ‚úÖ Self-attention e como palavras \"conversam\"\n",
        "- ‚úÖ Query, Key, Value - a matem√°tica por tr√°s\n",
        "- ‚úÖ Multi-Head Attention - m√∫ltiplas perspectivas\n",
        "\n",
        "#### 3. **Componentes Essenciais** ‚öôÔ∏è\n",
        "- ‚úÖ Feed Forward Networks - processamento individual\n",
        "- ‚úÖ Residual Connections - preservando informa√ß√£o\n",
        "- ‚úÖ Layer Normalization - estabilizando treinamento\n",
        "- ‚úÖ Positional Encoding - ensinando posi√ß√£o\n\n",
        "#### 4. **Implementa√ß√£o Pr√°tica** üíª\n",
        "- ‚úÖ C√≥digo funcional de cada componente\n",
        "- ‚úÖ Transformer completo do zero\n",
        "- ‚úÖ Testes e valida√ß√µes\n",
        "\n",
        "### üîó Conectando com o Curso:\n",
        "\n",
        "**M√≥dulos anteriores:**\n",
        "- ‚úÖ **M√≥dulo 1**: Setup que usamos hoje ‚ú®\n",
        "- ‚úÖ **M√≥dulo 2**: LLMs que agora sabemos como funcionam! üß†\n",
        "\n",
        "**Pr√≥ximos m√≥dulos:**\n",
        "- üîú **M√≥dulo 4**: Tokens e Tokeniza√ß√£o - como texto vira n√∫meros\n",
        "- üîú **M√≥dulo 5**: Embeddings - representa√ß√µes sem√¢nticas\n",
        "- üîú **M√≥dulo 6**: Tipos de Modelos - GPT, BERT, T5 e fam√≠lia\n\n",
        "### üí° Dica Final do Pedro:\n",
        "Os Transformers s√£o a base de **TUDO** na IA moderna: ChatGPT, Claude, Gemini, todos usam essa arquitetura! Voc√™ agora entende o \"motor\" por tr√°s da revolu√ß√£o da IA. **LIIINDO!** üöÄ\n\n",
        "### üéØ Para Casa:\n",
        "1. Complete os exerc√≠cios se n√£o terminou\n",
        "2. Experimente com diferentes configura√ß√µes\n",
        "3. Pense em como isso se conecta com os LLMs que usa no dia a dia\n\n",
        "**Nos vemos no pr√≥ximo m√≥dulo para falar sobre Tokeniza√ß√£o!** üî•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéä Celebra√ß√£o final - Estat√≠sticas do que constru√≠mos!\n",
        "\n",
        "def model_summary(model):\n",
        "    \"\"\"Cria um resumo detalhado do modelo\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    # Calculando tamanho aproximado em MB\n",
        "    param_size_mb = total_params * 4 / (1024 * 1024)  # 4 bytes por par√¢metro float32\n",
        "    \n",
        "    print(\"üèÜ RESUMO DO SEU TRANSFORMER\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üìä Par√¢metros totais: {total_params:,}\")\n",
        "    print(f\"üéØ Par√¢metros trein√°veis: {trainable_params:,}\")\n",
        "    print(f\"üíæ Tamanho aproximado: {param_size_mb:.2f} MB\")\n",
        "    print(f\"üß† Camadas: {len(model.transformer_blocks)}\")\n",
        "    print(f\"üé≠ Heads por camada: {model.transformer_blocks[0].multi_head_attention.num_heads}\")\n",
        "    print(f\"üìö Vocabul√°rio: {model.vocab_size:,} tokens\")\n",
        "    print(f\"üìè Dimens√£o do modelo: {model.d_model}\")\n",
        "    \n",
        "    # Compara√ß√£o com modelos famosos\n",
        "    print(\"\\nüîç COMPARA√á√ÉO COM MODELOS FAMOSOS:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    famous_models = {\n",
        "        \"Seu Transformer\": total_params,\n",
        "        \"GPT-2 Small\": 124_000_000,\n",
        "        \"GPT-2 Medium\": 355_000_000, \n",
        "        \"GPT-2 Large\": 774_000_000,\n",
        "        \"GPT-3\": 175_000_000_000,\n",
        "        \"GPT-4\": 1_760_000_000_000  # Estimativa\n",
        "    }\n",
        "    \n",
        "    for name, params in famous_models.items():\n",
        "        if name == \"Seu Transformer\":\n",
        "            print(f\"üöÄ {name}: {params:,} par√¢metros ‚≠ê\")\n",
        "        else:\n",
        "            ratio = params / total_params\n",
        "            print(f\"üìà {name}: {params:,} par√¢metros ({ratio:.0f}x maior)\")\n",
        "    \n",
        "    print(\"\\nüéâ PARAB√âNS! Voc√™ construiu um Transformer funcional!\")\n",
        "    print(\"üí° √â o mesmo princ√≠pio do ChatGPT, s√≥ que em escala menor!\")\n",
        "    \n",
        "model_summary(model)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéä M√ìDULO 3 CONCLU√çDO COM SUCESSO! üéä\")\n",
        "print(\"üöÄ Voc√™ agora √© um expert em Arquitetura Transformer!\")\n",
        "print(\"üî• Preparado para os pr√≥ximos desafios do curso!\")\n",
        "print(\"üìö Pr√≥ximo stop: Tokens e Tokeniza√ß√£o!\")\n",
        "print(\"=\"*60)"
      ]
    }
  ]
}